Session details: Session 3: What is Real,No abstract available.
When IoT met Augmented Reality: Visualizing the Source of the Wireless Signal in AR View,"This paper presents VisIoT, a system that tracks the location of a wireless transmitter in IoT devices and displays it in the screen of an AR device such as smart glasses and tablet. The proposed system benefits existing IoT systems by enabling intuitive interaction between a user and IoT devices and further enhancing visualization of the data collected from IoT sensors. VisIoT achieves them through a combination of wireless sensing and camera motion tracking. By using the azimuth and elevation angles between the wireless transmitter and the camera-equipped mobile device, VisIoT can instantly identify the location of the IoT device from the camera image. This paper introduces novel azimuth and elevation estimation algorithms that leverage the phase difference of the signals from two antennas together with the tracked camera rotation. We prototype VisIoT using a tablet PC and a USRP software radio, and develop a software that tracks and visualizes the location of ZigBee nodes in real time. The evaluation results show that VisIoT can accurately track the nodes with the median position error of 6%."
Freedom: Fast Recovery Enhanced VR Delivery Over Mobile Networks,"In this paper we design and implement Freedom, a mobile VR system that deliver high quality VR content on today's mobile devices using 4G/LTE cellular networks. Compared to existing state-of-the-art, Freedom does not rely on any video frame pre- rendering or viewpoint prediction. We send a latency-adaptive VAM frame that contains pixels around the FoV. This allows the clients to render locally at a high refresh rate of 60 Hz to accommodate and compensate for the user's head movements before the next server update arrives. We demonstrate that Freedom is the first system in the world that can support dynamic and live 8K resolution VR content, while adapting to the real-world latency variations experienced in cellular networks. Compared to streaming the whole 360Â° panoramic VR content, we show that Freedom achieves up to 80% bandwidth savings. Finally, we provide detailed end to end latency measurements of actual VR systems by running extensive experiments in a private LTE testbed using a Mobile Edge Cloud (MEC)."
GLEAM: An Illumination Estimation Framework for Real-time Photorealistic Augmented Reality on Mobile Devices,"Mixed reality mobile platforms attempt to co-locate virtual scenes with physical environments, towards creating immersive user experiences. However, to create visual harmony between virtual and physical spaces, the virtual scene must be accurately illuminated with realistic lighting that matches the physical environment. To this end, we design GLEAM, a framework that provides robust illumination estimation in real-time by integrating physical light-probe estimation with current mobile AR systems. GLEAM visually observes reflective objects to compose a realistic estimation of physical lighting. Optionally, GLEAM can network multiple devices to sense illumination from different viewpoints and compose a richer estimation to enhance realism and fidelity. Using GLEAM, AR developers gain the freedom to use a wide range of materials, which is currently limited by the unrealistic appearance of materials that need accurate illumination, such as liquids, glass, and smooth metals. Our controlled environment user studies across 30 participants reveal the effectiveness of GLEAM in providing robust and adaptive illumination estimation over commercial status quo solutions, such as pre-baked directional lighting and ARKit 2.0 illumination estimation. Our benchmarks reveal the need for situation driven tradeoffs to optimize for quality factors in situations requiring freshness over quality and vice-versa. Optimizing for different quality factors in different situations, GLEAM can update scene illumination as fast as 30ms by sacrificing richness and fidelity in highly dynamic scenes, or prioritize quality by allowing an update interval as high as 400ms in scenes that require high-fidelity estimation."
LpGL: Low-power Graphics Library for Mobile AR Headsets,"We present LpGL, an OpenGL API compatible Low-power Graphics Library for energy efficient AR headset applications. We first characterize the power consumption patterns of a state of the art AR headset, Magic Leap One, and empirically show that its internal GPU is the most impactful and controllable energy consumer. Based on the preliminary studies, we design LpGL so that it uses the device's gaze/head orientation information and geometry data to infer user perception information, intercepts application-level graphics API calls, and employs frame rate control, mesh simplification, and culling techniques to enhance energy efficiency of AR headsets without detriment of user experience. Results from a comprehen- sive set of controlled in-lab experiments and an IRB-approved user study with 25 participants show that LpGL reduces up to 22% of total energy usage while adding only 46 sec of latency per object with close to no loss in subjective user experience."
