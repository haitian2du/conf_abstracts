ECHO: Efficiently Overbooking Applications to Create a Highly Available Cloud.,"Ensuring high availability for applications despite unpredictable cloud component failure events is a well-known problem in managing cloud infrastructure. One proposed solution uses a VM redundancy approach, reserving cloud resources for backup VMs that can substitute for primary ones in case of a failure event. However, this solution decreases the cloud resource utilization, since the backup resources usually remain idle. In this paper, we propose ECHO, a cloud resource management system that overbooks these backup VMs by optimizing the overbooking rate tradeoff between maximizing the cloud resource utilization, and thus maximizing the cloud provider's revenue; and improving application availability, thus satisfying users. Specifically, ECHO first obtains the optimal overbooking rate required to achieve a cloud provider's desired resource utilization level. It then computes the optimal (required) number of backup VMs that are required to maintain a given application availability level. Our extensive experimental and simulation results show that using ECHO can increase the number of accepted applications with satisfied availability by about 30%, while increasing the defined resource utilization at the same time."
A Novel Timestamping Mechanism for Clouds and Its Application on Available Bandwidth Estimation.,"The packet time information at their receivers carries useful network information and is used by various networking applications and protocols. However it is challenging to accurately measure the packet time information in a cloud network due to various software and hardware factors at their receivers, such as virtual machine (VM) scheduling. In order to mitigate the impact of those receiver factors, we propose a novel packet timestamping mechanism to measure the inter-packet gaps just before they arrive at their receiver using an external clock server, which periodically sends packets to the receiver. We demonstrate the application of the proposed timestamping mechanism using an improved available bandwidth estimation method, which leverages two types of packet time information: the original packet time information measured using the local receiver clock, and the additional inter-packet gap information measured using the external clock server. Both our experiment results and analysis show that the additional information can greatly improve the available bandwidth estimation accuracy in a cloud network even with heavy VM scheduling at the cost of little additional traffic overhead."
MEER: Online Estimation of Optimal Memory Reservations for Long Lived Containers in In-Memory Cluster Computing.,"Modern in-memory data-intensive computing systems like Spark create long-lived containers to execute diverse types of applications. They rely on a cluster manager like YARN or Mesos to perform resource allocation to the containers. The cluster manager or scheduler requires users of the containers to reserve resources beforehand. It is a challenge to estimate just right amounts of memory to run the applications before execution, so as to avoid over-or under-provisioning of memory space. We discover a general property of memory reservation elasticity, which allows applications to run with a reservation limit smaller than they would ideally need while only paying a moderate performance penalty. Based on the property, we designed a system, namely MEER, which performs online estimation of minimum necessary amount of memory limit that achieves nearly optimal performance. We referred to it as optimal reservation, which divides memory over-provisioning from under-provisioning. It is non-trivial to efficiently estimate optimal reservations on line through one step without runtime history. MEER uses a two-step approach to dealing with the challenge: 1) Do robust profiling and probability density analysis of applications' memory footprints in two pilot runs. By using confidence levels for the prediction, we reduce the negative effects of container footprints' randomness and achieve a highly accurate online initial estimation (over 80% accuracy) of optimal reservation. 2) By exploiting a self-decay property of the analytical results, MEER adaptively performs iterative search based on a feed-back control mechanism over subsequent recurring executions. We implemented MEER atop of YARN and evaluated the prototype by running 15 benchmark workloads on a 16-node local cluster. Evaluation results show that it achieves an average accuracy of more than 95%. By deploying MEER on schedulers and allocating memory according to the optimal reservations, one could improve cluster memory utilization by about 40%. It reduces individual application execution time by 2 to 6 times on average compared to the state-of-the-art approaches. A 90 times peak speedup for PageRank in comparison with the default Spark/Yarn is observed."
An Online Mechanism for Purchasing IaaS Instances and Scheduling Pleasingly Parallel Jobs in Cloud Computing Environments.,"Nowadays, many users select to outsource their job executions to service clouds. These users often have heterogeneous demands while they dynamically arrive at the clouds. For reducing the costs and risks, lots of service cloud operators purchase on-demand instances from public IaaS clouds and provide professional services elastically to users. However, without knowing the future information, it is hard for cloud operators to optimally determine the instance purchasing as well as job scheduling and pricing schemes. In order to achieve maximum social welfare, this paper targets to design an auction mechanism which executes in an online fashion for service clouds, with unique features of job-oriented users, pleasingly parallel jobs and soft deadline constraints. Such a mechanism ought to run in polynomial time, provide truthfulness guarantee, satisfy individual rationality and budget balance, and achieve competitive social welfare. Nevertheless, when designing mechanisms there are a few significant challenges, including the difficulty for finding optimal solution, the strategic behaviours of selfish users with private information and the online arrivals of users. Facing these challenges, we leverage the idea of proportional sharing and propose an online mechanism which is proven to achieve all desired properties. The efficiency of the proposed mechanism is validated by both theoretical analysis and extensive simulations which use both synthetic data and Google's job traces."
Reducing Flow Completion Time with Replaceable Redundant Packets in Data Center Networks.,"In the data center network, a packet-level load balancer such as random packet spraying (RPS) achieves high throughput by spraying data packets to all transmission paths, which easily suffers from the packet out-of-order problem under network asymmetry. While state-of-the-art network coding schemes can mitigate the issue, too many encoded redundant packets introduced by the network coding will cause extra traffic overhead, larger queueing delay and even TCP time out. In this paper, we propose OPportunistic Encoded Redundant (OPER), a middle-layer design upon existing coding schemes to mitigate the curse of redundant packets. Specifically, OPER uses opportunistic redundant packets which are replaceable by the data packets in the switches under heavy congestion. OPER is implemented as a shim layer between TCP and IP layers at end-hosts and a loadable plugin at switches, leaving existing TCP/IP protocols unmodified. The testbed and NS2 experiments show that, OPER reduces the average flow completion time by up to 71% compared with the state-of-the-art multipath coding schemes."
Improving TCP Robustness over Asymmetry with Reordering Marking and Coding in Data Centers.,"Modern data center networks provide multiple paths between host pairs to guarantee high aggregated network bandwidth and transmission reliability. However, data center networks suffer from various uncertainties such as highly dynamic traffic and heterogeneous devices. These uncertainties easily lead to network asymmetry and cause significant packet reordering. Unfortunately, due to lacking the explicit reordering feedback, existing sender-based and receiver-based solutions hardly adapt to asymmetric data center networks and cause long-tailed flow completion time as well as throughput loss. In this paper, we propose a per-packet transmission scheme RMC to eliminate the impact of packet reordering and handle uncertainties in asymmetric networks. To avoid unnecessary fast retransmission, the switch proactively identifies packet reordering according to local queue length and global path latency. Furthermore, we employ a coding technique to reduce long-tailed flow completion time under network asymmetry. Through a series of large-scale NS2 simulations and testbed experiments, we demonstrate that RMC reduces flow completion time by up to 72% compared with existing protocols."
SpeedyBox: Low-Latency NFV Service Chains with Cross-NF Runtime Consolidation.,"Software-based service chains in Network Function Virtualization (NFV) typically suffers high processing latency. This latency grows as chain lengths increase and possibly violates application requirements. Previous efforts focus on reducing latency while maintaining the perspective of each NF being an independent, isolated module. This results in processing redundancy that could eventually become the performance bottleneck. In this paper, we propose a low-latency NFV framework called SpeedyBox, that innovatively enables cross-NF runtime optimizations in a service chain to eliminate processing redundancy. SpeedyBox builds a fast data path for flows at runtime by consolidating the aggregate actions across diverse network functions (NFs) in a service chain. In SpeedyBox, each NF is instrumented with a stateful Local Match-Action Table (MAT), and leverages our easy-to-use APIs to record its per-flow behavior in the Local MAT. Next, SpeedyBox uses a Global MAT to build the fast data path by consolidating actions from each Local MAT, while providing the ability to express the stateful NF behaviors with an Event Table. We have implemented a prototype of SpeedyBox on the BESS and OpenNetVM NFV platforms. Our trace-driven evaluation on common NFs shows that SpeedyBox achieves significant latency reduction under real world scenarios."
HyScale: Hybrid and Network Scaling of Dockerized Microservices in Cloud Data Centres.,"When designing modern software, care must be taken to allow for applications to scale based on the demands of its users while still accommodating flexibility in development. Recently, microservices architectures have garnered the attention of many organizations-providing higher levels of scalability, availability, and fault isolation. Many organizations choose to host their microservices architectures in cloud data centres to offset costs. Incidentally, data centres become over-encumbered during peak usage hours and underutilized during off-peak hours. Traditional microservice scaling methods perform either horizontal or vertical scaling exclusively. When used in combination, however, these methods offer complementary benefits and compensate for each other's deficiencies. To leverage the high availability of horizontal scaling and the fine-grained resource control of vertical scaling, we developed two novel hybrid autoscaling algorithms and a dedicated network scaling algorithm and benchmarked them against Google's popular Kubernetes horizontal autoscaling algorithm. Results indicated up to 1.49x speedups in response times for our hybrid algorithms, and 1.69x speedups for our network algorithm under high-burst network loads."
A Tight Lower Bound for Relaxed Loop-Free Updates in SDNs.,"Due to the unpredictable update orders in the data plane, a transient forwarding loop may be introduced in Software Defined Networks (SDNs) during the updates, which can result in packet drops and influence the application performance. The node-ordering protocol is a major update mechanism without incurring extra flow table space overhead. Existing algorithms using this protocol with loop-free constraint require Ω(n)-round lower bound, where n is the number of switches in the network. To accelerate the updates, a weaker notion of loop-freedom - relaxed loop-freedom - has been introduced. However, many problems about the theoretical bound of the node-ordering protocol with relaxed loop-free constraint remain unsolved yet. In this paper, we provide a rigorous proof to derive a Ω (log n)-round lower bound for relaxed loop-free update problem, which closes a long-term open problem whether O(1)-round schedules always exist or not. In addition, we develop a fast relaxed loop-free update algorithm named Savitar that touches the tight lower bound. Specifically, we prove that Savitar can use at most 2⌊log
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sub>
 n⌋ - 1 rounds to guarantee relaxed loop-freedom for any update instance. Extensive experiments on Mininet using a Floodlight controller show that Savitar can significantly decrease the update time, achieve the near optimal solution and save the number of rounds over 30% compared with the state of the art."
Taming Latency in Data Centers Via Active Congestion-Probing.,"In cloud environments, interactive applications deployed in data centers often generate swarms of short-lived data transfers (or flows) that face dramatic competition for the scarce switch buffer space from other short-lived as well as the long-lived flows. In the presence of bloated queues, such short-lived flows often experience multiple packet losses per round-trip time which often triggers the timeout-based loss recovery mechanism. A direct consequence of this is an inflated application response time that turns out to be orders of magnitude larger than what it should be. A data center aware TCP protocol (DCTCP) was designed as a new TCP specifically to address this issue, however, it does not consider its co-existence with other transport protocol (e.g., CuBIC and NewReno of Linux). In such situations, which are abundant in multi-tenant data centers, the legacy large initial congestion window sizes (e.g., 10 segments), induce multiple packet losses at the onset of a TCP flow, which forces timeout and even binary exponential backoff. In this paper, we propose a novel Hypervisor-based, application-transparent approach for active congestion probing to enable the hypervisor to infer on-path congestion before the TCP connection is fully established for new traffic to avoid such massive packet losses and timeout. The so-called ProBoSCIS mechanism does not require any changes to TCP, works with all versions of TCP and does not need any special network hardware features other than those that exist in today's data center commodity switches. We show its effectiveness via ns2 simulation and demonstrate its practical feasibility by implementing and deploying it in a small-scale data center test-bed. We show the significant reduction in application latency by adopting ProBoSCIS in a series of real experiments."
Reco: Efficient Regularization-Based Coflow Scheduling in Optical Circuit Switches.,"To improve the application-level data efficiency, the scheduling of coflows, defined as a collection of parallel flows sharing the same objective, is prevailing in recent data centers. Meanwhile, optical circuit switches (OCS) are gradually applied to provide high data rate with low power consumption. However, so far few research outputs have covered the flow scheduling in the context of OCS, let alone the coflow scheduling problems. In this paper, we investigate coflow scheduling in the OCS-based data centers. We first derive a novel operation called regularization processed respectively on the flow traffic demands and the flow start times. Regularization can be efficiently implemented and reduce the circuit reconfiguration frequency dramatically. We then propose a 2-approximation algorithm, called Reco-Sin, for single coflow scheduling to minimize the coflow completion time (CCT). For multiple coflows, we derive another approximation algorithm, called Reco-Mul, to minimize the total weighted CCT, which can transform any non-preemptive multi-coflow scheduling in packet switches to that in OCS. Extensive simulations based on Facebook data traces show that Reco-Sin and Reco-Mul outperform state-of-the-art schemes significantly, i.e., one single coflow can be finished up to 2.72× faster with Reco-Sin, and multiple coflows can be completed up to 3.44× faster with Reco-Mul."
MIRAS: Model-based Reinforcement Learning for Microservice Resource Allocation over Scientific Workflows.,"Microservice, an architectural design that decomposes applications into loosely coupled services, is adopted in modern software design, including cloud-based scientific workflow processing. The microservice design makes scientific workflow systems more modular, more flexible, and easier to develop. However, cloud deployment of microservice workflow execution systems doesn't come for free, and proper resource management decisions have to be made in order to achieve certain performance objective (e.g., response time) within constraint operation cost. Nevertheless, effective online resource allocation decisions are hard to achieve due to dynamic workloads and the complicated interactions of microservices in each workflow. In this paper, we propose an adaptive resource allocation approach for microservice workflow system based on recent advances in reinforcement learning. Our approach (1) assumes little prior knowledge of the microservice workflow system and does not require any elaborately designed model or crafted representative simulator of the underlying system, and (2) avoids high sample complexity which is a common drawback of model-free reinforcement learning when applied to real-world scenarios. We show that our proposed approach automatically achieves effective policy for resource allocation with limited number of time-consuming interactions with the microservice workflow system. We perform extensive evaluations to validate the effectiveness of our approach and demonstrate that it outperforms existing resource allocation approaches with read-world emulated workflows."
Online Collection and Forecasting of Resource Utilization in Large-Scale Distributed Systems.,"Large-scale distributed computing systems often contain thousands of distributed nodes (machines). Monitoring the conditions of these nodes is important for system management purposes, which, however, can be extremely resource demanding as this requires collecting local measurements of each individual node and constantly sending those measurements to a central controller. Meanwhile, it is often useful to forecast the future system conditions for various purposes such as resource planning/allocation and anomaly detection, but it is usually too resource-consuming to have one forecasting model running for each node, which may also neglect correlations in observed metrics across different nodes. In this paper, we propose a mechanism for collecting and forecasting the resource utilization of machines in a distributed computing system in a scalable manner. We present an algorithm that allows each local node to decide when to transmit its most recent measurement to the central node, so that the transmission frequency is kept below a given constraint value. Based on the measurements received from local nodes, the central node summarizes the received data into a small number of clusters. Since the cluster partitioning can change over time, we also present a method to capture the evolution of clusters and their centroids. As an effective way to reduce the amount of computation, time-series forecasting models are trained on the time-varying centroids of each cluster, to forecast the future resource utilizations of a group of local nodes. The effectiveness of our proposed approach is confirmed by extensive experiments using multiple real-world datasets."
Trading Private Range Counting over Big IoT Data.,"Data privacy arises as one of the most important concerns, facing the pervasive commoditization of big data statistic analysis in Internet of Things (IoT). Current solutions are incapable to thoroughly solve the privacy issues on data pricing and guarantee the utility of statistic outputs. Therefore, this paper studies the problem of trading private statistic results for IoT data, by considering three factors. Specifically, a novel framework for trading range counting results is proposed. The framework applies a sampling-based method to generate approximated counting results, which are further perturbed for privacy concerns and then released. The results are theoretically proved to achieve unbiasedness, bounded variance, and strengthened privacy guarantee under differential privacy. Moreover, a pricing approach is proposed for the traded results, which is proved to be immune against arbitrage attacks. The framework is evaluated by estimating the air pollution levels with different ranges on 2014 CityPulse Smart City datasets. The analysis and evaluation results demonstrate that our framework greatly reduces the error of range counting approximation; and the optimal perturbation approach enables that the private counting satisfies the specified approximation degree while providing strong privacy guarantee."
Probabilistic Skyline Computation on Vertically Distributed Uncertain Data.,"The skyline query is important in database community. Recently, owing to the inherent uncertainty of some applications, skyline query on uncertain data has been widelystudied using probabilistic model, e.g. p-skyline. In the scenario where uncertain data is vertically distributed among multiple servers, the main purpose of p-skyline computation is to minimize the retrieved records from servers to the local client due to the dominance factor of expensive network communication. In this paper, we present three communication-efficient p-skyline algorithms ASR, IASR and FSLR on vertically distributed uncertain data. ASR alternates sorted and random accesses to retrieve the records at servers and performs retrieving-boundingchecking iteration until all the objects can be determined whether they are in the p-skyline result or not. The communication of the instances not retrieved can be saved. IASR is an improved version of ASR. By examining the net gain of retrieving-boundingchecking iteration, IASR early terminates the iteration to further reduce the cost of communication. Compared to ASR and IASR, FSLR performs random accesses only on demand. FSLR first conducts sorted accesses to get loose upper bounds of skyline probabilities of the instances. Then, FSLR uses random accesses to complement a part of retrieved instances to get tighter upper and lower bounds of skyline probabilities until the p-skyline result is computed. Our experimental results demonstrate that our algorithms ASR, IASR and FSLR significantly outperform the intuitive method for p-skyline computation on vertically distributed uncertain data."
Catfish: Adaptive RDMA-enabled R-Tree for Low Latency and High Throughput.,"R-tree is a foundational data structure used in spatial databases and scientific databases. With the advancement of Internet and computer architectures, in-memory data processing for R-tree in distributed systems has become a common platform. We have observed new performance challenges to process R-tree as the amount of multidimensional datasets become increasingly huge. Specifically, an R-tree server can be heavily overloaded while the network and client CPU are lightly loaded, and vice versa. In this paper, we present the design and implementation of Catfish, an RDMA enabled R-tree for low latency and high throughput by adaptively utilizing the available network bandwidth and computing resources to balance the workloads between clients and servers. We design and implement two basic mechanisms of using RDMA for the client-server R-tree. First, in the fast messaging design, we use RDMA writes to send R-tree requests to the server and let server threads process R-tree requests to achieve low query latency. Second, in the RDMA offloading design, we use RDMA reads to offload tree traversal from the server to the client, which rescues the server as it is overloaded. We further develop an adaptive scheme to effectively switch an R-tree search between fast messaging and RDMA offloading, maximizing the overall performance. Our experiments show that the adaptive solution of Catfish on InfiniBand significantly outperforms R-tree that uses only fast messaging or only RDMA offloading in both latency and throughput. Catfish can also deliver up to one order of magnitude performance over the traditional schemes using TCP/IP on 1 Gbps and 40 Gbps Ethernet. We make a strong case to use RDMA to effectively balance workloads in distributed systems for low latency and high throughput."
Simois: A Scalable Distributed Stream Join System with Skewed Workloads.,"Many BigData applications require to perform quick join operations on different large-scale real-time data streams. The key challenge to design an efficient stream join system is how to reasonably partition the streaming data among distributed processing nodes to avoid high density of join computation. However, the skewed distribution of real world streams raises great challenges for streaming data partitioning in distributed stream join systems. Existing hash based partitioning schemes incur significant load imbalance which leads to low system throughput and long processing latency, while shuffling based strategies incur redundant join computation and much more communication. To address this issue, in this paper, we propose and implement a scalable distributed stream join system, Simois, which shuffles the potential top heavy-load keys while hashing the others. However, how to identify the keys which lead to the heavy workload imbalance is challenging, because the heavy workload is determined by the current joint status of two streams, and the distribution of the two streams may change with time. To solve this problem, we design a novel efficient exponential counting scheme for identifying the keys with the heaviest workload in the two dynamic streams. The proposed exponential counting scheme needs extremely low computation and space cost, so that it can be well implemented in a stream processing system. Moreover, we design a popularity decline algorithm to make our design adaptive to the highly dynamic changes of streams. We implement Simois on top of Apache Storm and conduct comprehensive experiments using large-scale real world traces. Experiment results show that Simois improves the system throughput significantly by 52% and reduces the average latency by 37%, compared to existing state-of-the-art designs."
Co-scheduler: Accelerating Data-Parallel Jobs in Datacenter Networks with Optical Circuit Switching.,"The optical circuit switch (OCS) in recently proposed hybrid electrical/optical datacenter networks (Hybrid-DCN) can only be used to transfer large flows (i.e., flows with a large size of data). Current job schedulers for data-parallel frameworks are not suitable for Hybrid-DCN, since they neither place tasks to aggregate data traffic to take advantage of OCS nor schedule tasks to minimize the Coflow completion time (CCT). In this paper, we propose Co-scheduler, a job scheduler for dataparallel frameworks that aims to improve job performance by attempting to place the tasks of a job to aggregate enough data traffic to take advantage of OCS and minimize the CCT in Hybrid-DCN. Specifically, for each job, Co-scheduler computes a guideline on the number of racks to place the job's input data and to run the job's map tasks, so that the job can potentially take full advantage of OCS to transfer its data. When the map tasks of a job complete, Co-scheduler computes all the possible schedules of the reduce tasks of the job. Each possible schedule includes the number of racks to schedule the reduce tasks that enables the job to use OCS to transfer its data, and the number of reduce tasks to place on each of the racks that minimizes CCT of the job. Next, Co-scheduler selects a best schedule among all the possible schedules so that the job completion time is minimized. Finally, Co-scheduler schedules the map tasks and reduce tasks of the job based on the computed guideline and best schedule. The evaluation demonstrates that compared to the state-of-theart schedulers, Co-scheduler achieves performance improvements on makespan, average job completion time, and average CCT by up to 51.2%, 54.6% and 73.6%, respectively."
Falcon: Towards Computation-Parallel Deep Learning in Heterogeneous Parameter Server.,"Parameter server paradigm has shown great performance superiority for handling deep learning (DL) applications. One crucial issue in this regard is the presence of stragglers, which significantly retards DL training progress. Previous solutions for solving straggler may not fully exploit the computation capacity of a cluster as evidenced by our experiments. This motivates us to make an attempt at building a new parameter server architecture that mitigates and addresses stragglers in heterogeneous DL from the perspective of computation parallelism. We introduce a novel methodology named straggler projection to give a comprehensive inspection of stragglers and reveal practical guidelines for resolving this problem: (1) reducing straggler emergence frequency via elastic parallelism control and (2) transferring blocked tasks to pioneer workers for fully exploiting cluster computation capacity. Following the guidelines, we propose the abstraction of parallelism as an infrastructure and elaborate the Elastic-Parallelism Synchronous Parallel (EPSP) that supports both enforced-and slack-synchronization schemes. The whole idea has been implemented in a prototype called Falcon which efficiently accelerates the DL training progress with the presence of stragglers. Evaluation under various benchmarks with baseline comparison evidences the superiority of our system. Specifically, Falcon yields shorter convergence time, by up to 61.83%, 55.19%, 38.92% and 23.68% reduction over FlexRR, Sync-opt, ConSGD and DynSGD, respectively."
LACS: Load-Aware Cache Sharing with Isolation Guarantee.,"Cluster caching has been increasingly deployed in front of cloud storage to improve I/O performance. In shared, multi-tenant environments such as cloud datacenters, cluster caches are constantly contended by many users. Enforcing performance isolation between users hence becomes imperative to cluster caching. A user's caching performance critically depends on two factors: (1) the amount of cache allocation and (2) the load of servers in which its files are cached. However, existing cache sharing policies only provide guarantees on the amount of cache allocation, while remaining agnostic to the load of cache servers. Consequently, ""mice"" users having files co-located with ""elephants"" contributing heavy data accesses may experience extremely long latency, hence receiving no isolation. In this paper, we propose a Load-Aware Cache Sharing scheme (LACS) to enforce isolation between users. LACS keeps track of the load contributed by each user and reins back the congestions caused by elephant users by throttling their cache usage and network bandwidth. We have implemented LACS atop Alluxio, a popular cluster caching system. EC2 deployment shows that LACS achieves performance isolation in the presence of elephants, while improving the mean read latency by up to 80.4% (25.3% on average) over the state-of-the-art load balancing technique."
Thresholded Monitoring in Distributed Data Streams.,"In this paper, we consider the problem of thresholded monitoring in distributed data streams, that is, given multiple distributed data streams observed by multiple monitors during a certain period, finding the items whose global frequencies overall data streams exceeding a given threshold. We first derive a lower bound of communication overhead for any deterministic algorithm for this problem. Then, we propose two different schemas, i.e., Low-threshold Cascaded Cuckoo Filter (L-CCF) for low-threshold monitoring and High-threshold Cascaded Cuckoo Filter (H-CCF) for high-threshold monitoring. L-CCF and H-CCF can identify items whose frequency are more than the given threshold while a desired false negative rate (FNR) is achieved and communication overhead is optimized. The key idea is to compress the communication overhead caused by transferring the ID and frequency information at the same time. First, to reduce the communication overhead of transferring IDs, we propose to encode the IDs into separate tiny parts and store these tiny parts in L-CCF or H-CCF. Second, to reduce the communication overhead of transferring frequencies, we adopt carry-in counter technique in L-CCF and multiple sampling technique in H-CCF. We evaluated L-CCF and H-CCF on two real-world traces and compared their performance with two prior adapted algorithms. Our experimental results show that on average, L-CCF and H-CCF achieve FNRs with 55.7% and 65.56% better than that of comparison algorithms while FPRs is maintained at the level of 2.23%."
Local Graph Edge Partitioning with a Two-Stage Heuristic Method.,"Graph edge partitioning divides the edges of an input graph into multiple balanced partitions of a given size to minimize the sum of vertices that are cut, which is critical to the performance of distributed graph computation platforms. Existing graph partitioning methods can be classified into two categories: offline graph partitioning and streaming graph partitioning. The first category requires global information for a graph during the partitioning, which is expensive in terms of time and memory for large-scale graphs. The second category, however, creates partitions solely based on the received edge information, which may result in lower performance than the offline methods. Therefore, in this study, the concept of local graph partitioning is introduced from local community detection to consider only local information, i.e., a part of the graph, instead of the graph as a whole, during the partitioning. The characteristic of storing only local information is important because real-world graphs are often large in scale, or they increase incrementally. Based on this idea, we propose a two-stage local partitioning algorithm, where the partitioning process is divided into two stages according to the structural changes of the current partition, and two different strategies are introduced to deal with the respective stages. Experimental results with real-world graphs demonstrate that the proposed algorithm outperforms the rival algorithms in most cases, including the state-of-the-art algorithm METIS."
IPSO: A Scaling Model for Data-Intensive Applications.,"Today's data center applications are predominantly data-intensive, calling for scaling out the workload to a large number of servers for parallel processing. Unfortunately, the existing scaling laws, notably, Amdahl's and Gustafson's laws are inadequate to characterize the scaling properties of dataintensive workloads. To fill this void, in this paper, we put forward a new scaling model, called In-Proportion and Scale-Out-induced scaling model (IPSO). IPSO generalizes the existing scaling models in two important aspects. First, it accounts for the possible in-proportion scaling, i.e., the scaling of the serial portion of the workload in proportion to the scaling of the parallelizable portion of the workload. Second, it takes into account the possible scaleout-induced scaling, i.e., the scaling of the collective overhead or workload induced by scaling out. IPSO exposes scaling properties of data-intensive workloads, rendering the existing scaling laws its special cases. In particular, IPSO reveals two new pathological scaling properties. Namely, the speedup may level off even in the case of the fixed-time workload underlying Gustafson's law, and it may peak and then fall as the system scales out. Extensive MapReduce and Spark-based case studies demonstrate that IPSO successfully captures diverse scaling properties of dataintensive applications. As a result, it can serve as a diagnostic tool to gain insights on or even uncover counter-intuitive root causes of observed scaling behaviors, especially pathological ones, for data-intensive applications. Finally, preliminary results also demonstrate the promising prospects of IPSO to facilitate effective resource provisioning to achieve the best speedup-versuscost tradeoffs for data-intensive applications."
Smoother: A Smooth Renewable Power-Aware Middleware.,"The large electricity bills and the negative impacts on environments accelerate the use of renewable power to supply systems. However, frequent fluctuation and intermittency of renewable power often cause the challenges in terms of the stability of both electricity grid and systems, as well as decrease the utilization of renewable power. Existing schemes fail to alleviate the renewable power fluctuation, which is caused by the essential properties of renewable power. In order to address this problem, we propose an efficient and easy-to-use smooth renewable power-aware middleware, called Smoother, which consists of Flexible Smoothing (FS) and Active Delay (AD). First, in order to smooth the fluctuation of renewable power, Flexible Smoothing carries out the optimized charge/discharge operation via computing the minimum variance of the renewable power that is supplied to systems per interval. Second, Active Delay improves the utilization of renewable power via actively adjusting the execution time of deferrable workloads. Extensive experimental results via examining the traces of real-world systems demonstrate that Smoother significantly reduces the negative impact of renewable power fluctuations on systems and improves the utilization of renewable power by 169.85% on average. We have released the source codes for public use."
The Cask Effect of Multi-source Content Delivery: Measurement and Mitigation.,"With the explosive growth of Internet traffic, multi-source content delivery has been introduced for improving the performance and quality-of-experience (QoE) of Internet services. Upgrading from single-source content delivery to multi-source content delivery, however, may not always lead to a better performance. Instead, a decline in terms of delivery speed often occurs. By conducting a comprehensive study, we show that the underlying reason of this counter-intuitive phenomenon is actually due to the cask effect of data sources at both macro and micro level. Specifically, at the macro level, data sources with different types are highly heterogeneous in terms of delivery performance, which means data sources with certain types are particularly easy to become the ""short boards"". At the micro level, for the data sources chosen by a client, the high diversity of participation time (DPT) of the sources could impair the acceleration effect. Motivated by the above findings, we design MDR (Multi-source Delivery Redirector), a middleware that contains two optimizations to improve the acceleration effect. One is the feature-greedy selection algorithm which can avoid selecting data sources with inferior types, and the other is the DPT-driven shuffle strategy which can avoid using unstable data sources. Simulation-based experiments show that the MDR outperforms existing approaches in terms of overall downloading performance."
OCStore: Accelerating Distributed Object Storage with Open-Channel SSDs.,"SSDs are getting widely used in data centers. It is a critical issue to design efficient software for exploiting the benefits of fast SSD hardware. In this paper, we propose OCStore, an object store based on open-channel SSDs for distributed object storage system. OCStore manages the objects directly on raw flash memory, mitigating redundant functions across the object store, the file system, and the FTL layers. It provides streamed transactional update, which not only ensures the multi-page atomicity leveraging the non-overwrite flash write features, but also provides isolation for independent I/O streams while enabling parallel accesses to different channels. OCStore also coordinates different channels to enable transaction-aware scheduling, so as to reduce transaction-level latency and provide low response time to distributed storage. We implement OCStore in Linux kernel on the real open-channel SSDs, and evaluate them as OSDs in Ceph. Evaluations show that OCStore outperforms state-of-the-art object stores by 1.5× to 3.0×, while providing much lower and stable latencies, and decreases up to 70% write traffic under heavy workloads."
PandaSync: Network and Workload Aware Hybrid Cloud Sync Optimization.,"With the widespread use and increasing popularity of cloud storage, more and more data are moved to the cloud, making cloud storage a platform for both data sharing among users, devices, and data backup for data reliability. Thus, it is critically important to ensure data consistency through efficient cloud synchronization (sync). The existing cloud synchronization schemes are either delta sync, which sends only the updated portion of a file but incurs high compute overhead of data deduplication for small files, or full sync, which avoids data deduplication by sending the full file but wastes network bandwidth and lengthens sync time by transferring significant amount of redundant data over the networks for large files. In this paper, we propose a hybrid cloud sync scheme, PandaSync, that combines full sync and delta sync dynamically based on file size and network conditions. To further improve small-file sync performance, we propose an optimization, Full2Sync, that merges the sync request with the file-sending request to reduce the number of network round-trips between the client and the cloud servers. The experiments conducted on our lightweight prototype implementation of PandaSync show that PandaSync reduces the sync time by an average of 85.1% and 74.6% from the delta sync scheme and full sync scheme, respectively."
One Size Never Fits All: A Flexible Storage Interface for SSDs.,"The rapid adoption of solid-state drives (SSDs) as a major storage component has been made possible thanks to their ability to export a standard block I/O interface to file system and application developers. Meanwhile, this high-level abstraction has been shown to limit the utilization of the devices and the performance of applications running on top of them. Indeed, many optimizations of performance-critical applications bypass the standard block interface and rely on low-level control over SSD internal processes. However, the need to directly manage the physical device significantly increases development complexity and cost, and reduces its portability. Thus, application developers must choose between two extreme options, either easy development or optimal performance, without a real possibility to balance between these two objectives. To bridge this gap, we propose a flexible storage interface that exports the SSD hardware in three levels of abstraction: as a raw flash media with its low-level details, as a group of functions to manage flash capacity, or as a configurable block device. This multi-level abstraction allows developers to choose the degree in which they desire to control the flash hardware in a manner that best suits their applications' semantics and performance objectives. We demonstrate the usability of this new model with Prism-SSD-a prototype of this interface as a user-level library on the Open-Channel SSD platform. We use each of the interface's three abstraction levels to modify the I/O module of three representative applications: a key-value cache system, a user-level file system, and a graph processing engine. Prism-SSD improves application performance by 5% to 27%, at varying development costs, between 200 and 3,500 lines of code."
PaRiS: Causally Consistent Transactions with Non-blocking Reads and Partial Replication.,"Geo-replicated data platforms are the backbone of several large-scale online services. Transactional Causal Consistency (TCC) is an attractive consistency level for building such platforms. TCC avoids many anomalies of eventual consistency, eschews the synchronization costs of strong consistency, and supports interactive read-write transactions. Partial replication is another attractive design choice for building geo-replicated platforms, as it reduces storage requirements and update propagation costs. This paper presents PaRiS, the first TCC system that supports partial replication and implements non-blocking parallel read operations. The latter reduce read latency which is of paramount importance for the performance of read-intensive applications. PaRiS relies on a novel protocol to track dependencies, called Universal Stable Time (UST). By means of a lightweight background gossip process, UST identifies a snapshot of the data that has been installed by every data center (DC) in the system. Hence, transactions can consistently read from such a snapshot on any server in any replication site without having to block. Moreover, PaRiS requires only one timestamp to track dependencies and define transactional snapshots, thereby achieving resource efficiency and scalability. We evaluate PaRiS on an AWS deployment composed of up to 10 replication sites. We demonstrate a performance gain of non-blocking reads vs. a blocking alternative (up to 1.47x higher throughput with 5.91x lower latency for read-dominated workloads and up to 1.46x higher throughput with 20.56x lower latency for write-heavy workloads). We also show that the throughput penalty incurred to implement causal consistency, compared to variant without the causal consistency guarantees, is as low as 20% for read-heavy workloads and 37% for write-heavy workloads. We furthermore show that PaRiS scales well with the number of DCs and partitions, while being able to handle larger datasets than existing solutions that assume full replication."
Beyond QoE: Diversity Adaption in Video Streaming at the Edge.,"Adaptive bitrate (ABR) algorithms have been critical techniques for high quality-of-experience (QoE) Internet video delivery. Prior work designs ABR algorithms by conducting the overall QoE function of fixed parameters. However, the QoE of end users are diverse and video bitrate may be chosen in a misleading way when leaving out the diversity. State-of-the-art ABR algorithms like MPC, Pensieve utilize off-line modeling techniques and result in performance degradation for online QoE diversity adaption. To address this issue, we propose Elephanta, an online flexible ABR algorithm for edge users which incorporates (1) user QoE perception interface and (2) adaption algorithm with flexible parameters. To avoid overheads for updating parameters online, we model video streaming as a renewal system and formulate specific QoE function into flexible formats by setting constraints on corresponding QoE metrics. To validate parameter setting, we emulate Elephanta under 5 thousand throughput traces including FCC broadband, 3G HSDPA data set from the Internet and 4G/LTE data set collected by ourselves. Accordingly, we implement Elephanta in dash.js at client side for user test. Evaluation results show that Elephanta achieves QoE improvement by 21.1% over MPC, in part for its superior adaptability to QoE diversity."
Influence Maximization at Community Level: A New Challenge with Non-submodularity.,"Motivated by various settings, we study a new Influence Maximization problem at the Community level (IMC) which aims at finding k users to maximize the benefit of influenced communities where a community is influenced iff the number of influenced users belong to this community exceeds its predefined threshold. In general, IMC objective function is not submodular nor supermodular, thereby making it very challenging to apply existing greedy solutions of the classic influence maximization (IM) where submodular function is required. Furthermore, the major challenge in the traditional methods for any related IM problem is the inefficiency in estimating the influence spread. IMC brings this difficulty to a higher level when considering influenced communities instead of influencing each individual user. In this paper, we propose different approximation algorithms for IMC: (1) Using Sandwich approach with a tight submodular function to bound the IMC objective function, (2) Activating the top-k influencing nodes found from network sampling. Furthermore, when the activated thresholds of communities are bounded by a constant, we propose an algorithm with performance guarantee tight to the inapproximability of IMC assuming the exponential time hypothesis. Each algorithm has its own strengths in a trade-off between effectiveness and running time, which are illustrated both in theory and comprehensive experimental evaluation."
The Energy-Data Dual Coverage in Battery-free Sensor Networks.,"Battery-free sensor network is a new network architecture in which battery-free nodes can harvest energy from the ambient environment and the network lifetime is unlimited in terms of energy. However, the unbalanced ambient energy distribution results in a low data coverage quality of the battery-free sensor network. In this paper, we want to deploy artificial power stations in the monitored region, improve the energy distribution and achieve the energy-data dual coverage. We have investigated two energy-data dual coverage problems in this paper to meet different user requirements. We have proved that these two problems are at least NP-Hard. Two approximated algorithms are proposed accordingly to solve these two problems. The ratio bound and time complexity of these two algorithms are analyzed. Furthermore, extensive simulations are carried out to evaluate the performance of the algorithms. The experimental results show that the algorithms are effective and efficient."
Streaming Submodular Maximization Under Noises.,"Motivated by the need for analyzing the rapidly producing data streams, such as images, videos, sensor data, etc, in a timely manner, the study on the streaming algorithms to extract representative information from massive data to maximize some objective function is therefore important and urgent. Most of previous works are assumed under a noise-free environment, while in many realistic applications obtaining the exact function value is hard or computing the function value may cost much, which brings the noisy version. Hence in this paper, we address a more general problem to select a subset of at most k elements from the stream to maximize a noisy set function (not necessarily submodular). To be specific, we cast our problem as the streaming submodular maximization problem under multiplicative and additive noise models. We develop an efficient thresholding streaming algorithm, which calls several copies of a subroutine in parallel. Therefore, this algorithm only requires two passes over data and has a memory independent of data size. For both of noisy models, its approximation guarantee approaches 2/k. In our numerical experiments, we extensively evaluate the effectiveness of our thresholding streaming algorithm on some applications in real data set."
The Power of Better Choice: Reducing Relocations in Cuckoo Filter.,"Efficient set representation and membership testing are important in various big data applications. The state-of-the-art Cuckoo filter design shows great advantages in both query efficiency and the support of item deletion, compared to previous Bloom filter and its variants. However, in this work, we show mathematically and experimentally that Cuckoo filter may suffer serious performance degradation during element insertion because of its random choice strategy for inserting an element into the candidate buckets. Such a random choice strategy incurs load imbalance among different buckets in Cuckoo filter and can lead to frequent relocations and the consequent long time for inserting an item. To solve this problem, we propose a novel design which leverages the principle of the power of two choices to select the better candidate bucket during inserting an element. Our design balances the load distribution among buckets in Cuckoo filter and avoids a large amount of relocations during insertion. We implement our design and apply it in real-world applications. We conduct comprehensive experiments using large-scale data sets collected from real world systems to evaluate the performance of this design. The results show that our design significantly reduces the average number of relocations of Cuckoo filter by 35%, as well as reducing item inserting latency by 25%."
The Communication Cost of Information Spreading in Dynamic Networks.,"This paper investigates the message complexity of distributed information spreading in adversarial dynamic networks. While distributed computations in dynamic networks have been studied intensively over the last years, almost all of the existing work solely focuses on the time complexity of distributed algorithms. In information spreading, the goal is to spread k tokens of information to every node on an n-node network. We consider the amortized (average) message complexity of spreading a token, assuming that the number of tokens is large. In a static network, this basic problem can be solved using (asymptotically optimal) O(n) amortized messages per token. Our focus is on token-forwarding algorithms, which do not manipulate tokens in any way other than storing, copying, and forwarding them. We present two sets of results depending on how nodes send messages to their neighbors: 1. Local broadcast: We show a tight lower bound of Ω̃(n
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
) on the number of amortized local broadcasts, which is matched by the naive flooding algorithm. The lower bound holds for randomized algorithms against a strongly adaptive adversary. 2. Unicast: We study the message complexity as a function of the number of dynamic changes in the network. To facilitate this, we introduce adversary-competitive message complexity as a natural complexity measure for analyzing dynamic networks: The adversary pays a unit cost for every topological change and the message cost of an algorithm is determined as the actual number of messages sent minus the total cost of the adversary. Under this model, we give a deterministic algorithm that obtains an optimal amortized message complexity of O(n) if the number of tokens k is sufficiently large. We also present a randomized algorithm that achieves subquadratic amortized message complexity for much smaller k under an oblivious adversary."
Self-Stabilizing Distributed Cooperative Reset.,"We propose a self-stabilizing reset algorithm working in anonymous networks. This algorithm resets the network in a distributed non-centralized manner, as each process detecting an inconsistency may initiate a reset. It is also cooperative in the sense that it coordinates concurrent reset executions in order to gain efficiency. Our approach is general since our reset algorithm allows to build self-stabilizing solutions for various problems and settings. As a matter of fact, it applies to both static and dynamic specifications since we propose efficient self-stabilizing reset-based algorithms for the 1-minimal (f,g)-alliance (a generalization of the dominating set problem) in identified networks and the unison problem in anonymous networks. These two latter instantiations enhance the state of the art. Indeed, in the former case, our solution is more general than the previous ones; while in the latter case, the time complexity of the proposed unison algorithm is better than that of previous ones."
DMRA: A Decentralized Resource Allocation Scheme for Multi-SP Mobile Edge Computing.,"Mobile Edge Computing (MEC) is a burgeoning paradigm that pushes data and services away from remote clouds to distributed Base Stations (BSs) equipped with MEC servers, which are deployed by Service Providers (SPs) at the edge of cellular networks. Normally, a SP prefers to use its own BSs, instead of those deployed by other SPs, to provide data and storage services. This can not only improve the quality of user experience but also increase its own revenue. In a densely deployed MEC network where a User Equipment (UE) tends to be covered by multiple BSs from varied SPs, how to allocate the resources in the BSs to provide the best service is a challenging problem. In this paper, we propose a novel resource allocation scheme, Decentralized Multi-SP Resource Allocation (DMRA), for densely-deployed MEC networks in order to maximize the total profit of all SPs and provide high-quality services. Our experimental results indicate that the proposed scheme outperforms the existing resource allocation algorithms for MEC."
Threshold-Based Widespread Event Detection.,"Widespread event detection is a fundamental network function that has many important applications in cybersecurity, traffic engineering, and distributed data mining. This paper introduces a new probabilistic threshold-based event detection problem, which is to find all events that appear in any w-out-of-a monitors with probabilistic guarantee on false positives, where a is the total number of monitors and the threshold w(≤ a) is a positive integer parameter that can be arbitrarily set, according to specific application requirements. We develop an efficient threshold filter solution and its improved versions, which combine Bloom filters, counting Bloom filter, threshold filter and compressed filters in a series of encoding and filtering steps, providing tradeoff between detection accuracy and communication overhead. We theoretically optimize the system parameters in the proposed solutions to minimize the communication overhead under the constraint of probabilistic detection guarantee. Extensive simulations demonstrate the practical viability of the proposed solutions in their ability of finding widespread events in a large network with few false positives and low communication overhead."
Efficient Distributed Community Detection in the Stochastic Block Model.,"Designing effective algorithms for community detection is an important and challenging problem in large-scale graphs, studied extensively in the literature. Various solutions have been proposed, but many of them are centralized with expensive procedures (requiring full knowledge of the input graph) and have a large running time. In this paper, we present a distributed algorithm for community detection in the stochastic block model (also called planted partition model), a widely-studied and canonical random graph model for community detection and clustering. Our algorithm called CDRW(Community Detection by Random Walks) is based on random walks, and is localized and lightweight, and easy to implement. A novel feature of the algorithm is that it uses the concept of local mixing time to identify the community around a given node. We present a rigorous theoretical analysis that shows that the algorithm can accurately identify the communities in the stochastic block model and characterize the model parameters where the algorithm works. We also present experimental results that validate our theoretical analysis. We also analyze the performance of our distributed algorithm under the CONGEST distributed model as well as the k-machine model, a model for large-scale distributed computations, and show that it can be efficiently implemented."
Incentivizing Microservices for Online Resource Sharing in Edge Clouds.,"The microservice architecture provides high agility, making it a suitable choice for implementing edge cloud services. Provisioning microservices at the network edge requires the dynamic allocation of resources. However, due to the resource limitation in the edge cloud environment, there is no guarantee that enough resources are always available upon a microservice's requests. In this paper, we design an online auction-based mechanism to incentivize microservices to spare their occupied resources so that the edge cloud platform can reclaim them and reallocate them to other microservices that need resources. We firstly design a single-stage auction that determines the winning bids to satisfy the resource demands in polynomial time, while calculating the payments. Then, we design an online framework to tie a series of such single-stage auctions into a multi-stage online mechanism without requiring the knowledge of future bids and demands. Via rigorous analysis, we exhibit that our mechanism design achieves truthful bidding and individual rationality, with a constant competitive ratio regarding the social cost of the system in the long run. Finally, we verify the practical performance of our mechanism through extensive simulations."
Non-stationary Stochastic Network Optimization with Imperfect Estimations.,"We investigate the problem of stochastic network optimization in presence of non-stationarity and estimations of average states in the future. Specifically, we first prove that the widely-used Drift and Penalty Algorithm in the Lyapunov optimization framework works well for non-stationary systems with periodical states. However, when the system is not periodical, non-stationarity may lead to severe performance degradation, which motivates the design of a novel, online algorithm named DPNP that incorporates the estimations of average future states into the stochastic optimization framework for decision making. DPNP is an online algorithm that requires zero a-prior distributional information about estimation errors. DPNP not only has near-optimal theoretical performance guarantees, but also outperforms existing Drift and Penalty Algorithm in numerical simulations. The improvement of DPNP highlights the importance of combining historic and future state estimations in non-stationary stochastic network optimization."
Towards Maximal Service Profit in Geo-Distributed Clouds.,"With the proliferation of globally-distributed services and the quick growth of user requests for inter-datacenter bandwidth, cloud providers have to lease a good deal of bandwidth from Internet service providers to satisfy the user demands. Neither maximizing the service revenue nor minimizing the service cost can bring the maximal service profit to cloud providers. The diversity of user requests and the large unit of inter-datacenter bandwidth further increase the difficulty of scheduling user requests. In this paper, we propose a cloud operational model to help cloud providers to make more service profit by properly selecting requests to serve rather than serving all user requests. We formulate the problem of service profit maximization and prove its NP-hardness. Considering the complicated coupling between maximizing revenue and minimizing cost, we propose a framework, Metis, for the efficient scheduling of user requests over inter-datacenter networks to maximize the service profit for cloud providers. Metis is formed with the alternate operations of two algorithms derived from randomized rounding techniques and Chernoff-Hoeffding bound. We prove that they can provide the guarantees on approximation ratios. Our extensive evaluations demonstrate that Metis can achieve more than 1.3x the service profits of existing solutions."
Maintaining Social Connections through Direct Link Placement in Wireless Networks.,"Mobile Social Network (MSN), built on inter-connected mobile devices, enables the flexibility of information exchanges of individuals within a virtual community. MSN may be self-organized and/or infrastructure-less, and the communication links may frequently fluctuate. When link qualities degrade, it remains critical to maintain the connections of important social pairs when supporting all social pairs is impossible. To achieve this goal, we propose to proactively place some reliable links (e.g., satellite links or UAV links) into the underlying communication network, so as to improve the service quality of the important social pairs, referred to as the Maintaining Social Connections (MSC) problem. We formulate the MSC problem and prove it is NP-hard. As such, we first study a special case of MSC, which is submodular and solvable with high approximation-ratio. Since the general MSC problem is not submodular, we propose an efficient approximation algorithm. Specially, by carefully choosing two submodular functions to lower/upper bound the MSC problem, we are able to prove the high approximation ratio of the proposed algorithm using these selected submodular functions. We further develop evolutionary algorithms to iteratively adjust the link placement via different exploration strategies, which also yield guaranteed approximation-ratio. Extensive evaluations based on both synthetic and real-world social network traces demonstrate the effectiveness of our proposed algorithms."
One for All and All for One: Scalable Consensus in a Hybrid Communication Model.,"This paper addresses consensus in an asynchronous model where the processes are partitioned into clusters. Inside each cluster, processes can communicate through a shared memory, which favors efficiency. Moreover, any pair of processes can also communicate through a message-passing communication system, which favors scalability. In such a “hybrid communication” context, the paper presents two simple binary consensus algorithms (one based on local coins, the other one based on a common coin). These algorithms are straightforward extensions of existing message-passing randomized round-based consensus algorithms. At each round, the processes of each cluster first agree on the same value (using an underlying shared memory consensus algorithm), and then use a message-passing algorithm to converge on the same decided value. The algorithms are such that, if all except one processes of a cluster crash, the surviving process acts as if all the processes of its cluster were alive (hence the motto “one for all and all for one”). As a consequence, the hybrid communication model allows us to obtain simple, efficient, and scalable fault-tolerant consensus algorithms. As an important side effect, according to the size of each cluster, consensus can be obtained even if a majority of processes crash."
Deterministic Contention Resolution on a Shared Channel.,"A shared communication channel (also known as a multiple access channel) is among the most popular and widely studied models of communication and distributed computing. In this model, stations are able to communicate by transmitting and listening to a shared channel. A fundamental problem, called contention resolution, is to allow any station to successfully deliver its message by resolving the conflicts that arise when several stations transmit simultaneously on the channel. Despite a long history, many fundamental questions remain open in the realistic scenario when up to k stations out of n join the channel at different times. In this work we explore the impact of asynchrony, knowledge (or linear estimate) of contenders, and acknowledgments, on latency and channel utilization of non-adaptive deterministic algorithms. We show that if the number of contenders k (or a linear upper bound on it) is known and the stations switch-off after acknowledgment of their successful transmissions, the channel admits efficient solutions. In the same settings, we show that the ignorance of contention k makes the channel nearly quadratically less efficient, even if the stations could switch-off after acknowledgments. We present an algorithm which nearly matches this complexity (for unknown k) which is achieved even if acknowledgments are not provided. We show how the above algorithm could be further improved if stations could switch off upon acknowledgment. Surprisingly, our results imply an exponential impact of knowledge of contention on deterministic utilization of asynchronous channel by deterministic algorithms - it is known that for synchronized channel this feature does not influence asymptotically the channel utilization. The second implication concerns the impact of acknowledgments - they exponentially improve deterministic channel utilization if (some estimate of) k is known, unlike in the case of randomized algorithms where the improvement is only polynomial, while they are not particularly helpful in case of unknown contention. Finally, note that non-adaptive algorithms use fixed transmission schedules, which could be naturally translate into codes in the radio or beeping model - in this context our results indicate under which conditions such codes could be efficient."
On Consistency of Graph-based Semi-supervised Learning.,"Graph-based semi-supervised learning is one of the most popular methods in machine learning. Some of its theoretical properties such as bounds for the generalization error and the convergence of the graph Laplacian regularizer have been studied in computer science and statistics literature. However, a fundamental statistical property - consistency - that is, the prediction by the algorithm can identify the underlying truth with unlimited data. This is not to be confused with the existence of solutions in an equation system, which is a term used in algebra. - has not been proved. In this article, we study the consistency problem under a non-parametric framework. We obtain the following two results: 1) We prove that graph-based semi-supervised learning on the test data is consistent in the case that the estimated scores are enforced to be equal to the observed responses for the labeled data (the hard criterion). The sample size of unlabeled data are allowed to grow at a slower rate than the size of the labeled data in this result. 2) We give a counterexample demonstrating that the estimator can be inconsistent for the case when the estimated scores are not required to be equal to the observed responses (the soft criterion), where a tuning parameter is used to balance the loss function and the graph Laplacian regularizer. These somewhat surprising theoretical findings are supported by numerical studies on both synthetic and real datasets. Moreover, numerical studies show that the hard criterion constantly outperforms the soft criterion even when the sample size of unlabeled data is smaller than the size of labeled data. This suggests that practitioners can safely choose the hard criterion without the burden of selecting the tuning parameter in the soft criterion."
Distributed Traffic Engineering for Multi-Domain Software Defined Networks.,"The increasing scale of software defined networks (SDN) raises the requirement of distributed control plane, for providing scalable, reliable and high performance network management capabilities. In particular, the flat design of distributed control plane enables the management of networks with multiple independent domains that are incapable of deploying a root controller. However, it is very difficult to avoid policy conflicts between multiple controllers in flat plane due to the lack of arbitration. In this paper, we address the problem of traffic engineering in a flat control plane, and design a distributed traffic engineering algorithm, called DisTE, which can provide max-min fair bandwidth allocation for flows and maximize the resource utilization, using a fully distributed arbitration mechanism. DisTE also preserves the local topology of each domain using the topology aggregation method, and supports consistency by multiple rounds of synchronizations. We examine four strategies for determining the synchronization timings, and find that linearly decreasing interval method provides a better trade-off between network utilization and time costs. Experiments on a 717-switches 5-domain network topology demonstrate that DisTE could drive the link utilization ratio to more than 93%, and reduce up to 95% convergence time at cost of 3% relative error on fairness, compared to the centralized approach."
A Cyclic Game for Joint Cooperation and Competition of Edge Resource Allocation.,"Managing edge resources is one of key issues in edge computing. Recent works of resource allocation in edge computing focus on service caching and request scheduling on edge nodes by distributing tasks over cloud and edge nodes, so as to achieve high-quality services and low latency. Unfortunately, most schemes do not pay enough attention on service providers which also have their own resources. To serve users, service providers cooperate with edge nodes and are sometimes independent of the edge operators with their own objectives. To deal with the cooperation and conflict among users, service providers and edge nodes, we propose a cyclic selection model to depict the resource allocation among users, edge nodes and service providers where they cooperate for completing user requests and compete for their own interest. Such resource allocation problem can be formulated as a non-linear integer programming which is very difficult to solve. Thus, we propose a three-sided cyclic game (3CG) involving users, edge nodes, and service providers who make their decisions: users select preferred services, service providers select cost-effective edge nodes, edge nodes select high-value users. 3CG is proved to have pure-strategy Nash equilibriums and an approximation ratio. We develop central and distributed approximate algorithms for resource allocation. The evaluation results of 3CG show the effectiveness and efficiency of the proposed algorithms."
Cooper: Cooperative Perception for Connected Autonomous Vehicles Based on 3D Point Clouds.,"Autonomous vehicles may make wrong decisions due to inaccurate detection and recognition. Therefore, an intelligent vehicle can combine its own data with that of other vehicles to enhance perceptive ability, and thus improve detection accuracy and driving safety. However, multi-vehicle cooperative perception requires the integration of real world scenes and the traffic of raw sensor data exchange far exceeds the bandwidth of existing vehicular networks. To the best our knowledge, we are the first to conduct a study on raw-data level cooperative perception for enhancing the detection ability of self-driving systems. In this work, relying on LiDAR 3D point clouds, we fuse the sensor data collected from different positions and angles of connected vehicles. A point cloud based 3D object detection method is proposed to work on a diversity of aligned point clouds. Experimental results on KITTI and our collected dataset show that the proposed system outperforms perception by extending sensing area, improving detection accuracy and promoting augmented results. Most importantly, we demonstrate it is possible to transmit point clouds data for cooperative perception via existing vehicular network technologies."
OptChain: Optimal Transactions Placement for Scalable Blockchain Sharding.,"A major challenge in blockchain sharding protocols is that more than 95% transactions are cross-shard. Not only those cross-shard transactions degrade the system throughput but also double the confirmation time, and exhaust an already scarce network bandwidth. Are cross-shard transactions imminent for sharding schemes? In this paper, we propose a new sharding paradigm, called OptChain, in which cross-shard transactions are minimized, resulting in almost twice faster confirmation time and throughput. By treating transactions as a stream of nodes in an online graph, OptChain utilizes a lightweight and on-the-fly transaction placement method to group both related and soon-related transactions into the same shards. At the same time, OptChain maintains a temporal balance among shards to guarantee the high parallelism. Our comprehensive and large-scale simulation using Oversim P2P library confirms a significant boost in performance with up to 10 folds reduction in cross-shard transactions, more than twice reduction in confirmation time, and 50% increase in throughput. When combined with Omniledger sharding protocol, OptChain delivers a 6000 transactions per second throughput with 10.5s confirmation time."
Fast Fault-Tolerant Sampling via Random Walk in Dynamic Networks.,"We study the fundamental problem of fault-tolerant distributed sampling towards uniform probabilistic distribution in dynamic multi-hop wireless networks. Whereas uniform sampling has been extensively studied without concerning fault tolerance, only quite few proposals investigate how the uniform sampling algorithm tolerate Byzantine faults on dynamic networks with very special topologies, e.g., regular graphs with constant node degree. Therefore, designing fault-tolerant uniform sampling algorithms for more general graphs is still an open problem. To this end, we propose a fast and highly fault-tolerate randomized algorithm, such that nearly-uniform sampling is achieved in O(log
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
 n) rounds, while up to O(√n/(polylog(n)·Δ)) Byzantine nodes can be tolerated, where Δ is the maximum degree of the network. Moreover, the proposed algorithm is also communication efficient in the sense that only O(log n) bits need to be exchanged on each link in every round. To show the power of distributed uniform sampling, we apply the proposed algorithm in designing polylogarithmic time distributed algorithms for two typical fundamental issues, i.e., to achieve agreement or data aggregation in Byzantine dynamic networks."
Dependable Policy Enforcement in Traditional Non-SDN Networks.,"Middleboxes are widely used in modern net-works for a variety of network functions in cybersecurity, performance enhancement, and monitoring. Middlebox policy enforcement is however complex and tedious with unreliable manual re-configuration of legacy routers. The existing solution on automated policy enforcement relies on software-defined networking and does not apply to the traditional non-SDN net-works, which remain popular today in enterprise deployment and core networks. This paper proposes a new architecture based entirely on software-defined middleboxes (instead of using software-defined switches in the prior art) to enable dependable and automated policy enforcement in non-SDN networks whose routers forward packets based on traditional routing protocols that are not policy-sensitive. We present a hot-potato enforcement strategy, which is then enhanced with two optimizations for load-balanced policy enforcement. Further enhancements are made to relieve middlebox processing overhead and avoid packet fragmentation due to policy enforcement."
Heterogeneity-aware Gradient Coding for Straggler Tolerance.,"Gradient descent algorithms are widely used in machine learning. In order to deal with huge volume of data, we consider the implementation of gradient descent algorithms in a distributed computing setting where multiple workers compute the gradient over some partial data and the master node aggregates their results to obtain the gradient over the whole data. However, its performance can be severely affected by straggler workers. Recently, some coding-based approaches are introduced to mitigate the straggler problem, but they are efficient only when the workers are homogeneous, i.e., having the same computation capabilities. In this paper, we consider that the workers are heterogenous which are common in modern distributed systems. We propose a novel heterogeneity-aware gradient coding scheme which can not only tolerate a predetermined number of stragglers but also fully utilize the computation capabilities of heterogenous workers. We show that this scheme is optimal when the computation capabilities of workers are estimated accurately. A variant of this scheme is further proposed to improve the performance when the estimations of the computation capabilities are not so accurate. We conduct our schemes for gradient descent based image classification on QingCloud clusters. Evaluation results show that our schemes can reduce the whole computation time by up to 3× compared with a state-of-the-art coding scheme."
ezBFT: Decentralizing Byzantine Fault-Tolerant State Machine Replication.,"We present ezBFT, a novel leaderless, distributed consensus protocol capable of tolerating byzantine faults. ezBFT's main goal is to minimize the client-side latency in WAN deployments. It achieves this by (i) having no designated primary replica, and instead, enabling every replica to order the requests that it receives from clients; (ii) using only three communication steps to order requests in the common case; and (iii) involving clients actively in the consensus process. In addition, ezBFT minimizes the potentially negative effect of a byzantine replica on the overall system performance. We developed ezBFT's formal specification in TLA+, show that it provides the classic properties of BFT protocols including consistency, stability, and liveness, and developed an implementation. Our experimental evaluation reveals that ezBFT improves client-side latency by as much as 40% over state-of-the-art byzantine fault-tolerant protocols including PBFT, FaB, and Zyzzyva."
Reo: Enhancing Reliability and Efficiency of Object-based Flash Caching.,"The fast-pace advancement of flash technology has enabled us to build a very large-capacity cache system at a low cost. However, the reliability of flash devices still remains a non-negligible concern, especially for flash-based cache. This is for two reasons. First, corruption of dirty data in cache would cause a permanent loss of user data. Second, warming up a huge-capacity cache would take an excessively long period of time. In this paper, we present a highly reliable, efficient, object-based flash cache, called Reo. Reo is designed to leverage the highly expressible object interface to exploit the rich semantic knowledge of the flash cache manager. Reo has two key mechanisms, differentiated data redundancy and differentiated data recovery, to make a flash cache highly reliable, and in the meantime, still remains space efficient for high cache hit ratio. We have prototyped Reo based on open-osd, an open-source implementation of T10 Object Storage Device (OSD) in Linux. Our experimental results show that compared to uniform data protection, Reo achieves graceful performance degradation and prioritized recovery upon device failures. Compared to full replication, Reo is more space efficient and delivers up to 3.1 times of the cache hit ratio and up to 3.6 times of the bandwidth."
SSS: Scalable Key-Value Store with External Consistent and Abort-free Read-only Transactions.,"We present SSS, a scalable transactional key-value store deploying a novel distributed concurrency control that provides external consistency for all transactions, never aborts read-only transactions due to concurrency, all without specialized hardware. SSS ensures the above properties without any centralized source of synchronization. SSS's concurrency control uses a combination of vector clocks and a new technique, called snapshot-queuing, to establish a single serialization order where transactions are guaranteed to read from the latest non-concurrent transaction externally visible to clients. We compare SSS against high performance key-value stores, Walter, ROCOCO, and a two-phase commit baseline. SSS outperforms 2PC-baseline by as much as 7x using 20 nodes; and ROCOCO by as much as 2.2x with long read-only transactions using 15 nodes."
Detecting Malicious Domains with Behavioral Modeling and Graph Embedding.,"The last decade has witnessed the explosive growth of malicious Internet domains which serve as the fundamental infrastructure for establishing advanced persistent threat command and control communication channels or hosting phishing Web sites. Given the big data nature of Internet traffic data and the ability of algorithmically generating domains and acquiring and registering the domains in a near-automated fashion, detecting malicious domains in real-time is a daunting task for security analysts and network operators. In this paper, we introduce bipartite graphs to capture the interactions between end hosts and domains, identify associated IP addresses of domains, and characterize time-series patterns of DNS queries for domains, and explore one-mode projections of these bipartite graphs for modeling the behavioral, IP-structural, and temporal similarities between domains. We employ graph embedding technique to automatically learn dynamic and discriminative feature representations for over 10,000 labeled domains, and develop an SVM-based classification algorithm for predicting malicious or benign domains. Our model makes the progress towards adapting to the changing and evolving strategies of malicious domains. The experimental results have shown that our proposed algorithm achieves an area under the curve (AUC) of 0.94 based on k-fold cross-validation. To the best of our knowledge, this is the first effort to apply the combination of behavioral modeling and graph embedding for effectively and accurately detecting malicious domains."
TFix: Automatic Timeout Bug Fixing in Production Server Systems.,"Timeout is widely used to handle unexpected failures in distributed systems. However, improper use of timeout schemes can cause serious availability and performance issues, which is often difficult to fix due to lack of diagnostic information. In this paper, we present TFix, an automatic timeout bug fixing system for correcting misused timeout bugs in production systems. TFix adopts a drill-down bug analysis protocol that can narrow down the root cause of a misused timeout bug and producing recommendations for correcting the root cause. TFix first employs a system call frequent episode mining scheme to check whether a timeout bug is caused by a misused timeout variable. TFix then employs application tracing to identify timeout affected functions. Next, TFix uses taint analysis to localize the misused timeout variable. Last, TFix produces recommendations for proper timeout variable values based on the tracing results during normal runs. We have implemented a prototype of TFix and conducted extensive experiments using 13 real world server timeout bugs. Our experimental results show that TFix can correctly localize the misused timeout variables and suggest proper timeout values for fixing those bugs."
Near Optimal Charging Scheduling for 3-D Wireless Rechargeable Sensor Networks with Energy Constraints.,"Wireless Rechargeable Sensor Network (WRSN) becomes a hot research issue in recent years owing to the breakthrough of wireless power transfer technology. Most prior arts concentrate on developing scheduling schemes in 2-D networks where mobile chargers are placed on the ground. However, few of them are suitable for 3-D scenarios, making it difficult or even impossible to popularize in practical applications. In this paper, we focus on the problem of charging a 3-D WRSN with an Unmanned Aerial Vehicle (UAV) to maximize charged energy within energy constraints. To deal with the problem, we propose a spatial discretization scheme to obtain a finite feasible charging spot set for UAV in 3-D environment and a temporal discretization scheme to determine charging duration for each charging spot. Then, we transform the problem into a submodular maximization problem with routing constraints, and present a cost-efficient approximation algorithm with a provable approximation ratio of e-1/4e(1-ε) to solve it. Lastly, extensive simulations and test-bed experiments show the superior performance of our algorithm."
Toward Efficient Compute-Intensive Job Allocation for Green Data Centers: A Deep Reinforcement Learning Approach.,"Reducing the energy consumption of the servers in a data center via proper job allocation is desirable. Existing advanced job allocation algorithms, based on constrained optimization formulations capturing servers' complex power consumption and thermal dynamics, often scale poorly with the data center size and optimization horizon. This paper applies deep reinforcement learning (DRL) to build an allocation algorithm for long-lasting and compute-intensive jobs that are increasingly seen among today's computation demands. Specifically, a deep Q-network is trained to allocate jobs, aiming to maximize a cumulative reward over long horizons. The training is performed offline using a computational model based on long short-term memory networks that capture the servers' power and thermal dynamics. This offline training approach avoids slow online convergence, low energy efficiency, and potential server overheating during the DRL's extensive state-action space exploration if it directly interacts with the physical data center in the usually adopted online learning scheme. At run time, the trained Q-network is forward-propagated with little computation to allocate jobs. Evaluation based on 8 months' physical state and job arrival records from a national supercomputing data center hosting 1,152 processors shows that our solution reduces computing power consumption by nearly 10% and processor temperature by more than 3°C without sacrificing job processing throughput."
DeepEE: Joint Optimization of Job Scheduling and Cooling Control for Data Center Energy Efficiency Using Deep Reinforcement Learning.,"The past decade witnessed the tremendous growth of power consumption in data centers due to the rapid development of cloud computing, big data analytics, and machine learning, etc. The prior approaches that optimize the power consumption of the information technology (IT) system and/or the cooling system always fail to capture the system dynamics or suffer from the complexity of system states and action spaces. In this paper, we propose a Deep Reinforcement Learning (DRL) based optimization framework, named DeepEE, to improve the energy efficiency for data centers by considering the IT and cooling systems concurrently. In DeepEE, we first propose a PArameterized action space based Deep Q-Network (PADQN) algorithm to solve the hybrid action space problem and jointly optimize the job scheduling for the IT system and the airflow rate adjustment for the cooling system. Then, a two-time-scale control mechanism is applied in PADQN to coordinate the IT and cooling systems more accurately and efficiently. In addition, to train and evaluate the proposed PADQN in a safe and quick way, we build a simulation platform to model the dynamics of IT workload and cooling systems simultaneously. Through extensive real-trace based simulations, we demonstrate that: 1) our algorithm can save up to 15% and 10% energy consumption in comparison with the baseline siloed and joint optimization approaches respectively; 2) our algorithm achieves more stable performance gain in terms of power consumption by adopting the parameterized action space; and 3) our algorithm leads to a better tradeoff between energy saving and service quality."
Collision-resistant Communication Model for State-free Networked Tags.,"Traditional radio frequency identification (RFID) technologies allow tags to communicate with a reader but not among themselves. By enabling peer-to-peer communications among nearby tags, the emerging networked tags make a fundamental enhancement to today's RFID systems. This new capability supports a series of system-level functions in previously infeasible scenarios where the readers cannot cover all tags due to cost or physical limitations. This paper makes the first attempt to design a new communication model that is specifically tailored to efficient implementation of system-level functions in networked tag systems, in terms of energy cost and execution time. Instead of exploiting complex mechanisms for collision detection and resolution, we propose a collision-resistant communication model (CCM) that embraces the collision in tag communications and utilizes it to merge the data from different sources in a benign way. Two fundamental applications: RFID estimation and missing-tag detection, are presented to illustrate how CCM assists efficient system-level operations in networked tag systems. Simulation results show that the system-level applications through CCM are able to reduce the energy cost and execution time by one order of magnitude, compared with the ID-collection based solution."
Goldilocks: Adaptive Resource Provisioning in Containerized Data Centers.,"Power management in data centers is challenging because of fluctuating workloads and strict task completion time requirements. Recent resource provisioning systems, such as Borg and RC-Informed, pack tasks on servers to save power. However, current power optimization frameworks based on packing leave very little headroom for spikes, and the task completion times are compromised. In this paper, we design Goldilocks, a novel resource provisioning system for optimizing both power and task completion time by allocating tasks to servers in groups. Tasks hosted in containers are grouped together by running a graph partitioning algorithm. Containers communicating frequently are placed together, which improves the task completion times. We also leverage new findings on power consumption of modern-day servers to ensure that their utilizations are in a range where they are power-proportional. Both testbed implementation measurements and large-scale trace-driven simulations prove that Goldilocks outperforms all the previous works on data center power saving. Goldilocks saves power by 11.7%-26.2% depending on the workload, whereas the best of the implemented alternatives, Borg, saves 8.9%-22.8%. The energy per request for the Twitter content caching workload in Goldilocks is only 33% of RC-Informed. Finally, the best alternative in terms of task completion time, E-PVM, has 1.17-3.29 times higher task completion times than Goldilocks across different workloads."
HyperEar: Indoor Remote Object Finding with a Single Phone.,"Finding a small object (e.g., keys or a wallet) in an indoor environment (e.g., in a house or an office) can be frustrating. In this paper, we propose an innovative system, called HyperEar, to localize such an object using only one single smartphone, based on enhanced time-difference-of-arrival (TDoA) measurements over acoustic signals issued from the object. One major challenge is the hardware limitations of a Commercial-Off-The-Shelf (COTS) phone with a short separation between the two microphones and the low sampling rate of such microphones. HyperEar enhances the accuracy of TDoA measurements by virtually increasing distances between microphones through sliding the phone in the air. HyperEar requires no communication for synchronization between the phone and the object and is a low-cost and easy-to-use system. We evaluate the performance of HyperEar via extensive experiments in various indoor conditions and the results demonstrate that, for an object of 7m away, HyperEar can achieve a mean localization accuracy of about 15cm when the object in normal indoor environments."
p^2Charging: Proactive Partial Charging for Electric Taxi Systems.,"Electric taxis (e-taxis) have been increasingly deployed in metropolitan cities due to low operating cost and reduced emissions. Compared to conventional taxis, e-taxis require frequent recharging and each charge takes half an hour to several hours, which may result in unpredictable number of working taxis on the street. In current systems, E-taxi drivers usually charge their vehicles when the battery level is below a certain threshold, and then make a full charge. Although this charging strategy directly decreases the number of charges and the time to visit charging stations, our study reveals that it also significantly reduces the availability of number of taxis during busy hours with our data driven analysis. To meet dynamic passenger demand, we propose a new charging strategy: proactive partial charging (p
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
 Charging), which allows an e-taxi to get partially charged before its remaining battery level is running too low. Based on this strategy, we propose a charging scheduling framework for e-taxis to meet dynamic passenger demand in spatial-temporal dimensions as much as possible while minimizing idle time to travel to charging stations and waiting time at charging stations. This work implements and evaluate our solution with large datasets that consist of (i) 7,228 regular internal combustion engine taxis and 726 e-taxis, (ii) an automatic taxi payment transaction collection system with total 62,100 records per day, (iii) charging station system, including 37 working charging stations over the city. The evaluation results show that p
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
 Charging improves the ratio of unserved passengers by up to 83.2% on average and increases e-taxi utilization by up to 34.6% compared with ground truth and existing charging strategies."
WiMi: Target Material Identification with Commodity Wi-Fi Devices.,"Target material identification is playing an important role in our everyday life. Traditional camera and video-based methods bring in severe privacy concerns. In the last few years, while RF signals have been exploited for indoor localization, gesture recognition and motion tracking, very little attention has been paid in material identification. This paper introduces WiMi, a device-free target material identification system, implemented on ubiquitous and cheap commercial off-the-shelf (COTS) Wi-Fi devices. The intuition is that different materials produce different amounts of phase and amplitude changes when a target appears on the line-of-sight (LoS) of a radio frequency (RF) link. However, due to multipath and hardware imperfection, the measured phase and amplitude of the channel state information (CSI) are very noisy. We thus present novel CSI pre-processing schemes to address the multipath and hardware noise issues before they can be used for accurate material sensing. We also design a new material feature which is only related to the material type and is independent of the target size. Comprehensive real-life experiments demonstrate that WiMi can achieve fine-grained material identification with cheap commodity Wi-Fi devices. WiMi can identify 10 commonly seen liquids at an overall accuracy higher than 95% with strong multipath indoors. Even for very similar items such as Pepsi and Coke, WiMi can still differentiate them at a high accuracy."
Modeling and Forecasting of Timescale Network Traffic Dynamics in M2M Communications.,"With an unparalleled number of Machine-to-Machine (M2M) devices being deployed to support a variety of smart-world systems powered by Internet of Things (IoT) technologies, the heterogeneity, uncertainty, and complexity of M2M communications have increased enormously. Thus, how to conduct network resource planning (NRP) has become a challenging issue. In this paper, we propose a novel time series framework (TSF) to model and forecast timescale network traffic dynamics in M2M communications that is capable of providing useful guidance for effective NRP. Specifically, our TSF utilizes the statistical techniques INGARCH(p,q) (integer valued generalized autoregressive conditional heteroskedasticity) and βARMA(p,q) (beta autoregressive moving average) to accurately capture both the internal and external impact factors of the asynchronous and synchronous M2M traffic dynamics over a large time scale, and produces forecasts for multiple upcoming time points by leveraging conditional maximum-likelihood estimators (CMLE). Through a combination of theoretical analysis and extensive simulation, we have validated the modeling and forecasting efficacy of TSF. Our experimental results demonstrate that TSF achieves superior performance with respect to goodness-of-fit and prediction accuracy."
Multi-Sensor Calibration Planning in IoT-Enabled Smart Spaces.,"Emerging applications in smart cities and communities require massive IoT deployments using sensors/actuators (things) that can enhance citizens' quality of life and public safety. However, budget constraints often lead to limited instrumentation and/or the use of low-cost sensors that are subject to drift and bias. This raises concerns of robustness and accuracy of the decisions made on uncertain data. To enable effective decision-making while fully exploiting the potential of low-cost sensors, we propose to send mobile units (e.g., trained personnel) equipped with high-quality (more expensive) and freshly-calibrated reference sensors so as to carry out calibration in the field. We design and implement an efficient cooperative approach to solve the calibration planning problem, which aims at minimizing the cost of the recurring calibration of multiple sensor types in the long-term operation. We propose a two-phase solution that consists of a sensor selection phase that minimizes the average cost of recurring calibration, and a path planning phase that minimizes the travel cost of multiple calibrators which have load constraints. We provide fast and effective heuristics for both phases. We further build a prototype that facilitates the mapping of the deployment field and provides navigation guidance to mobile calibrators. Extensive use-case-driven simulations show our proposed approach significantly reduces the average cost compared to naive approaches: up to 30% in a moderate-sized indoor case, and higher in outdoor cases depending on the scale."
Providing Reliability-Aware Virtualized Network Function Services for Mobile Edge Computing.,"Mobile Edge Computing (MEC) has emerged as a promising paradigm to address the conflict between increasing computing-intensive applications and resource-constrained mobile Internet-of-Thing (IoT) devices with portable size and storage. In MEC environments, Virtualized Network Functions (VNFs) are deployed for provisioning network services to users to reduce the service cost on top of dedicated hardware infrastructures. However, VNFs may suffer from failures and malfunctions while network service providers have to guarantee continuously reliable services to their users to meet ever-growing service demands of the users. In this paper, we focus on reliable VNF service provisioning in MECs, by provisioning primary and backup VNF instances in order to meet the reliability requirements of mobile users. We first formulate a novel VNF service reliability problem with the aim to maximize the revenue collected by admitting as many as user requests while meeting individual user service reliability requirements. We then develop two efficient on-line scheduling algorithms for the problem under two different backup schemes: on-site (local) and off-site (remote) schemes, by adopting the primal and dual updating technique. Particularly for the on-site scheme, the proposed on-line algorithm achieves a provable competitive ratio with bounded moderate resource violations. We finally evaluate the proposed algorithms through experimental simulations. The experimental results demonstrate that the proposed algorithms are promising, compared with existing baseline algorithms."
Understanding Energy Efficiency in IoT App Executions.,"Billions of Internet-of-Things (IoT) devices such as sensors, actuators, computing units, etc., are connected to form IoT platforms. However, it is observed that such hardware platforms today spend a significant proportion of their energy in communication between the CPU and the sensors (which are controlled by micro-controller unit (MCU)). Motivated by this observation, two simple, yet effective, optimizations are proposed to minimize the energy consumption. The first optimization, called Batching, interrupts the CPU after collecting multiple sensor data points at the MCU (instead of only 1), and thus, minimizes the interrupt overheads. The second optimization, called Computation Offloading to MCU (COM), offloads app-specific computations to the MCU to minimize data transfer overheads, and makes use of the relatively low energy footprint, and low-compute capabilities of the MCU in place of the CPU in the hub. However, questions such as why these two schemes are needed, where the energy benefit comes from, which IoT apps are suitable for these optimizations, etc., remain unclear. To better understand the Batching and COM approaches towards energy efficiency in IoT app executions, we characterize ten representative workloads on a Raspberry Pi and ESP8266 MCU platform, and evaluate the energy savings using these two optimizations and illustrate that for light-weight workloads (where COM is applicable), Batching and COM reduce the energy consumption by 52% and 85%, respectively when compared to the baseline. And for heavy-weight apps (where COM is not possible due to limited capacity of MCU), by offloading the light-weight apps and batching for the heavy-weight, Batching + COM (BCOM) benefits 10% energy savings compared to the baseline."
DÏoT: A Federated Self-learning Anomaly Detection System for IoT.,"IoT devices are increasingly deployed in daily life. Many of these devices are, however, vulnerable due to insecure design, implementation, and configuration. As a result, many networks already have vulnerable IoT devices that are easy to compromise. This has led to a new category of malware specifically targeting IoT devices. However, existing intrusion detection techniques are not effective in detecting compromised IoT devices given the massive scale of the problem in terms of the number of different types of devices and manufacturers involved. In this paper, we present DÏoT, an autonomous self-learning distributed system for detecting compromised IoT devices. DÏoT builds effectively on device-type-specific communication profiles without human intervention nor labeled data that are subsequently used to detect anomalous deviations in devices' communication behavior, potentially caused by malicious adversaries. DÏoT utilizes a federated learning approach for aggregating behavior profiles efficiently. To the best of our knowledge, it is the first system to employ a federated learning approach to anomaly-detection-based intrusion detection. Consequently, DÏoT can cope with emerging new and unknown attacks. We systematically and extensively evaluated more than 30 off-the-shelf IoT devices over a long term and show that DÏoT is highly effective (95.6% detection rate) and fast (257 ms) at detecting devices compromised by, for instance, the infamous Mirai malware. DÏoT reported no false alarms when evaluated in a real-world smart home deployment setting."
Road Gradient Estimation Using Smartphones: Towards Accurate Estimation on Fuel Consumption and Air Pollution Emission on Roads.,"Accurate estimations on vehicle fuel consumption and pollution emission on roads are important for vehicle velocity optimization and driving route planning. Existing methods for such estimations only consider vehicle driving speed and acceleration but neglect the influence of road gradient. This is mainly because the road gradients for most road networks are not available and none of existing methods for road gradient estimation can be conducted inexpensively in practice and keep high road gradient estimation accuracy simultaneously. Thus, how to estimate the road gradient conveniently and accurately is an important but challenging problem. To handle this challenge, we propose a new road gradient estimation system which estimates the road gradient only using a smartphone. When a vehicle is driving, a smartphone in the vehicle continuously measures vehicle states (velocity, acceleration, steering rate, position), which are used to estimate the road gradient. To eliminate measuring noise and drift noise, the deviation between the measured value and estimated value is used to adjust the estimated value. Since measured vehicle states when a vehicle changes lane adversely influence the accuracy of road gradient estimation, we design lane change detection to eliminate such influences. Finally, given a group of road gradient estimates for a given route, we use the track fusion algorithm to further eliminate measuring noise and drift noise and improve road gradient estimation accuracy. We conducted driving experiments in a city area to evaluate our system. The experimental results show that our system's estimation error is reduced by 22% compared with existing methods. The results also demonstrate the accuracy of our lane change detection. Finally, we integrated the road gradient values into vehicle fuel consumption and air pollution emission model to estimate fuel consumption and air pollution emission and found that the estimation values increase by 33.4% compared with the values without considering road gradient."
EchoWrite: An Acoustic-based Finger Input System Without Training.,"Recently, wearable devices have become increasingly popular in our lives because of their neat features and stylish appearance. However, their tiny sizes bring about new challenges to human-device interaction such as texts input. Although some novel methods have been put forward, they possess different defects and are not applicable to deal with the problem. As a result, we propose an acoustic-based texts-entry system, i.e., EchoWrite, by which texts can be entered with a finger writing in the air without wearing any additional device. More importantly, different from many previous works, EchoWrite runs in a training-free style which reduces the training overhead and improves system scalability. We implement EchoWrite with commercial devices and conduct comprehensive experiments to evaluate its texts-entry performance. Experimental results show that EchoWrite enables users to enter texts at a speed of 7.5 WPM without practice, and 16.6 WPM after about 30- minute practice. This speed is better than touch screen-based method on smartwatches, and comparable with previous related works."
Towards Energy-Fairness in LoRa Networks.,"LoRa has recently become one of the most promising networking technologies for the Internet of Things applications. Distant end devices have to use a low data rate to reach a LoRa gateway, which can cause long in-the-air transmission time and high energy consumption. Compared with the end devices using high data rate, they will drain the batteries much earlier and the network may be broken. Such an energy unfairness can be mitigated by deploying more gateways, since it allows end devices to reach closer gateways with higher data rates. However, multiple gateways may not solve the energy unfairness problem efficiently due to the collision problem caused by the chirp spread spectrum modulation of LoRa networks. Spreading factors of LoRa links can determine both data rate and multiplexing of different transmissions. With more gateways, more end devices may choose low spreading factors and reach closer gateways, which increase the collision probability. In this paper, we propose a networking solution for LoRa networks named EF-LoRa that can achieve fair energy consumption among end devices by carefully allocating different network resources, including frequency channels, spreading factors and transmission power, to achieve fair energy consumption among end devices in LoRa networks. We develop a LoRa network model to study the energy consumption of all the end devices in a network by considering the unique features of LoRa networks, such as LoRaWAN MAC protocol, spreading factors, interference, and the capacity limitation of a LoRa gateway. We formulate the energy fairness problem as an optimization problem and finally propose a greedy resource allocation algorithm to achieve the max-min fairness of energy efficiency in the LoRa networks. Simulation results show that the proposed solution EF-LoRa can improve the energy fairness of legacy LoRa networks by 177.8%."
CBMA: Coded-Backscatter Multiple Access.,"The ever-increasing number of IoT devices in our surrounding environment bring us tremendous amount of opportunities but also challenges including limited battery life, low computational capability and scalability of multiple access. Recent advances in backscatter communication have enabled ubiquitous IoT devices to communicate in a cost-and power-efficient way. However, most of the proposed backscatter solutions nowadays focus on the single tag paradigm, i.e., multiple tags do not transmit simultaneously and thus the solutions have difficulties to scale with a large number of tags. This work presents CBMA, a backscatter system that enables multiple concurrent backscatter tags to communicate reliably and efficiently. For the first time, we demonstrate that multiple tags can backscatter concurrently and efficiently with novel impedance-based power control at the tag, and can be successfully decoded with commodity WiFi devices without affecting the existing WiFi communication. We present the design details of CBMA and build a prototype with off-the-shelf WiFi devices and FPGA. The CBMA system achieves a 10-tag bit rate of 8Mbps while supporting a communication distance up to 10m. Compared to single-tag solutions, CBMA improves the backscatter throughput by more than 10× even in challenging indoor scenarios with rich multipath and interference."
Bundle Charging: Wireless Charging Energy Minimization in Dense Wireless Sensor Networks.,"Using a mobile charger to wirelessly charge sensors is a promising yet not well-solved technique. Existing trajectory planning schemes for wireless charger either (1) fail to optimize the one-to-many characteristic of wireless charging or (2) fail to jointly optimize the charger movement cost and the charging cost. The objective of this paper is to find the optimal trajectory planning for a mobile charger in terms of energy minimization in the quadratic attenuation charging model. There exists a trade-off between charging efficiency and trajectory distance. If the mobile charger comes close to sensors, the charging efficiency is high, but the entire charging trajectory of the charger will be long and vice versa. To address this trade-off, we propose the idea of charging bundle and optimize the charger's trajectory based on the charging bundle rather than each sensor. The optimal charging bundle generation problem and the bundle trajectory optimization problem are discussed gradually. Both of them are proven to be NP-hard. Then, we first propose a greedy bundle generation algorithm with an approximation ratio of lnn, where n is the number of sensors. After that, we propose a TSP-based solution and further optimize the TSP-trajectory by jointly considering the adjacent charging locations. Theorems are proposed to effectively find the optimal location. Extensive experiments show that our scheme achieves a much better performance than traditional schemes."
Online NFV-Enabled Multicasting in Mobile Edge Cloud Networks.,"Mobile Edge Computing (MEC) reforms the cloud paradigm by bringing unprecedented computing capacity to the vicinity of mobile users at the mobile network edge. This provides end-users with swift and powerful computing, energy efficiency, storage capacity, mobility-and context-awareness support. Furthermore, provisioning virtualized network services in MEC can improve user service experience, simplify network service deployments, and ease network resource management. However, user requests usually arrive into the system dynamically and different user requests may have different resource demands. How to optimize and guarantee the performance of MEC is of significant importance and challenging. In this paper, we study the problem of online NFV-enabled multicasting in an MEC network with resource capacity constraints on both cloudlets and links. We first devise an approximation algorithm for the cost minimization problem for a single NFV-enabled multicast request admission. We then propose an online algorithm with a provable competitive ratio for the online throughput maximization problem where NFV-enabled multicast requests arrive one by one without the knowledge of future request arrivals. We admit the requests through placing or sharing VNF instances of network functions in their service chains to meet their computing and bandwidth resource demands, and we introduce a novel cost model to capture the dynamic usages of different resources and perform network resource allocations based on the proposed cost model. We finally evaluate the performance of the proposed algorithms through experimental simulations. Simulation results demonstrate that the proposed algorithms are promising."
Heterogeneous Statistical QoS Driven Collaborative Learning Based Energy Harvesting Over Full-Duplex Cognitive Radio Networks.,"With the recent developments in energy harvesting (EH) technologies, mobile devices are able to support the wireless multimedia services by harvesting energy from various external sources. As one of the promising technologies to solve the energy scarcity problem, EH schemes have brought many new challenges due to the stochastic nature of the wireless channel and the harvested energy in supporting the statistical quality-of-service (QoS) provisionings. On the other hand, the full-duplex spectrum sensing (FD-SS) scheme has been designed to improve the spectrum efficiency while significantly enhancing the system performance over cognitive radio networks (CRNs). However, due to the unknown dynamics of the channel state information and the energy state information, it is challenging to design the efficient EH based power allocation policies for all the users with different operating modes while guaranteeing the statistical delay-bounded QoS constraints. To overcome the aforementioned problems, in this paper we develop the collaborative learning system model by choosing the optimal operation strategies and power allocation policies through learning from the EH process while satisfying the heterogeneous statistical delay-bounded QoS constraints over CRNs. In particular, we establish and analyze the FD-SS based EH system models over CRNs. Under the heterogeneous statistical delay-bounded QoS requirements, we formulate and solve the max-min fairness effective-capacity optimization problem for the battery-free EH based CRNs. Then, we apply the collaborative learning algorithm for deriving the optimal joint EH based mode selection and power allocation schemes. Also conducted is a set of simulations which evaluate the system performances and show that our proposed collaborative learning based EH scheme outperforms the other existing schemes under the heterogeneous statistical delay-bounded QoS constraints over CRNs."
Computation Offloading for Mobile-Edge Computing with Multi-user.,"New age smartphones are equipped with high processing power and internet connectivity. Hence, smartphones are capable of executing applications, which were only possible by desktops or laptops until recently. Some examples of such applications are email, banking, flight booking etc. People prefer to use mobile devices for these applications due to the usability and portability of mobile devices. However, because of hardware limitations, mobiles have limited resources such as battery life, power and capacity. Researchers are constantly looking for ways to maximize the usage of these resources. The execution of any application on mobile, needs storage capacity of mobile to store, battery life of mobile to keep running and processing capacity of mobile to process. Thus, more resources are needed to run more applications on these devices. To reduce the load of applications on mobile devices and use the resources efficiently, it is necessary to move some load of applications to remote cloud server in such a way that the applications will run seamlessly. Computation offloading for mobile-edge computing MEC) is a mechanism to utilize mobile resources well by moving resource-intensive applications to cloud server at network edge. In the case of multiple users, the total computing capacity of the server needs to be taken into consideration for allocating resources to multiple users. The key to efficient computation offloading is allocating applications to mobile and remote server in such a way that minimizes transmission energy. In this paper, we formulate the computation offloading problem as graph cut problem and propose a solution based on spectral clustering computation. First, for the applications on mobile a corresponding network flow graph model is defined. Then, label propagation theory is applied on the network graph and the network graph is simplified by compressing and combining. Finally, the optimal solution is obtained by computation using spectral clustering algorithm. Experiments show that the algorithm is effective in handling programs with loosely coupled as well as highly coupled functions."
Low-Latency Concurrent Broadcast Scheduling in Duty-Cycled Multihop Wireless Networks.,"Broadcasting is a fundamental networking service where the source node disseminates the message to all the other nodes. Unfortunately, the problem of Minimum Latency Broadcast Scheduling (MLBS) in duty-cycled wireless networks is not well studied. In the existing works, the construction of broadcast tree and the scheduling of transmissions are conducted separately, which may result in a bad-structured broadcast tree and then a large latency is obtained even using the optimal scheduling method. Thus, the MLBS problem in duty-cycled wireless networks without above limitation is investigated in this paper. Firstly, a Two-Step Scheduling algorithm is proposed to construct the broadcast tree and compute a collision-free schedule simultaneously. The proposed method can generate a latency-aware broadcast tree adaptively to reduce the broadcast latency. To the best of our knowledge, this is the first work that can integrate these two kinds of operations together. Additionally, a novel transmission mode, i.e., concurrent broadcasting, is first introduced in wireless networks and several techniques are designed to further improve the broadcast latency. Finally, the theoretical analysis and experimental results demonstrate the efficiency of the proposed algorithms in term of latency."
Energy-Aware and Context-Aware Video Streaming on Smartphones.,"Although streaming video at a higher bitrate (resolution) can lead to better Quality of Experience (QoE), a larger amount of data will have to be downloaded and processed on smartphones and thus consuming more energy. On a moving bus where the wireless signal is weak, more energy will have to be spent on maintaining high bitrate video streaming than at a static environment such as at home or a cafe where the wireless signal is strong. On the other hand, the user perceived QoE does not increase too much by watching high bitrate videos in a vibrating environment (i.e., a moving vehicle), because the perception of video quality is affected by the environment such as the vibration or shaking on a moving bus. To address this problem, we propose to save energy by considering the context (environment) of video streaming. To model the impact of context, we exploit the embedded sensors (e.g., accelerometer) in smartphones to record the vibration level during video streaming. Based on quality assessment experiments, we collect traces and model the impacts of video bitrate and vibration level on QoE, and model the impacts of video bitrate and signal strength on power consumption. Based on the QoE model and the power model, we formulate the energy-aware and context-aware video streaming problem as an optimization problem. We present an optimal algorithm which can maximize QoE and minimize energy. Since the optimal algorithm requires perfect knowledge of future tasks, we further propose an online bitrate selection algorithm. Through real measurements and trace-driven simulations, we demonstrate that the proposed algorithm can significantly outperform existing approaches when considering both energy and QoE."
A Sybil-Resistant Truth Discovery Framework for Mobile Crowdsensing.,"The rapid proliferation of sensor-embedded devices has enabled the mobile crowdsensing (MCS), a new paradigm which effectively collects sensing data from pervasive users. In order to identify the true information from the noisy data submitted by unreliable users, truth discovery algorithms have been proposed for the MCS systems to aggregate data. However, the power of truth discovery algorithms will be undermined by the Sybil attack, in which an attacker can benefit from using multiple accounts. In addition, an MCS system will be jeopardized unless it is resistant to the Sybil attack. In this paper, we proposed a Sybil-resistant truth discovery framework for MCS, which ensures high accuracy under the Sybil attack. To diminish the impact of the Sybil attack, we design three account grouping methods for the framework, which are used in pair with a truth discovery algorithm. We evaluate the proposed framework through a real-world experiment. The results show that existing truth discovery algorithms are vulnerable to the Sybil attack, and the proposed framework can effectively diminish the impact of the Sybil attack."
Minimizing the Longest Charge Delay of Multiple Mobile Chargers for Wireless Rechargeable Sensor Networks by Charging Multiple Sensors Simultaneously.,"Wireless energy charging has emerged as a very promising technology for prolonging sensor lifetime in Wireless Rechargeable Sensor Networks (WRSNs). Existing studies focused mainly on the 'one-to-one' charging scheme that a sensor can be charged by a single mobile charger at each time, this charging scheme however suffers from poor charging scalability and inefficiency. Recently, another charging scheme - the 'multiple-to-one' charging scheme that allows multiple sensors to be charged simultaneously by a single charger, becomes dominant and can mitigate charging scalability and improve the charging efficiency. Most research studies on this latter scheme focused on the use of a mobile charger to charge multiple sensors simultaneously. However, for large scale WRSNs, it is insufficient to deploy just a single mobile charger to charge many lifetime-critical sensors, and consequently sensor expiration durations will increase dramatically. Instead, in order to charge as many as lifetime-critical sensors, the use of multiple mobile chargers for charging sensors can speed up sensor charging significantly, thereby reducing their expiration durations and improving the monitoring quality of WRSNs. However, this poses great challenges to schedule multiple mobile chargers for sensor charging at the same time such that the longest delay among the chargers is minimized due to multiple critical constraints. One such an important constraint in multiple mobile chargers is that each sensor cannot be charged by more than one mobile charger at each time; otherwise, the sensor cannot receive any energy from either of the chargers. In this paper we address this challenge by first formulating a novel longest delay minimization problem that is NP-hard. We then devise the very first approximation algorithm with a provable approximation ratio for the problem. We finally evaluate the performance of the proposed algorithm through experimental simulations. Simulation results demonstrate that the proposed algorithm is very promising, which outperforms the other heuristics in various settings."
SoftStage: Content Staging for Vehicular Content Delivery in the eXpressive Internet Architecture.,"Client mobility is a fundamental challenge when accessing the current Internet, especially in the context of vehicular networking because of its intermittent connectivity nature. Meanwhile, today's network applications are evolving from host-to-host communication to content retrieval, and fostering new designs of Information-centric networking (ICN) protocol and system optimized towards this end. In this paper, we present SoftStage, a client instructed ICN-based network layer function that effectively manages the edge caching to perform reactive content staging to improve vehicular content delivery without any assumption about the client mobility pattern. Experimental results based on an implementation in eXpressive Internet Architecture (XIA) shows that SoftStage achieves up to 10x throughput gain in vehicular networking environments."
WiDrive: Adaptive WiFi-Based Recognition of Driver Activity for Real-Time and Safe Takeover.,"Autonomous vehicles often need human driver to take over in some complicated conditions. Such a sudden takeover could jeopardize the vehicle's safety and stability if not han-dled properly. Hence, if the driver's takeover intention can be recognized as early as possible, the vehicle can have sufficient time to make important takeover preparation. The existing in-car monitoring systems are mostly based on camera, which have several key limitations, such as brightness condition and motion obscurity. On the other hand, WiFi-based wireless sensing has recently shown a great promise in human activity recognition, but mainly for large-scale movements performed in the room environment. In this paper, we propose WiDrive, a real-time in-car driver activity recognition system based on Channel State Information (CSI) changes of WiFi signals. WiDrive consists of three major components: A novel algorithm to extract small-scale in-car human activity features, a real-time recognition system based on Hidden Markov Model (HMM), and an online adaptation algo-rithm to adapt for different drivers and vehicles. We implement WiDrive with commercial WiFi devices and evaluate it in real cars. Our results show that WiDrive has an average recognition accuracy of 91.3% and improves the takeover safety."
Service Demand Prediction with Incomplete Historical Data.,"Predicting service demand of users based on the historical network traffic data in a large area has been of great importance towards better service management for geo-distributed systems, such as edge computing. Existing works relying on complete information of historical service demands cannot deal with real-world scenarios that a significant portion of historical data is not available because collecting such data comprehensively is either costly or impossible. Motivated by the challenge of data incompleteness for the demand prediction, in this paper, we propose a deep neural network model based on the encoder-decoder architecture consisting of an encoder, a converter, and a decoder. This model successfully extracts global features from incomplete historical data, predicts the future information in the feature space, and finally generates complete future demands of the whole area from future features. We devise an encoder with mixed 3D and 2D convolutional layers such that rich temporal and spatial features of service demands can still be captured even when historical data are partially missing. We embed the conditional generative adversarial networks (GANs) into our decoder that generates real demand distribution with magnified quality. Another major difference from the traditional encoder-decoder architecture is to establish a fine-grained association between the features of history and future by adding the converter in the middle of encoder and decoder. With the three components, our model can overcome the data incompleteness challenge and use the extracted global features to make an accurate prediction of service demands. We conduct experiments on a real-world dataset with demands of telecommunications services in an urban environment. The experimental results show that our model achieves much higher prediction accuracy than the state-of-the-art approaches when only incomplete historical data are available."
Joint Online Edge Caching and Load Balancing for Mobile Data Offloading in 5G Networks.,"This paper considers how to cache popular contents and load balancing in 5G networks to minimize the total operating cost. Specifically, popular contents requested by mobile users (MUs) are cached in small base stations (SBSs) to serve them with better quality and lower cost because the SBSs are often much closer to MUs than the base station (BS). Due to limited caching capacity and bandwidth of SBSs, the caching policy and load balancing algorithm need to be carefully designed jointly and dynamically over time. In this paper, we formulate the joint content placement and load balancing by an online optimization problem. This problem is challenging because of the integer constraint in content placement and the lack of future information. We tackle the challenges in two progressive steps. First, we propose a primal-dual algorithm to solve the problem efficiently and prove it always achieves the optimal cost assuming all system information is available. Then we integrate promising online optimization algorithms with the proposed primal-dual algorithm so that only limited short-term predictions are needed. Theoretical performance bounds are also derived. We conduct extensive numerical simulations to evaluate the performance of proposed algorithms. Results highlight that the proposed online algorithms can reduce the system cost significantly (by as much as 27%) compared to the existing solutions and perform similarly to the offline optimal solution."
Deep Reinforcement Learning Based VNF Management in Geo-distributed Edge Computing.,"Edge computing is an effective approach for resource provisioning at the network edge to host virtualized network functions (VNF). Considering the cost diversity in edge computing, from the perspective of service providers, it is significant to orchestrate the VNFs and schedule the traffic flows for network utility maximization (NUM) as it implies maximal revenue. However, traditional model-based optimization methods usually follow some assumptions and impose certain limitations. In this paper, inspired by the success of deep reinforcement learning in solving complicated control problems, we propose a deep deterministic policy gradients (DDPG) based algorithm. We first formulate the NUM problem with the consideration of end-to-end delays and various operation costs into a non-convex optimization problem and prove it to be NP-hard. We then redesign the exploration method and invent a dual replay buffer structure to customize the DDPG. Meanwhile, we also apply our formulation to guide our replay buffer update. Through extensive trace-driven experiments, we show the high efficiency of our customized DDPG based algorithm as it significantly outperforms both model-based methods and traditional non-customized DDPG based algorithm."
Heterogeneous Statistical QoS-Driven Power Allocation for Collaborative D2D Caching Over Edge-Computing Networks.,"With the exponentially increasing demand for wireless multimedia services over 5G mobile wireless networks, the statistical quality-of-service (QoS) provisioning has been proven to be able to effectively guarantee the multimedia data transmissions over highly time-varying wireless channels. On the other hand, many research efforts have been focused on various 5G-promising candidate techniques, such as device-to-device (D2D) caching based communications to offload cellar traffics and address the data explosion problem for the next generation wireless networks. Furthermore, in order to enhance the reliability of video delivery without causing too much interference to other mobile users, researchers have applied the coordinated joint transmission techniques for collaborative D2D caching scheme, which enables mobile users to share popular multimedia files within a D2D communication group instead of downloading from remote backhaul networks. Towards this end, one of the key issues lies in the power allocation problems subject to the heterogeneous statistical delay-bounded QoS requirements for the collaborative D2D caching over edge-computing networks. To overcome the aforementioned challenges, in this paper we propose the collaborative D2D caching model and D2D communication schemes over edge-computing networks. Under the heterogeneous statistical delay-bounded QoS requirements, we formulate and solve the effective-capacity optimization problem for our proposed collaborative D2D caching schemes over edge-computing networks. Then, we develop the collaborative D2D-cache matching algorithms by using bipartite graph technique for selecting D2D-caching users to maximize the effective capacity. Also conducted is a set of simulations which evaluate the system performances and show that our proposed collaborative D2D caching schemes outperform the other existing schemes under heterogeneous statistical delay-bounded QoS constraints."
CMFL: Mitigating Communication Overhead for Federated Learning.,"Federated Learning enables mobile users to collaboratively learn a global prediction model by aggregating their individual updates without sharing the privacy-sensitive data. As mobile devices usually have limited data plan and slow network connections to the central server where the global model is maintained, mitigating the communication overhead is of paramount importance. While existing works mainly focus on reducing the total bits transferred in each update via data compression, we study an orthogonal approach that identifies irrelevant updates made by clients and precludes them from being uploaded for reduced network footprint. Following this idea, we propose Communication-Mitigated Federated Learning (CMFL) in this paper. CMFL provides clients with feedback information regarding the global tendency of model updating. Each client checks if its update aligns with this global tendency and is relevant enough to model improvement. By avoiding uploading those irrelevant updates to the server, CMFL can substantially reduce the communication overhead while still guaranteeing the learning convergence. CMFL is shown to achieve general improvement in communication efficiency for almost all of the existing federated learning schemes. We evaluate CMFL through extensive simulations and EC2 emulations. Compared with vanilla Federated Learning, CMFL yields 13.97x communication efficiency in terms of the reduction of network footprint. When applied to Federated Multi-Task Learning, CMFL improves the communication efficiency by 5.7x with 4% higher prediction accuracy."
Demystifying Traffic Statistics for Edge Cache Deployment in Large-Scale WiFi System.,"How to deploy cache in large-scale WiFi system is not well studied yet quite challenging since numerous Aps turn to be heterogeneous in terms of traffic consumption, and future traffic conditions are unknown ahead. In this paper, given the cache storage budge, we explore the cache deployment in a large-scale WiFi system which contains 8,000 APs and serves more than 40,000 active users, to maximize the long-term caching gain, i.e., the total reduced backhaul traffic. Specifically, we first collect enormous user association records and conduct intensive statistical analysis on the collected data, gaining two major observations. First, per AP traffic consumption varies in a rather wide range and the AP proportion distributes evenly within the range, which indicates that the cache size should be heterogeneously allocated in accordance to the underlying traffic demands. Second, compared to a single AP, the traffic consumption of a group of APs (clustered by physical locations) is more stable, which means that the short-term traffic statistics can be used to infer the future long-term traffic conditions. We then propose our cache deployment strategy, named LEAD (i.e., Large-scale wifi Edge cAche Deployment), in which we first cluster large-scale APs into well-sized edge nodes, then conduct the stationary testing on edge level traffic consumption and sample sufficient traffic statistics in order to precisely characterize future traffic conditions, and finally devise the TEG (Traffic-wEighted Greedy) algorithm to solve the long-term caching gain maximization problem. Extensive trace-driven simulations are carried out and simulation results demonstrate the efficacy of LEAD."
FRAME: Fault Tolerant and Real-Time Messaging for Edge Computing.,"Edge computing systems for Industrial Internet of Things (IIoT) applications require reliable and timely message delivery. Both latency discrepancies within edge clouds, and heterogeneous loss-tolerance and latency requirements pose new challenges for proper quality of service differentiation. Efficient differentiated edge computing architectures are also needed, especially when common fault-tolerant mechanisms tend to introduce additional latency, and when cloud traffic may impede local, time-sensitive message delivery. In this paper, we introduce FRAME, a fault-tolerant real-time messaging architecture. We first develop timing bounds that capture the relation between traffic/service parameters and loss-tolerance/latency requirements, and then illustrate how such bounds can support proper differentiation in a representative IIoT scenario. Specifically, FRAME leverages those timing bounds to schedule message delivery and replication actions to meet needed levels of assurance. FRAME is implemented on top of the TAO real-time event service, and we present empirical evaluations in a local edge computing test-bed and an Amazon Virtual Private Cloud. The results of those evaluations show that FRAME can efficiently meet different levels of message loss-tolerance requirements, mitigate latency penalties caused by fault recovery, and meet end-to-end soft deadlines during normal, fault-free operation."
EF-Dedup: Enabling Collaborative Data Deduplication at the Network Edge.,"The advent of IoT and edge computing will lead to massive amounts of data that need to be collected and transmitted to online storage systems. To address this problem, we push data deduplication to the network edge. Specifically, we propose a new technique for collaborative edge-facilitated deduplication (EF-dedup), wherein we partition the resource-constrained edge nodes into disjoint clusters, maintain a deduplication index structure for each cluster using a distributed key-value store and perform decentralized deduplication within those clusters. This is a challenging partitioning problem that addresses a novel tradeoff: edge nodes with highly correlated data may not always be within the same edge cloud, with non-trivial network cost among them. We address this challenge by first formulating an optimization problem to partition the edge nodes, considering both the data similarities across the nodes and the inter-node network cost. We prove that the problem is NP-Hard, provide bounded heuristics to solve it and build a prototype EF-dedup system. Our experiments on EF-dedup, performed on edge nodes in AT&T research lab and a central cloud at AWS, demonstrate that EF-dedup achieves 38.3-118.5% better deduplication throughput than sole cloud-based techniques and achieves 43.4-60.2% lesser aggregate cost in terms of the network-storage tradeoff as compared to approaches that solely favor one over the other."
Task Assignment Algorithms in Data Shared Mobile Edge Computing Systems.,"The appearance of the Mobile Edge Computing (MEC) successfully solves the bottlenecks of traditional Cloud based networks as computation ability of each mobile edge is sufficiently utilized. Since the mobile edges, including mobile devices and base stations, have certain data processing abilities, it is not necessary to offload all the computation tasks to the remote could for handling. Therefore, it is quite important to decide the optimal task assignment in a MEC system, and a series of algorithms have been proposed to solve it. However, the existing algorithms ignored the data distribution during task assignment, so that the applied ranges of these algorithms are quite limit. Considering the data sharing is very important and common in a MEC system, this paper studies the task assignment algorithm in Data Shared Mobile Edge Computing Systems, and three algorithms are proposed to deal with holistic tasks and divisible tasks, respectively. The theoretical analysis on the hardness of the problem, the correctness, complexities and ratio bounds of the algorithms are also provided. Finally, the extensive experiment results were carried out. Both of the theoretical analysis and experiment results show that our algorithms have high performance in terms of latency and energy consumption."
Privacy-Preserving Data Integrity Verification in Mobile Edge Computing.,"Mobile edge computing (MEC) is proposed as an extension of cloud computing in the scenarios where the end devices desire better services in terms of response time. Edge nodes are deployed at the proximity of the end devices, and it can pre-download parts of data stored in the cloud so that the end devices can access these data with low latency. However, because the edges are usually owned by individuals and small organizations, which have limited operation capacities for maintaining the machines, the data on the edges are easily corrupted (due to external attacks or internal hardware failures). Therefore, it is essential to verify data integrity in the MEC. We propose two Integrity Checking protocols for mobile Edge computing, called ICE-basic and ICE-batch, which are designed for the cases where the user wants to check data integrity on a single edge or multiple edges, respectively. Based on the concept of provable data possession and the technique of private information retrieval, our protocols allow a third-party verifier to check the data integrity on the edges without violating users' data privacy and query pattern privacy. We rigorously prove the security and privacy guarantees of the protocols. Furthermore, we have implemented a proof-of-concept system that runs ICE, and extensive experiments are conducted. The theoretical analysis and experimental results demonstrate the proposed protocols are efficient both in computation and communication."
"F3C: Fog-enabled Joint Computation, Communication and Caching Resource Sharing for Energy-Efficient IoT Data Stream Processing.","Fog/edge computing has been recently regarded as a promising approach for supporting emerging mission-critical Internet of Things (IoT) applications on capacity and battery constrained devices. By harvesting and collaborating a massive crowd of devices in close proximity for computation, communication and caching resource sharing (i.e., 3C resources), it enables great potentials in low-latency and energy-efficient IoT task execution. To efficiently exploit 3C resources of fog devices in proximity, we propose F3C, a fog-enabled 3C resource sharing framework for energy-efficient IoT data stream processing by solving an energy cost minimization problem under 3C constraints. Nevertheless, the minimization problem proves to be NP-hard via reduction to a Generalized Assignment Problem (GAP). To cope with such challenge, we propose an efficient F3C algorithm based on an iterative task team formation mechanism which regards each task's 3C resource sharing as a subproblem solved by the elaborated min cost flow transformation. Via utility improving iterations, the proposed F3C algorithm is shown to converge to a stable system point. Extensive performance evaluations demonstrate that our F3C algorithm can achieve superior performance in energy saving compared to various benchmarks."
Efficient Data Placement and Retrieval Services in Edge Computing.,"Edge computing is a new paradigm in which the computing and storage resources are placed at the edge of the Internet. Data placement and retrieval are fundamental services of edge computing when a network of edge servers collaboratively provide data storage. These services require short-latency and low-overhead implementation in network devices and load balance on edge servers. However existing methods such as distributed hash tables (DHTs) are not able to achieve efficient data placement and retrieval services in the edge computing environment. This paper presents GRED, an efficient data placement and retrieval service for edge computing, which is efficient in not only the load balance but also routing path lengths and forwarding table sizes. GRED utilizes the software-defined networking paradigm to support a virtual-space based DHT with only one overlay hop. We implement GRED in a P4 prototype. Experimental results show that GRED uses <; 30% routing path lengths and achieves better load balance among edge servers compared to using Chord, a well-known DHT solution."
Data-driven Task Allocation for Multi-task Transfer Learning on the Edge.,"Edge computing for machine learning has become a heated research topic. On edge devices, data scarcity occurs as a common problem where transfer learning serves as a widely-suggested remedy. Nevertheless, one obstacle is that transfer learning imposes heavy computation burden to the resource-constrained edge devices. Motivated by the fact that only a few tasks of Multi-task Transfer Learning (MTL) have a higher potential for overall decision performance improvement, we design a novel task allocation scheme, which assigns more important tasks to more powerful edge devices to maximize the overall decision performance. In this paper, we focus on task allocation under multi-task scenarios by introducing task importance and make the following contributions. First, we reveal that it is important to measure the impact of tasks on overall decision performance improvement and quantify task importance. We also observe the long-tail property of task importance, i.e., only a few tasks are important, which facilitates more efficient task allocation. Second, we show that task allocation with task importance for MTL (TATIM) is in fact a variant of the NP-complete Knapsack problem, where the complicated computation to solve this problem needs to be conducted repeatedly under varying contexts. To solve TATIM with high computational efficiency, we innovatively propose a Data-driven Cooperative Task Allocation (DCTA) approach. Third, we evaluate the performance of our DCTA approach by applying it to a real-world industrial operation (e.g., AIOps) scenario. Experiments show that our DCTA approach can reduce 3.24 times of processing time compared with the state-of-the-art when solving TATIM. We offer our DCTA approach as an effective and practical mechanism for reducing the required resource associated with performing MTL on edge devices."
VirtualEdge: Multi-Domain Resource Orchestration and Virtualization in Cellular Edge Computing.,"5G network will carry compute-intensive applications from vertical industries. Network slicing and edge computing are key technologies for fulfilling the diverse requirements of these applications efficiently. We define a cellular network with the edge computing capability as cellular edge computing network. Dynamically slicing a cellular edge computing network is challenging because it needs to orchestrate multi-domain resources and ensure isolation among network slices. In this paper, we present the VirtualEdge system that enables the dynamic creation of a virtual node (vNode) on top of a physical cellular edge computing node to serve the traffic and workloads of a network slice. VirtualEdge introduces a realizable multi-domain resource orchestration and virtualization that provides isolation among network slices without losing the efficiency in virtualizing the radio resources. To efficiently orchestrate multi-domain resources, we design a new learning-assisted algorithm that allows the resource orchestrator to optimize the utilization of physical resources without knowing the utility functions of individual vNodes. For the resource virtualization, we develop a heuristic algorithm and a credit-based queue management scheme to dynamically map virtual radio and computing resources to underlying physical resources, respectively. VirtualEdge is developed and implemented based on the OpenAirInterface LTE and CUDA GPU computing platforms, and its performance is validated through both experiments and large-scale simulations."
Location Privacy Protection in Vehicle-Based Spatial Crowdsourcing Via Geo-Indistinguishability.,"Nowadays, vehicles have been increasingly adopted as participants in many spatial crowdsourcing (SC) applications. Similar to other SC applications, location privacy is of great concern to vehicle workers as they are required to disclose their own location information to servers to facilitate the utilities of SC services. Traditional location privacy protection mechanisms cannot be directly applied to vehicle-based SC since they assume workers' location on a 2-dimensional plane, which does not take into account the features of vehicle workers' mobility in vehicle road networks. Accordingly, in this paper, we aim at addressing issues related to Vehicle-based spatial crowdsourcing Location Privacy (VLP) in vehicle road networks. Our objective is to design a location obfuscation strategy to minimize the loss of quality-of-service (QoS) due to task distribution with location obfuscation, while guaranteeing geo-indistinguishability to be satisfied. Considering the computational complexity of the VLP problem, by resorting to discretization, we approximate VLP to a linear programming problem that can be solved by existing well-developed approaches (such as the simplex method). To further improve the time efficiency, we reduce the number of constraints in VLP by exploiting key features of geo-indistinguishability in vechicle road networks (such as transitivity). Finally, our experimental results demonstrate that our approach can achieve a reasonable approximation of the minimum QoS loss with location privacy protected, and also outperforms a known state-of-the-art location obfuscation strategy in terms of both QoS and privacy."
SPEED: Accelerating Enclave Applications Via Secure Deduplication.,"The emerging hardware-assisted security technologies facilitate the deployment of secure and trustworthy applications in today's cloud computing infrastructure. Despite promising, the advantages appear to diminish due to limited resources of trusted execution environments and ever-increasing workload to be processed inside. Different from existing task-specific and system-level optimizations, our key observation is that those redundant computations occur commonly among several applications when handling the same input data. In light of this, we propose SPEED, a secure and generic computation deduplication system in the context of Intel SGX. It allows SGX-enabled applications to identify redundant computations and reuse computation results, while protecting the confidentiality and integrity of code, inputs, and results. To maximize the benefit of computation deduplication, we design a cross-application deduplication scheme, empowering multiple applications to securely utilize the shared results as long as they perform identical computations. To ease the use of SPEED, we implement a fully functional prototype and provide a concise and expressive API for developers to deduplicate rich computations with minimal effort, as few as 2 lines of code per function call. Extensive evaluations of four popular applications demonstrate that SPEED improves performance by up to 400 times. The source code is available on GitHub for public use."
Optimal Task Allocation and Coding Design for Secure Coded Edge Computing.,"In recent years, edge computing has attracted increasing attention for its capability of facilitating delay-sensitive applications. In the implementation of edge computing, however, data confidentiality has been raised as a major concern because edge devices may be untrustable. In this paper, we propose a design of secure and efficient edge computing by linear coding. In general, linear coding can achieve data confidentiality by adding random information to the original data before they are distributed to edge devices. To this end, it is important to carefully design code such that the user can successfully decode the final result while achieving security requirements. Meanwhile, task allocation, which selects a set of edge devices to participate in a computation task, affects not only the total resource consumption, including computation, storage, and communication, but also coding design. In this paper, we study task allocation and coding design, two highly-coupled problems in secure coded edge computing, in a unified framework. In particular, we take matrix multiplication, a fundamental building block of many distributed machine learning algorithms, as the representative computation task, and study optimal task allocation and coding design to minimize resource consumption while achieving information-theoretic security."
CFP: Enabling Camera Fingerprint Concealment for Privacy-Preserving Image Sharing.,"It has been discovered that every photo carries an unique hardware fingerprint of the photographing digital camera. This camera fingerprint is remarkably effective in image-to-camera matching and has been applied in a wide variety of beneficial forensic tasks such as copyright protection and integrity validation. However, the fingerprints carried by the images can also be utilized for malicious purposes. An adversary can launch identity linking attack, which re-identifies the anonymous social network accounts, through exploiting the digital cameras' fingerprints that are carried by the posted images. Moreover, the adversary can easily frame an innocent victim or bypass camera-based smartphone authentication systems by launching identity forgery attacks, i.e., fabricating unoriginal fingerprints onto images. Currently, no effective counter measures against such attacks have been proposed yet. To solve this problem, in this paper, we first evaluate the effectiveness of the attacks in the current image sharing practices. We then propose CFP, an intermediary between smartphone users and image sharing platforms that conceals the camera fingerprint of the photographing device. Instead of removing the camera fingerprint from the image of interest, our system protects user privacy through obfuscating the camera fingerprint with a specially designed random perturbation component. With such design, the proposed system is enabled to prevent malicious utilizations of camera fingerprint while preserving the beneficial applications. Extensive experiments are conducted to demonstrate the effectiveness and efficiency of the CFP system on various social platforms. Using the CFP obfuscated images, the True Positive Rate of identity linking attacks is reduced by around 85%. For identity forgery attacks, our system enables an effective detection mechanism that could achieve 100% detection rate."
SmartCrowd: Decentralized and Automated Incentives for Distributed IoT System Detection.,"Internet of Things (IoT) devices achieve the rapid development and have been widely deployed recently. Meanwhile, inherent vulnerabilities of IoT systems (including firmware and software) have been continually uncovered and thus the systems are always exposed to various attacks. The root cause of the issue is that IoT systems always have design flaws and implementation bugs. In particular, the released systems (e.g., by third-party marketplaces and IoT vendors) may be maliciously repackaged with malware. Unfortunately, IoT consumers are not able to effectively capture such vulnerabilities because of the limited detection capabilities. In this paper, we propose SmartCrowd, a blockchain-based platform that aims to outsource security detection of IoT systems to distributed detectors with strong detection incentives. SmartCrowd enables built-in accountability for IoT providers and authoritative references of detection results for IoT consumers. By building smart contracts, we can incentivize the efficient and high-coverage security detection of IoT systems, while providing decentralized and automated incentives for both IoT providers releasing secure IoT systems and detectors uncovering vulnerabilities. We present the security and theoretical analysis that demonstrates the security of SmartCrowd and the incentives for participators. We prototype SmartCrowd by using Ethereum and the experimental results show that SmartCrowd has both technical feasibility and financial benefits, which can be applied to build a secure IoT ecosystem."
Hide and Seek: Waveform Emulation Attack and Defense in Cross-Technology Communication.,"The exponentially increasing number of heterogeneous Internet of Things (IoT) devices result in severe spectrum shortage and interference in the already crowded ISM band. Cross-Technology Communication (CTC) is dedicated to achieving direct communication among wireless devices with different radios and modulation schemes, which serves as an effective approach to address the above challenges. Nevertheless, CTC also provides opportunities for adversaries to manipulate IoT devices. In this paper, we identify a new attack. Built on CTC, WiFi devices are able to hide the pre-intercepted ZigBee message into their transmitted waveforms, achieving the objective of directly controlling ZigBee devices. To defend against the attack, we analyze possible strategies and consider constellation higher-order statistic analysis as the countermeasure. Extensive simulations and experiments with commodity devices (CC26x2R1) and USRP-based prototypes show the existence of the newly identified attack, and further, validate the effectiveness of the proposed defensive approach."
PPSAS: Lightweight Privacy-preserving Spectrum Aggregation and Auction in Cognitive Radio Networks.,"Cognitive radio network (CRN) enables dynamic spectrum management where spectrum aggregation and sharing have significantly facilitated relieving the supply-demand gap among skyrocketing number of mobile devices. Secondary users (SUs) can collaboratively detect and exploit the available spectrums of primary users (PUs) from different locations. Unfortunately, it has brought a series of security threats leaking user's location privacy to unauthorized entities. The existing work either had privacy breaches or depended on computationally-intensive public key homomorphic encryption loading intolerably high complexity on resource-constrained SUs. To well address these issues, in this paper, a lightweight privacy-preserving spectrum aggregation and auction scheme PPSAS is proposed in CRNs without leveraging public key homomorphic encryption. Firstly, a new primitive of efficient privacy preserving multiparty data aggregation protocol PPMDA is proposed based on any one-way trapdoor permutation. Then based on PPMDA, an efficient privacy preserving spectrum aggregation scheme PPSRA and spectrum auction scheme PPSSA are respectively devised to constitute our proposed PPSAS. Especially, a decentralized PPMDA is extended to resist collusion attack and a secure extended outsourced stable complete marriage matching protocol is given to flexibly realize SU/auctioneer optimization. Finally, formal security proof and extensive simulations demonstrate that our proposed PPSAS well protects the SUs' location privacy against collusion attacks and possesses the advantages over the sate-of-the-art in terms of auctioneer's revenue, SUs' satisfaction, computational and communication overhead."
Context-Aware Trust Management System for IoT Applications with Multiple Domains.,"The Internet of Things (IoT) provides connectivity between heterogeneous devices in different applications, such as smart wildlife, supply chain and traffic management. Trust management system (TMS) assesses the trustworthiness of service with respect to its quality. Under different context information, a service provider may be trusted in one context but not in another. The existing context-aware trust models usually store trust values under different contexts and search the closest (to a given context) record to evaluate the trustworthiness of a service. However, it is not suitable for distributed resource-constrained IoT devices which have small memory and low power. Reputation systems are applied in many trust models where trustor obtains recommendations from others. In context-based trust evaluation, it requires interactive queries to find relevant information from remote devices. The communication overhead and energy consumption are issues in low power networks like 6LoWPAN. In this paper, we propose a new context-aware trust model for lightweight IoT devices. The proposed model provides a trustworthiness overview of a service provider without storing past behavior records, that is, constant size storage. The proposed model allows a trustor to decide the significance of context items. This could result in distinctive decisions under the same trustworthiness record. We also show the performance of the proposed model under different attacks."
ADLP: Accountable Data Logging Protocol for Publish-Subscribe Communication Systems.,"Reasoning about the decision-making process of modern autonomous systems becomes increasingly challenging as their software systems become more inexplicable due to complex data-driven processes. Yet, logs of data production and consumption among the software components can provide useful run-time evidence to analyze and diagnose faulty operations. Particularly when the system is run by a number of software components that were individually developed by different parties (e.g., open source, third-party vendor), it is imperative to find out where the problems originated and thus who should be responsible for the problems. However, software components may act unfaithfully or non-cooperatively to make the run-time evidence refutable or unusable. Hence, this paper presents Accountable Data Logging Protocol (ADLP), a mechanism to build accountability into data distribution among software components that are not necessarily cooperative or faithful in reporting the logs of their data production and consumption. We demonstrate an application of ADLP to a miniaturized self-driving car and show that it can be used in practice with at a moderate performance cost."
Practical Verifiable In-network Filtering for DDoS Defense.,"In light of ever-increasing scale and sophistication of modern distributed denial-of-service (DDoS) attacks, recent proposals show that in-network filtering of DDoS traffic at a handful of transit networks can handle volumetric attacks effectively. In this paper, we identify a subtle but important security risk in existing in-network filtering proposals. That is, a transit network may use the in-network filtering services as an excuse for any arbitrary packet drops made for its own benefit. For example, a malicious transit network may execute any filtering rules to discriminate against some of its neighboring networks based on its business preference while claiming that it is for the purpose of DDoS defense. We argue that this is due to the lack of verifiable filtering-i.e., no single party can check if a transit network executes the filter rules correctly as requested by the DDoS victims. To make in-network filtering a more robust defense primitive, we propose a verifiable in-network filtering system, called VIF, that exploits emerging hardware-based trusted execution environments (TEEs) and offers filtering verifiability to DDoS victims and neighboring networks. Our proof of concept demonstrates that a VIF filter implementation on commodity servers with TEE support can handle traffic at line rate (e.g., 10 Gb/s) and execute up to 3,000 filter rules. We show that VIF can scale to handle larger traffic volume (e.g., 500 Gb/s) and more complex filtering operations (e.g., 150,000 filter rules) by parallelizing the TEE-based filters. As a practical deployment model, we suggest that Internet exchange points (IXPs) are the good candidates to be early adopters of our verifiable filters due to their central locations and flexible software-defined architecture. Our large-scale simulations of two realistic attacks (i.e., DNS amplification, Mirai-based flooding) show that adopting VIF filtering service at only a small number (e.g., 5-25) of large IXPs is sufficient to handle the majority (e.g., up to 80-90%) of DDoS traffic."
"Partitioning Attacks on Bitcoin: Colliding Space, Time, and Logic.","Bitcoin is the leading example of a blockchain application that facilitates peer-to-peer transactions without the need for a trusted intermediary. This paper considers possible attacks related to the decentralized network architecture of Bitcoin. We perform a data driven study of Bitcoin and present possible attacks based on spatial and temporal characteristics of its network. Towards that, we revisit the prior work, dedicated to the study of centralization of Bitcoin nodes over the Internet, through a fine-grained analysis of network distribution, and highlight the increasing centralization of the Bitcoin network over time. As a result, we show that Bitcoin is vulnerable to spatial, temporal, spatio-temporal, and logical partitioning attacks with an increased attack feasibility due to network dynamics. We verify our observations by simulating attack scenarios and the implications of each attack on the Bitcoin . We conclude with suggested countermeasures."
Towards Systematic Design of Collective Remote Attestation Protocols.,"Networks of and embedded (IoT) devices are becoming increasingly popular, particularly, in settings such as smart homes, factories and vehicles. These networks can include numerous (potentially diverse) devices that collectively perform certain tasks. In order to guarantee overall safety and privacy, especially in the face of remote exploits, software integrity of each device must be continuously assured. This can be achieved by Remote Attestation (RA) - a security service for reporting current software state of a remote and untrusted device. While RA of a single device is well understood, collective RA of large numbers of networked embedded devices poses new research challenges. In particular, unlike single-device RA, collective RA has not benefited from any systematic treatment. Thus, unsurprisingly, prior collective RA schemes are designed in an ad hoc fashion. Our work takes the first step toward systematic design of collective RA, in order to help place collective RA onto a solid ground and serve as a set of design guidelines for both researchers and practitioners. We explore the design space for collective RA and show how the notions of security and effectiveness can be formally defined according to a given application domain. We then present and evaluate a concrete collective RA scheme systematically designed to satisfy these goals."
CryptoNN: Training Neural Networks over Encrypted Data.,"Emerging neural networks based machine learning techniques such as deep learning and its variants have shown tremendous potential in many application domains. However, they raise serious privacy concerns due to the risk of leakage of highly privacy-sensitive data when data collected from users is used to train neural network models to support predictive tasks. To tackle such serious privacy concerns, several privacy-preserving approaches have been proposed in the literature that use either secure multi-party computation (SMC) or homomorphic encryption (HE) as the underlying mechanisms. However, neither of these cryptographic approaches provides an efficient solution towards constructing a privacy-preserving machine learning model, as well as supporting both the training and inference phases. To tackle the above issue, we propose a CryptoNN framework that supports training a neural network model over encrypted data by using the emerging functional encryption scheme instead of SMC or HE. We also construct a functional encryption scheme for basic arithmetic computation to support the requirement of the proposed CryptoNN framework. We present performance evaluation and security analysis of the underlying crypto scheme and show through our experiments that CryptoNN achieves accuracy that is similar to those of the baseline neural network models on the MNIST dataset."
A Universal Method Based on Structure Subgraph Feature for Link Prediction over Dynamic Networks.,"In dynamic networks, links are annotated with timestamps showing the emerging time and the link prediction problem is to infer the future links in networks. Universal link prediction methods are highly demanded in various applications, which require universal link features that are feasible for multiple kinds of network topological structures and capable to address the difference of links with different timestamps. In this paper, we propose a novel link feature called Structure Subgraph Feature (SSF). The SSF is an outstanding link feature that is feasible to various dynamic networks due to the following superiorities: (1) the proposed structure subgraph is so far the most effective manner to represent surrounding topological features of target link and (2) the normalized influence well specifies the influence of multiple links and different timestamps in structure subgraph. We finally propose two link prediction methods by applying SSF to a linear regression model and a neural machine. Experimental results on real-world dynamic network datasets indicate that the SSF-based methods consistently provide top-class performance on various dynamic networks."
CrowdLearn: A Crowd-AI Hybrid System for Deep Learning-based Damage Assessment Applications.,"Artificial Intelligence (AI) has been widely adopted in many important application domains such as speech recognition, computer vision, autonomous driving, and AI for social good. In this paper, we focus on the AI-based damage assessment applications where deep neural network approaches are used to automatically identify damage severity of impacted areas from imagery reports in the aftermath of a disaster (e.g., earthquake, hurricane, landslides). While AI algorithms often significantly reduce the detection time and labor cost in such applications, their performance sometimes falls short of the desired accuracy and is considered to be less reliable than domain experts. To exacerbate the problem, the black-box nature of the AI algorithms also makes it difficult to troubleshoot the system when their performance is unsatisfactory. The emergence of crowdsourcing platforms (e.g., Amazon Mechanic Turk, Waze) brings about the opportunity to incorporate human intelligence into AI algorithms. However, the crowdsourcing platform is also black-box in terms of the uncertain response delay and crowd worker quality. In this work, we propose the CrowdLearn, a crowd-AI hybrid system that leverages the crowdsourcing platform to troubleshoot, tune, and eventually improve the black-box AI algorithms by welding crowd intelligence with machine intelligence. The system is specifically designed for deep learning-based damage assessment (DDA) applications where the crowd tend to be more accurate but less responsive than machines. Our evaluation results on a real-world case study on Amazon Mechanic Turk demonstrate that CrowdLearn can provide timely and more accurate assessments to natural disaster events than the state-of-the-art AI-only and human-AI integrated systems."
Mutual-Preference Driven Truthful Auction Mechanism in Mobile Crowdsensing.,"Motivating the mobile users to participate in sensing services for efficient data generation and collection is one of the most critical issues in Mobile Crowdsensing Systems (MCSs). Auction based mechanisms are seen to be promising and effective solutions to incentivize mobile users. However, price is not the unique factor dominating participants' contribution in MCSs. Participant's preference for different sensing tasks is also a pivotal factor which should be considered in the auction mechanisms as assigning the least favorite tasks discourages them to participate in future sensing tasks. Unfortunately, participant's preference has been overlooked by most existing works, which motivates us to fill this gap in this paper. We first propose a new concept ""mutual preference degree"" to capture participant's preference and then design a preference-based auction mechanism (PreAM) to simultaneously guarantee individual rationality, budget feasibility, preference truthfulness, and price truthfulness. Finally, both the theoretical analysis and simulation results demonstrate the effectiveness of PreAM."
Adaptive Crawling with Cautious Users.,"In Online Social Networks (OSNs), privacy issue is a growing concern as more and more users are sharing their candid personal information and friendships online. One simple yet effective attack aims at private user data is to use socialbots to befriend the users and crawl data from users who accept the attackers' friend requests. With the attackers involving, individual users' preference and habit analysis is available, hence it is easier for the attackers to trick the users and befriend them. To better protect private information, some cautious, high-profile users may refer to their friends' decisions when receiving a friend request. The aim for this paper is to analyze the vulnerability of OSN users under this attack, in a more realistic setting that the high profile users having a different friend request acceptance model. Specifically, despite the existing probabilistic acceptance models, we introduce a deterministic linear threshold acceptance model for the cautious users such that they will only accept friend requests from users sharing at least a certain number of mutual friends with them. The model makes the cautious users harder to befriend with and complicates the attack. Although the new problem with multiple acceptance models is non-submodular and has no performance guarantee in general, we introduce the concept of adaptive submodular ratio and establish an approximation ratio under certain conditions. In addition, our results are also verified by extensive experiments in real-world OSN data sets."
Analysis of Antagonistic Dynamics for Rumor Propagation.,"The extreme boom of online social networks paves the way for rumor propagation, which may incur an economic loss and cause further public panic. Hence, there is a pressing need to develop countermeasures for reducing side effects posed by rumors. Different from the state-of-the-art work that mostly conducted micro-perspective studies, our paper focuses on a macro-perspective one. In detail, our study neglects technical details and analyzes the antagonistic dynamics between the rumormonger and the rumor suppressor, which provides a deep understanding of the overall development trend of rumor propagation. To reveal the potentials of the rumormonger and the rumor suppressor, the competence-oriented analysis is proposed, where the sufficient and necessary conditions for the existence of the Nash equilibrium in this rumor game are proved rigorously, helping us to derive steady ratios of people who trust or deny a rumor. To figure out the optimal strategy to strike back the rumormonger, the target-oriented analysis is conducted, in which the analytical solutions when the strategies of both players are static are solved and an iteration algorithm is employed to obtain the numerical solutions when their strategies are dynamic. Both numerical and real-world-data based simulations are adopted to verify the proposed competence-oriented and target-oriented analyses."
An Approximation Algorithm for Active Friending in Online Social Networks.,"Guiding users to actively expanding their online social circles is one of the primary strategies for enhancing user participation and growing online social networks. In this paper, we study the active friending problem which aims at providing users with the strategy for methodically sending invitations to successfully build a friendship with target users. We consider the prominent linear threshold model for the friending process and formulate the active friending problem as an optimization problem. The key observation is the relationship between the active friending problem and the minimum subset cover problem, based on which we present the first randomized algorithm with a data-independent approximation ratio and a controllable success probability for general graphs. The performance of the proposed algorithm is theoretically analyzed and supported by encouraging simulation results done on extensive datasets."
A Latent Hawkes Process Model for Event Clustering and Temporal Dynamics Learning with Applications in GitHub.,"Large volumes of event data are becoming increasingly available on online social networks. These events are usually causally dependent to each other, reflecting the interactions and collaborations among different parties. Learning and interpreting the temporal patterns and dynamics within these event streams plays an important role in many practical applications, such as trend prediction and anomaly detection. Since causal dependencies can be reflected in both event time (i.e., when) and event content (i.e., who and what), we thus develop a user community based generative model, called latent Hawkes process (LHP), taking into account both-side information to illustrate the generation of such inter-dependent event streams on GitHub repositories, where each attribute is assumed to be generated by interplays between correlated latent communities. Through learning of our model, two functionalities are fulfilled concurrently: event clustering (i.e., community discovery) and temporal dependency learning among these clusters (i.e., dependency profiling). To do so, we design an EM-based framework integrating sequential Monte Carlo sampling to estimate model parameters in an end-to-end manner. Through experiments on practical GitHub event data, we validate the effectiveness of LHP in extracting user community structures and learning their correlated temporal dynamics. Such knowledge further enables us to gain new insights into the development status of software, such as the project persistence and anomaly detection."
Incentivizing the Workers for Truth Discovery in Crowdsourcing with Copiers.,"Crowdsourcing has become an efficient paradigm for performing large scale tasks. Truth discovery and incentive mechanism are fundamentally important for the crowdsourcing system. Many truth discovery methods and incentive mechanisms for crowdsourcing have been proposed. However, most of them cannot be applied to dealing with the crowdsourcing with copiers. To address the issue, we formulate the problem of maximizing the social welfare such that all tasks can be completed with the least confidence for truth discovery. We design an incentive mechanism consisting of truth discovery stage and reverse auction stage. In truth discovery stage, we estimate the truth for each task based on both the dependence and accuracy of workers. In reverse auction stage, we design a greedy algorithm to select the winners and determine the payment. Through both rigorous theoretical analysis and extensive simulations, we demonstrate that the proposed mechanisms achieve computational efficiency, individual rationality, truthfulness, and guaranteed approximation. Moreover, our truth discovery method shows prominent advantage in terms of precision when there are copiers in the crowdsourcing systems."
Adversarial Learning Attacks on Graph-based IoT Malware Detection Systems.,"IoT malware detection using control flow graph (CFG)-based features and deep learning networks are widely explored. The main goal of this study is to investigate the robustness of such models against adversarial learning. We designed two approaches to craft adversarial IoT software: off-the-shelf methods and Graph Embedding and Augmentation (GEA) method. In the off-the-shelf adversarial learning attack methods, we examine eight different adversarial learning methods to force the model to misclassification. The GEA approach aims to preserve the functionality and practicality of the generated adversarial sample through a careful embedding of a benign sample to a malicious one. Intensive experiments are conducted to evaluate the performance of the proposed method, showing that off-the-shelf adversarial attack methods are able to achieve a misclassification rate of 100%. In addition, we observed that the GEA approach is able to misclassify all IoT malware samples as benign. The findings of this work highlight the essential need for more robust detection tools against adversarial learning, including features that are not easy to manipulate, unlike CFG-based features. The implications of the study are quite broad, since the approach challenged in this work is widely used for other applications using graphs."
Selfish Mining in Ethereum.,"As the second largest cryptocurrency by market capitalization and today's biggest decentralized platform that runs smart contracts, Ethereum has received much attention from both academia and industry. Nevertheless, there exist very few studies about the security of its mining strategies, especially from the selfish mining perspective. In this paper, we fill this research gap by analyzing selfish mining in Ethereum and understanding its potential threat. First, we introduce a 2-dimensional Markov process to model the behavior of a selfish mining strategy inspired by a Bitcoin mining strategy proposed by Eyal and Sirer. Second, we derive the stationary distribution of our Markov model and compute long-term average mining rewards. This allows us to determine the threshold of computational power which makes selfish mining profitable in Ethereum. We find that this threshold is lower than that in Bitcoin mining (which is 25% as discovered by Eyal and Sirer), suggesting that Ethereum is more vulnerable to selfish mining than Bitcoin."
Jidar: A Jigsaw-like Data Reduction Approach Without Trust Assumptions for Bitcoin System.,"The pioneer blockchain platform Bitcoin has drawn massive attention from various sectors in society, with its promising decentralized, irreversible, and trust-free natures. Increasing volume of Bitcoin transactions makes it impractical for each user to store a full copy of whole ledger, especially for a normal user who makes few transactions and owns constrained storage space. There are already some works conducted to reduce the storage overhead by redesigning the system protocol, based on different trust assumptions. An example is running light nodes, which trust the full nodes to store its relevant data and reply to its data request timely. However, it introduces the risks of permanent data loss, as the data relevant to a light node may be tailored by all the full nodes later. Another attempt is conducted by organizing the nodes into several cooperative units based on a trust assumption, which is hard to satisfy in practice. In this paper, we propose a novel Jigsaw-like Data Reduction (Jidar) approach. Each node in Jidar only stores transactions of interest and relevant Merkle branches from the complete blocks, just like selecting several pieces from the jigsaw puzzles. A node can maintain and verify all its relevant data locally and safely without any trust assumptions. To verify the validity of a newly proposed transaction, Merkle branches relevant to the inputs are provided along with the transaction by the proposer. In view of the requirement that a user wants to cohere all the fragments stored in different nodes into a complete block, just like stitching all the pieces into a complete jigsaw-puzzle picture, a mechanism to query full data is added to Jidar. Experimental results demonstrate that Jidar can reduce a normal node's storage cost to about 1.03% of the original Bitcoin system at a low communication cost"
Hierarchical Edge-Cloud Computing for Mobile Blockchain Mining Game.,"Computation offloading has been considered as a viable solution to blockchain mining in mobile environments. In this paper, we present a two-layer computation offloading paradigm that includes an edge computing service provider (ESP) and a cloud computing service provider (CSP). We formulate a multi-leader multi-follower Stackelberg game to address the computing resource management problem in such a network, by jointly maximizing the profits of each service provider (SP) and the payoffs of individual miners. Two practical scenarios are investigated: a fixed-miner-number scenario for permissioned blockchains and a dynamic-miner-number scenario for permissionless blockchains. For the fixed-miner-number scenario, we discuss two different edge operation modes, i.e., the ESP is connected (to the CSP) or standalone, which form different miner subgames based on whether each miner's strategy set is mutually dependent. The existence and uniqueness of Stackelberg equilibrium (SE) in both modes are analyzed, according to which algorithms are proposed to achieve the corresponding SE(s). For the dynamic-miner-number scenario, we focus on the impact of population uncertainty and find that the uncertainty inflates the aggressiveness in the ESP resource purchasing. Numerical evaluations are presented to verify the proposed models."
ParBlockchain: Leveraging Transaction Parallelism in Permissioned Blockchain Systems.,"Many existing blockchains do not adequately address all the characteristics of distributed system applications and suffer from serious architectural limitations resulting in performance and confidentiality issues. While recent permissioned blockchain systems, have tried to overcome these limitations, their focus has mainly been on workloads with no-contention, i.e., no conflicting transactions. In this paper, we introduce OXII, a new paradigm for permissioned blockchains to support distributed applications that execute concurrently. OXII is designed for workloads with (different degrees of) contention. We then present ParBlockchain, a permissioned blockchain designed specifically in the OXII paradigm. The evaluation of ParBlockchain using a series of benchmarks reveals that its performance in workloads with any degree of contention is better than the state of the art permissioned blockchain systems."
B-IoT: Blockchain Driven Internet of Things with Credit-Based Consensus Mechanism.,"Internet of Things (IoT) plays an indispensable role in our daily life, in many cases, IoT systems are implemented following the client-server paradigm, which are vulnerable to single point of failures and malicious attacks. Due to the resilience and security promise of blockchain, the idea of combining blockchain and IoT has gained considerable attention in recent years. However, blockchains are power-intensive and low-throughput, which may not suitable for power-constrained IoT devices. To tackle these challenges, we present B-IoT, a blockchain based IoT system with credit-based consensus mechanism. We propose a credit-based proof-of-work (PoW) mechanism for IoT devices, which enhances security and improves transaction efficiency simultaneously. In order to protect the confidentiality of sensitive IoT data, we design a data authority management method to regulate the access to sensor data. In addition, our system is built based on a directed acyclic graph (DAG)-structured blockchain, which is more efficient than the satoshi-style blockchain. We implement a prototype of B-IoT on Raspberry Pi, and conduct case studies of a smart factory. Extensive evaluation and analysis results demonstrate that the proposed credit-based PoW mechanism and data access control are practical for IoT."
Trust Mends Blockchains: Living up to Expectations.,"At the heart of Blockchains is the trustless leader election mechanism for achieving consensus among pseudo-anonymous peers, without the need of oversight from any third party or authority whatsoever. So far, two main mechanisms are being discussed: proof-of-work (PoW) and proof-of-stake (PoS). PoW relies on demonstration of computational power, and comes with the markup of huge energy wastage in return of the stake in cyrpto-currency. PoS tries to address this by relying on owned stake (i.e., amount of crypto-currency) in the system. In both cases, Blockchains are limited to systems with financial basis. This forces non-crypto-currency Blockchain applications to resort to ""permissioned"" setting only, effectively centralizing the system. However, non-crypto-currency permisionless blockhains could enable secure and self-governed peer-to-peer structures for numerous emerging application domains, such as education and health, where some trust exists among peers. This creates a new possibility for valuing trust among peers and capitalizing it as the basis (stake) for reaching consensus. In this paper we show that there is a viable way for permisionless non-financial Blockhains to operate in completely decentralized environments and achieve leader election through proof-of-trust (PoT). In our PoT construction, peer trust is extracted from a trust network that emerges in a decentralized manner and is used as a waiver for the effort to be spent for PoW, thus dramatically reducing total energy expenditure of the system. Furthermore, our PoT construction is resilient to the risk of small cartels monopolizing the network (as it happens with the mining-pool phenomena in PoW) and is not vulnerable to sybils. We evluate security guarantees, and perform experimental evaluation of our construction, demonstrating up to 10-fold energy savings compared to PoW without trading off any of the decentralization characteristics, with further guarantees against risks of monopolization."
DataEther: Data Exploration Framework For Ethereum.,"Ethereum is the largest blockchain platform supporting smart contracts with the second biggest market capitalization. Ethereum data can yield many useful insights because of the large volume of transactions, accounts and blocks as well as the popular applications developed as smart contracts. Studying Ethereum data can also reveal many new attacks to the platform and its smart contracts. Unfortunately, it is non-trivial to systematically explore Ethereum because it involves massive heterogeneous data, which are produced and stored in different ways. Although a few recent studies report some interesting observations about Ethereum, they are limited by their data acquisition methods which cannot provide comprehensive and precise data. In this paper, to fill the gap, we propose DataEther, a systematic and high-fidelity data exploration framework for Ethereum by exploiting its internal mechanisms. Besides supporting the analyses in existing studies, DataEther further empowers users to explore unknown phenomena and obtain in-depth understandings. We first describe how we tackle the challenging issues in developing DataEther, and then use four data-centric applications to demonstrate its usage and report many new observations."
Quantitative Impact Evaluation of an Abstraction Layer for Data Stream Processing Systems.,"With the demand to process ever-growing data volumes, a variety of new data stream processing frameworks have been developed. Moving an implementation from one such system to another, e.g., for performance reasons, requires adapting existing applications to new interfaces. Apache Beam addresses these high substitution costs by providing an abstraction layer that enables executing programs on any of the supported streaming frameworks. In this paper, we present a novel benchmark architecture for comparing the performance impact of using Apache Beam on three streaming frameworks: Apache Spark Streaming, Apache Flink, and Apache Apex. We find significant performance penalties when using Apache Beam for application development in the surveyed systems. Overall, usage of Apache Beam for the examined streaming applications caused a high variance of query execution times with a slowdown of up to a factor of 58 compared to queries developed without the abstraction layer. All developed benchmark artifacts are publicly available to ensure reproducible results."
An Industrial IoT Solution for Evaluating Workers' Performance Via Activity Recognition.,"The Industrial Internet of Things (IIoT) is a key pillar of the Fourth Industrial Evolution or Industry 4.0. It aims to achieve direct information exchange between industrial machines, people, and processes. By tapping and analysing such data, IIoT can more importantly provide for significant improvements in productivity, product quality, and safety via proactive detection of problems in the performance and reliability of production machines, workers, and industrial processes. While the majority of existing IIoT research is currently focusing on the predictive maintenance of industrial machines (unplanned production stoppages lead to significant increases in costs and lost plant productivity), this paper focuses on monitoring and assessing worker productivity. This IIoT research is particularly important for large manufacturing plants where most production activities are performed by workers using tools and operating machines. With this aim, this paper introduces a novel industrial IoT solution for monitoring, evaluating, and improving worker and related plant productivity based on workers activity recognition using a distributed platform and wearable sensors. More specifically, this IIoT solution captures acceleration and gyroscopic data from wearable sensors in edge computers and analyses them in powerful processing servers in the cloud to provide a timely evaluation of the performance and productivity of each individual worker in the production line. These are achieved by classifying worker production activities and computing Key Performance Indicators (KPIs) from the captured sensor data. We present a real-world case study that utilises our IIoT solution in a large meat processing plant (MPP). We illustrate the design of the IIoT solution, describe the in-plant data collection during normal operation, and present the sensor data analysis and related KPI computation, as well as the outcomes and lessons learnt."
Widening the Circle of Engagement Around Environmental Issues using Cloud-based Tools.,"Environmental data are being generated and collected at unprecedented rates. However, the diversity in form and format of these environmental assets poses challenges for collaborative and reproducible science. Moreover, access constraints that surround environmental data lead to difficulty in use and interpretation of results. Cloud computing offers high potential to break down such barriers and engender collaboration, attribution, reuse, and reproducibility. In this article we review the design of the Environmental Virtual Observatory pilot (EVOp) that was conceived as a cloud-enabled virtual research space for different users interested in environmental science, ranging from domain specialists to the general public. We discuss the key technologies and processes used: a hybrid cloud infrastructure; standard service interfaces; a unified service delivery platform; and a test-driven development cycle. We also discuss the methodology by showcasing one of the exemplars developed in EVOp, stressing the importance of weaving stakeholder engagement from the beginning and throughout the process. We also briefly highlight some of the lessons learnt of working in an interdisciplinary team."
HashFlow for Better Flow Record Collection.,"Collecting flow records is a common practice of network operators and researchers for monitoring, diagnosing and understanding a network. Traditional tools like NetFlow face great challenges when both the speed and the complexity of the network traffic increase. To keep pace up, we propose HashFlow, a tool for more efficient and accurate collection and analysis of flow records. The central idea of HashFlow is to maintain accurate records for elephant flows, but summarized records for mice flows, by applying a novel collision resolution and record promotion strategy to hash tables. We have implemented HashFlow as well as several latest flow measurement algorithms in a P4 software switch, and use traces from different operational networks to evaluate the algorithms. In these experiments, for various types of traffic analysis applications, HashFlow consistently demonstrates a clearly better performance against its state-of-the-art competitors. For example, using a small memory of 1 MB, HashFlow can accurately record around 55K flows, which is often 12.5% higher than the others. For estimating the sizes of 50K flows, HashFlow achieves a relative error of around 11.6%, while the estimation error of the best competitor is 42.9% higher. It detects 96.1% of the heavy hitters out of 250K flows with a size estimation error of 5.6%, which is 11.3% and 73.7% better than the best competitor respectively. At last, we show these merits of HashFlow come with almost no degradation of throughput."
Power Integrity Design of Distributed Power System for UGV Computer.,"In this paper, a distributed power system consisting of two power boards is designed for high-power distributed unmanned ground vehicle computer. This design scheme enables the power system to provide high-power supply at the same time to ensure a higher quality of power supply. Through the reasonable planning of power supply path and the accurate setting of decoupling capacitor, the power distribution network has higher voltage accuracy and lower voltage ripple noise. In addition, the Power Integrity simulation software is used to analyze, improve and verify the design to ensure that the power system design has a high quality. Finally, the actual measurement results show that the power system design scheme can effectively improve the power integrity of the distributed power network."
Caravel: Burst Tolerant Scheduling for Containerized Stateful Applications.,"In a containerized environment, the applications are generally categorized as either stateful or stateless, each consisting of multiple containers. Their co-scheduling in a cluster presents unique challenges for the container orchestration frameworks. The two types of applications differ from each other in how they handle the temporary load spikes. The stateless applications can scale out by instantly spawning new identical instances, whereas the stateful applications require deliberate planning to scale, as each application instance is unique. Instead, the stateful applications can more conveniently acquire the additional resources needed during a spike by scaling up on the same node. However, when an application's container uses more than requested resources, it risks being evicted from the node. The evictions are particularly detrimental for stateful applications because of their longer start up time and the resulting degradation. Moreover, the existing container orchestration frameworks schedule or evict containers without any knowledge of its impact on their owning applications. For instance, an eviction of the application's multiple containers in a short period of time could compromise its availability and severely degrade its performance. To address these challenges, we present Caravel, a scheduling approach that provides better experience to stateful applications in dealing with load spikes. It allows them to overstep the resource request during a burst and use the resources on the same node while minimizing their evictions. Moreover, the scheduler provides a fair opportunity to all the stateful applications to use the spare resources in the cluster. The evaluation shows that our approach reduces the eviction of stateful applications by up to 90% over the traditional approach."
On Efficiently Processing Workflow Provenance Queries in Spark.,"In this paper, we look at how we can leverage Spark platform for efficiently processing fine-grained provenance queries on large volumes of workflow provenance data. Simple recursive querying based Spark solutions involve large data scanning cost and hence do not work well. We propose a novel provenance framework which is engineered to quickly determine a small volume of data containing the entire lineage of the queried data-item. This small volume of data is then recursively processed to figure out the provenance of the queried data-item. We study the effectiveness of the proposed framework on a provenance trace obtained from a financial domain text curation workflow and report our observations. We show that the proposed framework easily outperforms the naive approaches."
DynaStar: Optimized Dynamic Partitioning for Scalable State Machine Replication.,"Classic state machine replication (SMR) does not scale well, since each replica must execute every command. To address this problem, several systems have investigated the use of state partitioning in the context of SMR, allowing client commands to be executed on a subset of replicas. Prior approaches range from completely static schemes, which do not adapt as workloads change, to dynamic schemes, which move data on-demand. This paper presents DynaStar, a new dynamic partitioning scheme for scaling state machine replication. In contrast to prior dynamic schemes, DynaStar uses a replicated location oracle to maintain a global view of the workload and inform heuristics about data placement. Using this oracle, DynaStar is able to adapt to workload changes over time, while also minimizing the number of state moves. The result is a practical technique that achieves excellent performance."
Kronos: A 5G Scheduler for AoI Minimization Under Dynamic Channel Conditions.,"Age of information (AoI) is a powerful new metric to quantify the freshness of information and has gained increasing popularity in IoT applications. Existing models on AoI remain primitive and do not consider state-of-the-art transmission technologies such as 5G. They also fail to consider the impact of dynamic channel conditions. In this paper, we present Kronos, a 5G-compliant AoI scheduling algorithm that can cope with highly dynamic channel conditions. Kronos is capable of performing RB allocation and selecting MCS for each source node based on channel conditions, with the objective of minimizing long-term AoI. To meet the stringent real-time requirement for 5G, we propose a GPU-based implementation of Kronos on low-cost offthe-shelf GPUs. Through simulations and experiments, we show that Kronos can find near-optimal AoI scheduling solutions in sub-millisecond time scale. To the best of our knowledge, this is the first 5G-compliant real-time AoI scheduler that can cope with dynamic channel conditions."
Resource Allocation and Consensus on Edge Blockchain in Pervasive Edge Computing Environments.,"Edge devices with sensing, storage, and communication resources are penetrating our daily lives. These resources make it possible for edge devices to conduct data transactions (e.g., micro-payments, micro-access control). The blockchain technology can be used to ensure transaction unmodifiable and undeniable. In this paper, we propose a blockchain system that adapts to the limitations of edge devices. The new blockchain system can fairly and efficiently allocate storage resources on edge devices, which makes it scalable. We find the optimal peer nodes for transaction data storage in the blockchain, and propose a recent block storage allocation scheme for quick retrieval of missing blocks. The proposed blockchain system can also reach mining consensus with low energy consumption in edge devices with a new Proof of Stake mechanism. Extensive simulations show that our proposed blockchain system works efficiently in edge environments. On average, the new system uses 15% less time and consumes 64% less battery power when compared with traditional blockchain systems."
TeamNet: A Collaborative Inference Framework on the Edge.,"With significant increases in wireless link capacity, edge devices are more connected than ever, which makes possible forming artificial neural network (ANN) federations on the connected edge devices. Partition is the key to the success of distributed ANN inference while unsolved because of the unclear knowledge representation in most of the ANN models. We propose a novel partition approach (TeamNet) based on the psychologically-plausible competitive and selective learning schemes while evaluating its performance carefully with thorough comparisons to other existing distributed machine learning approaches. Our experiments demonstrate that TeamNet with sockets and transmission control protocol (TCP) significantly outperforms sophisticated message passing interface (MPI) approaches and the state-of-the-art mixture of experts (MoE) approaches. The response time of ANN inference is shortened by as much as 53% without compromising predictive accuracy. TeamNet is promising for having distributed ANN inference on connected edge devices and forming edge intelligence for future applications."
A Lightweight Collaborative Recognition System with Binary Convolutional Neural Network for Mobile Web Augmented Reality.,"Lightweight and precise recognition is a key component of web-based augmented reality (Web AR) applications. Although edge-based distributed deep learning approach is now possible to achieve satisfactory recognition for Web AR applications, it puts significant pressure on the computation and energy consumption of the mobile web browser, especially the app-based embedded browser. Thus, reducing the model size and accelerating the inference are regarded as the two fundamental challenges to enable this edge-based collaborative recognition system efficiently. In this paper, we propose a lightweight collaborative recognition system (LCRS) for Web AR applications. LCRS contributes to three aspects: (1) we design a composite deep neural network for reducing the model size and inference latency by introducing binary convolutional neural network; (2) we provide a joint training method to co-train the general branch and the binary branch; (3) we develop a JavaScript library for the mobile web browser to execute and accelerate inference of the binary branch, which also provides a collaborative mechanism between the mobile web browser and the edge server. We have conducted extensive experiments using several well-known networks and datasets. The experimental results have shown that the proposed system outperforms the existing approaches in terms of reducing the model size by about 16x to 29x, and it also reduces end-to-end latency and outpaces the existing state-of-the-art approaches by over 3x to 60x when applying it in practical Web AR cases."
Dynamic Stale Synchronous Parallel Distributed Training for Deep Learning.,"Deep learning is a popular machine learning technique and has been applied to many real-world problems, ranging from computer vision to natural language processing. However, training a deep neural network is very time-consuming, especially on big data. It has become difficult for a single machine to train a large model over large datasets. A popular solution is to distribute and parallelize the training process across multiple machines using the parameter server framework. In this paper, we present a distributed paradigm on the parameter server framework called Dynamic Stale Synchronous Parallel (DSSP) which improves the state-of-the-art Stale Synchronous Parallel (SSP) paradigm by dynamically determining the staleness threshold at the run time. Conventionally to run distributed training in SSP, the user needs to specify a particular stalenes threshold as a hyper-parameter. However, a user does not usually know how to set the threshold and thus often finds a threshold value through trial and error, which is time-consuming. Based on workers' recent processing time, our approach DSSP adaptively adjusts the threshold per iteration at running time to reduce the waiting time of faster workers for synchronization of the globally shared parameters (the weights of the model), and consequently increases the frequency of parameters updates (increases iteration through-put), which speedups the convergence rate. We compare DSSP with other paradigms such as Bulk Synchronous Parallel (BSP), Asynchronous Parallel (ASP), and SSP by running deep neural networks (DNN) models over GPU clusters in both homogeneous and heterogeneous environments. The results show that in a heterogeneous environment where the cluster consists of mixed models of GPUs, DSSP converges to a higher accuracy much earlier than SSP and BSP and performs similarly to ASP. In a homogeneous distributed cluster, DSSP has more stable and slightly better performance than SSP and ASP, and converges much faster than BSP."
Optimal Admission Control For Secondary Users using Blockchain Technology In Cognitive Radio Networks.,"The optimal admission control for secondary users (SUs) using blockchain technology in a cognitive radio (CR) network is investigated in this paper. When an SU comes to the system, it will decide to either enter the system, or give up its service and then leave the system based on the current number of PU and SU users in the network. SUs are linked as a block in the waiting area to access the spectrum in an opportunistic way without interfering the operations of PUs. When a PU arrives, if the band is occupied by SU(s), the SU service on the band is interrupted and SU(s) is then placed back to the waiting queue. Admitting an SU would bring a reward to the system provider but holding an SU in the system would also incur an expense to the provider. We concentrate on the optimization problem of when to admit or reject an arriving SU in order to achieve the maximum total discounted expected reward for any initial state. By establishing a discounted Continuous Time Markov Decision Process (CTMDP) model, we verify that the optimal policies for admitting SUs are state-related control limit policies. An extensive numerical analysis is implemented to show how to determine the value of optimal threshold and of the corresponding maximum objective function. Especially, a simulation by using the idea of the neural network and machine learning is implemented to show how to determine the optimal threshold value for any new input parameters through learning process for a given input parameters. The idea and the corresponding result of this paper can be applied in designing optimal cognitive radio networks."
Tail Amplification in n-Tier Systems: A Study of Transient Cross-Resource Contention Attacks.,"Fast response time becomes increasingly important for modern web applications (e.g., e-commerce) due to intense competitive pressure. In this paper, we present a new type of Denial of Service (DoS) Attacks in the cloud, MemCA, with the goal of causing performance uncertainty (the long-tail response time problem) of the target n-tier web application while keeping stealthy. MemCA exploits the sharing nature of public cloud computing platforms by co-locating the adversary VMs with the target VMs that host the target web application, and causing intermittent and short-lived cross-resource contentions on the target VMs. We show that these short-lived cross-resource contentions can cause transient performance interferences that lead to large response time fluctuations of the target web application, due to complex resource dependencies in the system. We further model the attack scenario in n-tier systems based on queuing network theory, and analyze cross-tier queue overflow and tail response time amplification under our attacks. Through extensive benchmark experiments in both private and public clouds (e.g., Amazon EC2), we confirm that MemCA can cause significant performance uncertainty of the target n-tier system while keeping stealthy. Specifically, we show that MemCA not only bypasses the cloud elastic scaling mechanisms, but also the state-of-the-art cloud performance interference detection mechanisms."
Robust Profit Maximization with Double Sandwich Algorithms in Social Networks.,"Social networks are becoming important dissemination platforms, and a large body of works have been performed on viral marketing, but most are to maximize the benefits associated with the number of active nodes. In this paper, we study the benefits related to interactions among activated nodes. Furthermore, due to the uncertainty in edge probability estimates in social networks, we propose the robust profit maximization problem to have the best solution in the worst case of probability settings. We design a double sandwich algorithm to this problem and further improve the algorithm with sampling method such that it increases robustness of the output. Through real data sets, we verify the effectiveness of our proposed algorithm."
Intelligent Caching Algorithms in Heterogeneous Wireless Networks with Uncertainty.,"A burgeoning number of wireless devices connecting to the Internet tend to impose a heavy traffic load on the network backbone. Caching the most popular content at the heterogeneous wireless network edge is a promising way to alleviate the network overload. However, to cache the diverse content effectively, a file popularity profile that may not be known in advance to network operators has to be utilized. To tackle the challenge caused by this uncertainty, online learning techniques can be considered. Additionally, in practice, dense small-cell networks are often deployed to maximize spectral efficiency, which will naturally bring overlapping coverage areas among individual small cells. In this paper, we propose to address the content caching problem in a scenario of overlapping coverage areas among small cells while further allowing users distributed in the overlapping area to stochastically choose to connect to the small-cell base station they can reach. We propose two effective and efficient online learning algorithms to address the aforementioned problem and also provide theoretical guarantees. Finally, experiments are conducted to verify the performance of the proposed algorithms practically."
Optimizing the Crowdsourcing-based Bike Station Rebalancing Scheme.,"User dynamics in both spatial and temporal domains bring uncertainty to bike-sharing systems (BSSs) and usually lead to bike imbalance. This may generate out-of-service events due to bike underflow or overflow, at a bike station. In this paper, we recruit workers through crowdsourcing to rebalance loads among bike stations. We assume that workers have their individual sources and destinations, and assign them to move bikes from overflow stations to underflow stations. We partition the complex spatial and temporal problem into a sequence of slices with a fixed duration in the temporal domain. In each slice, we focus on the spatial domain and allocate a pair of overflow/underflow stations to a worker such that the summation of detour cost among workers is minimized. The hardness of finding the min-cost allocation is shown by a reduction from a 3-dimensional matching problem (i.e., matching among workers, overflow stations, and underflow stations). We propose a 3-approximation algorithm for the problem when the detour cost is proportional to the detour distances. Then, the configuration dynamic in the sequence of slices is captured by carefully determining the rebalancing frequency and target for each rebalancing operation. We investigate heuristic approaches to decide rebalancing frequencies and targets over a sequence of slices in order to minimize the total number of bike movements (i.e., the total number of workers), and hence to derive the average total detours per slice. We simulate our algorithms on both real-world and synthetic datasets based on different time-slice granularities. The experiment results show that our approaches can reduce the average total detour per slice."
Data-Driven Small Cell Placement Optimization with Users' Differential Privacy for Wireless NGNs.,"In the coming fifth generation (5G) or beyond 5G next generation networks (NGNs), the small cell deployment is a promising solution to meet the ever increasing demands of mobile devices, and the proliferation of wireless services. The low power base station (BS), such as femtocell BS, is a cost-effective and environmental friendly substitution for the power-hungry macrocell BS. One potentially effective way to deploy those small cells is to use two-tier NGN architecture, where the first-tier carrier can authorize the second-tier carrier's access to users' transmission information database (e.g., uplink/downlink service demands), and thereafter the second-tier carrier can decide how to place small cell BSs according to the mobile users' requirements locally. However, the second-tier carriers/operators for small cell placement may not be trustworthy, and the NGN users' data privacy might be compromised. To address this issue, we integrate differential privacy (DP) preserving techniques into data-driven optimization, and propose a novel scheme that not only preserves the privacy of NGN users' transmission information, but also maximizes the revenue of small cell deployment. Briefly, differential private noises are intentionally added into the users' transmission information database. Based on queries, the second-tier carrier can aggregate a given set of users' differentially private historical data, estimate the users' demands, and formulate the data-driven revenue maximization problem. Given the stochastic programming optimization formulation, we develop feasible solutions and conduct extensive simulations with real-world transmission datasets (i.e., transmission data collected hourly from 3072 4G eNBs deployed in several southern cities of China in 2015) to verify the effectiveness of the proposed scheme."
Concurrent Unrolled Skiplist.,"Skiplist is an important data structure used for storing and managing ordered data. It provides logarithmic time complexity (in list size) for lookup, insert and remove operations with high probability without the need for complex balancing actions. Several algorithms have been proposed for concurrent maintenance of a skiplist using both blocking and non-blocking synchronization techniques. In this work, we propose a new algorithm for maintaining a concurrent skiplist that uses two techniques to boost performance. The first technique, referred to as ""unrolling"", involves storing multiple key-value pairs in the same node. The second technique involves using an advanced locking primitive, based on ""group mutual exclusion"", to allow certain operations to work on the same node concurrently. In our experiments, our concurrent skiplist consistently outperformed existing concurrent skiplists by as much as 90% in some cases."
Generative Policies for Coalition Systems - A Symbolic Learning Framework.,"Policy systems are critical for managing missions and collaborative activities carried out by coalitions involving different organizations. Conventional policy-based management approaches are not suitable for next-generation coalitions that will involve not only humans, but also autonomous computing devices and systems. It is critical that those parties be able to generate and customize policies based on contexts and activities. This paper introduces a novel approach for the autonomic generation of policies by autonomous parties. The framework combines context free grammars, answer set programs, and inductionbased learning. It allows a party to generate its own policies, based on a grammar and some semantic constraints, by learning from examples. The paper also outlines initial experiments in the use of such a symbolic approach and outlines relevant research challenges, ranging from explainability to quality assessment of policies."
Applying Differential Privacy Mechanism in Artificial Intelligence.,"Artificial Intelligence (AI) has attracted a large amount of attention in recent years. However, several new problems, such as privacy violations, security issues, or effectiveness, have been emerging. Differential privacy has several attractive properties that make it quite valuable for AI, such as privacy preservation, security, randomization, composition, and stability. Therefore, this paper presents differential privacy mechanisms for multi-agent systems, reinforcement learning, and knowledge transfer based on those properties, which proves that current AI can benefit from differential privacy mechanisms. In addition, the previous usage of differential privacy mechanisms in private machine learning, distributed machine learning, and fairness in models is discussed, bringing several possible avenues to use differential privacy mechanisms in AI. The purpose of this paper is to deliver the initial idea of how to integrate AI with differential privacy mechanisms and to explore more possibilities to improve AIs performance."
AI Blockchain Platform for Trusting News.,"An interdisciplinary effort is needed for solving the fake news crisis, because the solutions depend not only on AI, but also on social mechanisms. In this paper, we propose an AI blockchain platform to build a strong collaboration among AI blockchain researchers and news media to advance the research fighting against fake news. This platform will provide journalists with blockchain crowd-sourced and AI validated factual data on emerging news. This platform will gather blockchain traced data and AI tools that can provide pointers to the original data sources, news propagation path, AI analyzed experts to consult on a given topic. This will provide journalists with cheaper and reliable sources of information in the Internet social media age. So that factual-sourced reporting can outpace the spread of fake news on social media which will encourage factual news sources as a way to value and promote truth for society. The technical contributions of this paper are (1) mechanism building the factual news database, (2) mechanism generating the news blockchain supply chain graph, and (3) AI blockchain based crowd sourcing fake news ranking mechanisms (4) AI blockchain platform for trusting news ecosystem. (5) reviewing the state of fake news research from the technology and social aspects, and providing list of key research issues and technical challenges."
The Best of Both Worlds: Challenges in Linking Provenance and Explainability in Distributed Machine Learning.,"Machine learning experts prefer to think of their input as a single, homogeneous, and consistent data set. However, when analyzing large volumes of data, the entire data set may not be manageable on a single server, but must be stored on a distributed file system instead. Moreover, with the pressing demand to deliver explainable models, the experts may no longer focus on the machine learning algorithms in isolation, but must take into account the distributed nature of the data stored, as well as the impact of any data pre-processing steps upstream in their data analysis pipeline. In this paper, we make the point that even basic transformations during data preparation can impact the model learned, and that this is exacerbated in a distributed setting. We then sketch our vision of end-to-end explainability of the model learned, taking the pre-processing into account. In particular, we point out the potentials of linking the contributions of research on data provenance with the efforts on explainability in machine learning. In doing so, we highlight pitfalls we may experience in a distributed system on the way to generating more holistic explanations for our machine learning models."
Eugene: Towards Deep Intelligence as a Service.,"The paper discusses an emerging suite of machine intelligence services that are of increasing importance in the highly instrumented world of the Internet of Things (IoT). The suite, called Eugene, would offer a form of intelligent behavior (based on deep neural networks) to otherwise simple embedded devices; the clients of the service. These devices would benefit from service resources to learn from data and to perform intelligent inference, classification, prediction, and estimation tasks that they are too limited to carry out on their own. The paper discusses the taxonomy of such services and the state of implementation, as well as the various challenges entailed, including scheduling, caching (of intelligent functions), and cooperative learning."
Polystore++: Accelerated Polystore System for Heterogeneous Workloads.,"Modern real-time business analytic consist of heterogeneous workloads (e.g., database queries, graph processing, and machine learning). These analytic applications need programming environments that can capture all aspects of the constituent workloads (including data models they work on and movement of data across processing engines). Polystore systems suit such applications; however, these systems currently execute on CPUs and the slowdown of Moore's Law means they cannot meet the performance and efficiency requirements of modern workloads. We envision Polystore++, an architecture to accelerate existing polystore systems using hardware accelerators (e.g., FPGAs, CGRAs, and GPUs). Polystore++ systems can achieve high performance at low power by identifying and offloading components of a polystore system that are amenable to acceleration using specialized hardware. Building a Polystore++ system is challenging and introduces new research problems motivated by the use of hardware accelerators (e.g., optimizing and mapping query plans across heterogeneous computing units and exploiting hardware pipelining and parallelism to improve performance). In this paper, we discuss these challenges in detail and list possible approaches to address these problems."
Global Data Plane: A Federated Vision for Secure Data in Edge Computing.,"We propose a federated edge-computing architecture for management of data. Our vision is to enable a service provider model for ""data-services"", where a user can enter into economic agreements with an infrastructure maintainer to provide storage and communication of data, without necessarily trusting the infrastructure provider. Toward this vision, we present cryptographically hardened cohesive collections of data items called DataCapsules, and an overview of the underlying federated architecture, called Global Data Plane."
Memory Disaggregation: Research Problems and Opportunities.,"Memory usage imbalance has been consistently observed in many virtualized Clouds and production datacenters. Such temporal memory utilization variance is a major root cause for excessive paging and thrashing on virtual servers even though there are sufficient idle memory on the same node or in the Cloud cluster. Memory disaggregation is an emerging research and development endeavor towards addressing these memory usage imbalance problems. This paper first defines and characterizes the concept of memory disaggregation, and discusses the demands and challenges of efficient memory disaggregation in cloud datacenters. It then examines some promising research issues, design choices and directions to overcome some of the challenges posed by memory disaggregation. Specifically, it proposes two major new research challenges and solution directions for enabling elastic, on-demand disaggregated memory orchestration: (1) virtual server memory and node level memory co-design and (2) local memory and remote memory co-design. A brief description of two ongoing research projects is provided for both solution directions. The paper ends with a brief discussion of other advanced and emerging memory and storage technologies and potential opportunities for memory disaggregation."
Software-Defined Infrastructure for Decentralized Data Lifecycle Governance: Principled Design and Open Challenges.,"Exploring and mining the explosive burst of ""big data"" has already generated a lot of innovative applications, especially the recent advances of AI applications, and thus produced big values to the human society and civilization. However, due to the centralized patterns of data governance activities, including creation, sharing, exchange, management, analytics, tracing, and accounting, the potential values of big data distributed on the Internet are far away from being adequately explored. The recent announcement of data protection policies/laws such as GDPR makes the problem even more challenging. We are now at a moment of truth where the data governance infrastructure should be reconsidered and redesigned. In this paper, we propose a software-defined infrastructure design in a decentralized fashion: data owners are able to implement and deploy their own rules to the application systems where the data are produced for further governance activities. Such a fashion is quite similar to the popular software-defined networking where users are allowed to deploy rules of switches and customize the use. Our principled infrastructure design can radically reform the current data governance activities into a decentralized topology. On the one hand, data can be separated from the application that generates the data, and data owners can have the full rights to decide where their data should be stored and how the data can be shared. On the other hand, data users can search, discover, integrate, and analyze the data from various data sources according to their application requirements and scenarios. As a result, we argue that our infrastructure can establish a new generation of responsive decentralized data governance that can promote the innovation of linking data to better adapt the open environment and diverse user requirements. With this perspective, we briefly discuss some key insights and enumerate several related new technologies and open challenges."
Distributed Mega-Datasets: The Need for Novel Computing Primitives.,"With the ongoing digitalization, an increasing number of sensors is becoming part of our digital infrastructure. These sensors produce highly, even globally, distributed data streams. The aggregate data rate of these streams far exceeds local storage and computing capabilities. Yet, for radical new services (e.g., predictive maintenance and autonomous driving), which depend on various control loops, this data needs to be analyzed in a timely fashion. In this position paper, we outline a system architecture that can effectively handle distributed mega-datasets using data aggregation. Hereby, we point out two research challenges: The need for (1) novel computing primitives that allow us to aggregate data at scale across multiple hierarchies (i.e., time and location) while answering a multitude of a priori unknown queries, and (2) transfer optimizations that enable rapid local and global decision making."
"Big RDF Data Storage, Computation, and Analysis: A Strawman's Arguments.","RDF data is big and continues to grow rapidly. RDF data sets are typically viewed as heterogeneous graph data sets with complex correlations and multifaceted heterogeneity. Although there have been flurry of research on processing and analyzing RDF data, efficient storage, computation, analysis of big and growing RDF data continue to challenge multiple computer science disciplines, ranging from systems, network computing, data management to data analytics. In this paper, we make use of strawman arguments on big RDF data challenges with respect to storage, computation, analysis, focusing on identifying the grand challenges in developing high performance RDF storage, parallel computation, efficient distribution, and smart RDF analytics. We attempt to answer a number of important and frequently asked questions: (1) Do we really need RDF-specific storage techniques and algorithms for building efficient and high performance RDF stores? (2) How hard can it be to effectively parallelize big RDF data for high performance storage, computation, and mining? (3) What type of distribution models can we employ to scale RDF processing models and algorithms for real-time querying (subgraph pattern matching) and mining of big RDF data? (4) Can machine learning algorithms be leveraged to effectively mine and dive into RDF data? We attempt to answer these questions by sharing some of our results, our unique experiences, and our lessons learned from a collection of research projects in RDF systems research and development. We will also discuss our ongoing research endeavors, potential applications and avenues for future work."
30 Sensors to Mars: Toward Distributed Support Systems for Astronauts in Space Habitats.,"In October 2017, an international crew participated in an emulated Mars colonization mission. For two weeks, they stayed confined in a special complex, a so-called analog habitat, where they were isolated from the outside world, including a lack of natural lighting and exterior noises, and lived on particularly adjusted Martian time. The mission followed a strict schedule, involving actual scientific work and activities envisioned as necessary for survival and exploration of the red planet. The main objective was to study the behavior and group dynamics of the crew in conditions recreating colonization of Mars, albeit under some unique circumstances compared to previous similar experiments. What was also special about the mission was the use of sociometric methods utilizing custom pervasive sensing solutions that we had built and deployed to complement classic methods based on self-reports and interviews. Based on that experiment, in this paper we contribute twofold. First, we share our deployment experiences to highlight the potential of pervasive distributed sensing systems in sociometric studies of habitat-based missions. The examples presented to this end include quantitative results that we obtained, among others, on social interactions between the astronauts, the impact of atypical situations on the crew, and the ergonomics of the habitat. Second, drawing from the experiences, in cooperation with the astronauts we attempt to highlight some unique challenges that space habitats pose for distributed support systems, such as ours. Among others, the challenges pertain to system deployment, autonomy, resilience, and flexibility. We believe that these challenges and, in general, space colonization constitute exciting research opportunities for the distributed systems community."
Machine Learning + Distributed IoT = Edge Intelligence.,"Internet-of-Things (IoT) systems provide large-scale sensor networks that can be used to monitor and analyze a wide range of physical systems. IoT systems can generate huge volumes of data that needs to be dealt with in a timely fashion. Machine learning (ML) provides compelling techniques for the analysis of large, complex data sets. Many existing machine learning systems operate in the cloud. However, bandwidth, power, latency, privacy, and other issues often require IoT systems to apply ML techniques at multiple levels of the network hierarchy. This paper describes important characteristics of edge intelligence and identifies several important research challenges related to machine learning + distributed IoT systems."
Workflow Environments for Advanced Cyberinfrastructure Platforms.,"Progress in science is deeply bound to the effective use of high-performance computing infrastructures and to the efficient extraction of knowledge from vast amounts of data. Such data comes from different sources that follow a cycle composed of pre-processing steps for data curation and preparation for subsequent computing steps, and later analysis and analytics steps applied to the results. However, scientific workflows are currently fragmented in multiple components, with different processes for computing and data management, and with gaps in the viewpoints of the user profiles involved. Our vision is that future workflow environments and tools for the development of scientific workflows should follow a holistic approach, where both data and computing are integrated in a single flow built on simple, high-level interfaces. The topics of research that we propose involve novel ways to express the workflows that integrate the different data and compute processes, dynamic runtimes to support the execution of the workflows in complex and heterogeneous computing infrastructures in an efficient way, both in terms of performance and energy. These infrastructures include highly distributed resources, from sensors and instruments, and devices in the edge, to High-Performance Computing and Cloud computing resources. This paper presents our vision to develop these workflow environments and also the steps we are currently following to achieve it."
"From Autonomous Vehicles to Vehicular Clouds: Challenges of Management, Security and Dependability.","Autonomous vehicles have the potential to enhance road safety, reduce traffic pressure and improve the driving experience. With the on-board sensors, compute units, storage devices, and communication modules, autonomous vehicles are becoming integrated information systems. Compared to conventional centralized approaches and traditional clouds, the emerging vehicular clouds (v-clouds) technology is a more promising solution for utilizing such rich resources. In v-clouds, vehicles can communicate with one another, form self-organized vehicular ad-hoc networks (VANETs), collect real-time sensing data, conduct intensive computation, and disseminate information. However, the highly dynamic and heterogeneous nature of autonomous vehicles raises many issues when designing v-cloud systems. In this paper, we focus on the challenges in designing v-cloud computing architectures, providing effective routing protocols, securing v-cloud environments and enhancing the dependability of v-clouds. We review the state of the art and discuss open research issues."
HPDL: Towards a General Framework for High-performance Distributed Deep Learning.,"With growing scale of the data volume and neural network size, we have come into the era of distributed deep learning. High-performance training and inference on distributed computing systems has been attracting increasing research attention in both academia and industry. Meanwhile, diversity of existing machine learning frameworks (e.g. TensorFlow, Pytorch and MXNet) and the explosion of deep learning hardwares (e.g. CPUs, GPUs, FPGAs and ASICs) bring more challenges for users to leverage new deep learning technologies and accelerating capability of hardware devices. We firstly search around the state-of-the-art work in the area which open our mind to take a vision upon the future deep learning framework. Then, we propose HPDL, a general framework for high-performance distributed deep learning which is compatible with existing frameworks and adaptive to various hardware architectures. At last, we discuss and foresee the key technologies fulfilling high-performance and large-scale deep learning, including optimization algorithm, hybrid communication mechanism, model parallelization, resource scheduling and single-node execution optimization."
"Towards Resilient Internet of Things: Vision, Challenges, and Research Roadmap.","Internet of Things (IoT) systems open up massive versatility and opportunity to our world. Providing solutions for smart cities, healthcare, energy, and mobility, such systems increasingly permeate critical aspects of human activity. In a flourish of growth, these complex systems run software, are dynamic, without stable spatial and temporal boundaries, and involve mostly independent software components with different lifespans and evolution models. IoT systems provide data-centric, device-centric and service-centric functionalities that are subject to continuous disruption, under limitations such as resource-constrained devices, platforms heterogeneity, deployment in adverse environments and administrative domains. As these systems evolve and gain complexity, resilience becomes a crucial system property. Bolstering resilience entails understanding and systematically managing dynamic behavior and decentralizing operations. We advocate that to systematically engineer resilience in IoT systems, a complete rethink is necessary regarding their design and operation. In this paradigm shift, systems demand conceptual frameworks, techniques, and mathematically-backed formalisms to treat change and achieve decentralization. We outline a vision for addressing fundamental challenges that software engineering and distributed systems research encounters when building resilient IoT systems. Within a roadmap, we identify techniques and methods that can be leveraged to maintain resilience in the face of disruption, especially in the absence of central control and persistently at the system's runtime."
The AtLarge Vision on the Design of Distributed Systems and Ecosystems.,"High-quality designs of distributed systems and services are essential for our digital economy and society. Threatening to slow down the stream of working designs, we identify the mounting pressure of scale and complexity of (eco-)systems, of ill-defined and wicked problems, and of unclear processes, methods, and tools. We envision design itself as a core research topic in distributed systems, to understand and improve the science and practice of distributed (eco-)system design. Toward this vision, we propose the AtLarge design framework, accompanied by a set of 8 core design principles. We also propose 10 key challenges, which we hope the community can address in the following 5 years. In our experience so far, the proposed framework and principles are practical, and lead to pragmatic and innovative designs for large-scale distributed systems."
Social Middleware for Civic Engagement.,"Civic engagement refers to any collective action towards the identification and solving of public issues. Current civic technologies are traditional Web-or mobile-based platforms that make difficult, or just impossible, the participation of citizens via different communication technologies. Moreover, connected objects sensing physical-world data can nourish participatory processes by providing physical evidence to citizens; however, leveraging these data is not direct and still a time-consuming process for civic technologies developers. This paper introduces the concept of social middleware for civic engagement. Social middleware allows citizens to engage in participatory processes - supported by civic technologies-via their favorite communication tools, and to interact not only with other citizens but also with relevant connected objects and software platforms. The mission of social middleware goes beyond the connection of all these heterogeneous entities. It aims at easing the implementation of distributed applications oriented toward civic engagement by featuring dedicated built-in services."
Context Recognition of Humans and Objects by Distributed Zero-Energy IoT Devices.,"Understanding humans and its environment is a key enabler of smart, intelligent applications and services for a future smart society. To deploy such services in our ambient environment, it is expected to fully utilize battery-less and maintenance-free IoT devices and technologies for more ambient, distributed computing. In recent years, Wi-Fi-based communications are becoming more energy-efficient, and channel state information (CSI) has the potential to sense more detailed information about the things in the real world. Besides, ambient backscatter has appeared as a promising technology for zero-energy sensing and communications. Leveraging those state-of-the-art technologies, energy harvested IoT devices for context recognition of humans and objects will be in reality. A significant challenge is how to make use of inferior, less-powerful zero-energy IoT devices to achieve processing of interest, i.e., accurate recognition of humans and objects, while a single device does not work. Therefore, we consider orchestrating distributed tiny IoT devices for both sensing and communications. Particularly, distributed machine learning in the local environment will achieve highly promising sensing in our ambient environment. In this paper, we survey the state-of-the-art technologies for zero-energy sensing and communications in the context of humans and objects sensing and recognition. Then, we address the challenges to be tackled in terms of such distributed, intelligent sensing using zero-energy devices. Finally, we introduce the concept of utilizing distributed IoT devices, followed by the statement about our ongoing work toward future zero-energy sensing and processing."
LATTICE: A Framework for Optimizing IoT System Configurations at the Edge.,"The Internet of Things is expected to contribute to a ""smarter world"" by connecting the physical to the virtual, i.e., enabling advanced knowledge engineering over the big data gathered about the physical world. However, such a promise comes along with high resource consumption, spanning the network, storage and computational resources, not to mention possible security and privacy threats. As a result, it tends to be admitted that the IoT smartness will not be accommodated at scale by a centralized cloud-based approach. Instead, the deployment of IoT systems needs to leverage a highly distributed system architecture, which optimizes the distribution of the computation -from the edge to the cloud-according to the unique business requirements in terms of financial cost, latency, availability, etc. Toward that goal, this paper introduces the LATTICE framework, which aims at taming the complexity of configuring edge-based IoT systems. LATTICE builds upon ontologies that have proven useful to characterize the constituents of IoT systems in the required domain-specific way. However, LATTICE also revisits the exploitation of ontologies -i.e., the formal description of the real world, spanning the physical and cyber entities-across the development life-cycle of the IoT systems. As a first evidence, this paper introduces an automated approach to the optimization of IoT system configurations at the edge, provided the ontological description of the target IoT system."
A Vision for Managing Extreme-Scale Data Hoards.,"Scientific data collections grow ever larger, both in terms of the size of individual data items and of the number and complexity of items. To use and manage them, it is important to directly address issues of robust and actionable provenance. We identify three key drivers as our focus: managing the size and complexity of metadata, lack of a priori information to match usage intents between publishers and consumers of data, and support for campaigns over collections of data driven by multi-disciplinary, collaborating teams. We introduce the Hoarde abstraction as an attempt to formalize a way of looking at collections of data to make them more tractable for later use. Hoarde leverages middleware and systems infrastructures for scientific and technical data management. Through the lens of a select group of challenging data usage scenarios, we discuss some of the aspects of implementation, usage, and forward portability of this new view on data management."
When FPGA-Accelerator Meets Stream Data Processing in the Edge.,"Today, stream data applications represent the killer applications for Edge computing: placing computation close to the data source facilitates real-time analysis. Previous efforts have focused on introducing light-weight distributed stream processing (DSP) systems and dividing the computation between Edge servers and the clouds. Unfortunately, given the limited computation power of Edge servers, current efforts may fail in practice to achieve the desired latency of stream data applications. In this vision paper, we argue that by introducing FPGAs in Edge servers and integrating them into DSP systems, we might be able to realize stream data processing in Edge infrastructures. We demonstrate that through the design, implementation, and evaluation of F-Storm, an FPGA-accelerated and general-purpose distributed stream processing system on Edge servers. F-Storm integrates PCIe-based FPGAs into Edge-based stream processing systems and provides accelerators as a service for stream data applications. We evaluate F-Storm using different representative stream data applications. Our experiments show that compared to Storm, F-Storm reduces the latency by 36% and 75% for matrix multiplication and grep application. It also obtains 1.4x and 2.1x improvement for these two applications, respectively. We expect this work to accelerate progress in this domain."
XLF: A Cross-layer Framework to Secure the Internet of Things (IoT).,"The burgeoning Internet of Things (IoT) has offered unprecedented opportunities for innovations and applications that are continuously changing our life. At the same time, the large amount of pervasive IoT applications have posed paramount threats to the user's security and privacy. While a lot of efforts have been dedicated to deal with such threats from the hardware, the software, and the applications, in this paper, we argue and envision that more effective and comprehensive protection for IoT systems can only be achieved via a cross-layer approach. As such, we present our initial design of XLF, a cross-layer framework towards this goal. XLF can secure the IoT systems not only from each individual layer of device, network, and service, but also through the information aggregation and correlation of different layers."
OpenEI: An Open Framework for Edge Intelligence.,"In the last five years, edge computing has attracted tremendous attention from industry and academia due to its promise to reduce latency, save bandwidth, improve availability, and protect data privacy to keep data secure. At the same time, we have witnessed the proliferation of AI algorithms and models which accelerate the successful deployment of intelligence mainly in cloud services. These two trends, combined together, have created a new horizon: Edge Intelligence (EI). The development of EI requires much attention from both the computer systems research community and the AI community to meet these demands. However, existing computing techniques used in the cloud are not applicable to edge computing directly due to the diversity of computing sources and the distribution of data sources. We envision that there missing a framework that can be rapidly deployed on edge and enable edge AI capabilities. To address this challenge, in this paper we first present the definition and a systematic review of EI. Then, we introduce an Open Framework for Edge Intelligence (OpenEI), which is a lightweight software platform to equip edges with intelligent processing and data sharing capability. We analyze four fundamental EI techniques which are used to build OpenEI and identify several open problems based on potential research directions. Finally, four typical application scenarios enabled by OpenEI are presented."
The Importance of Being Thing Or the Trivial Role of Powering Serious IoT Scenarios.,"In this article, we call for a ""Walk Before You Run"" adjustment in the Internet-of-Things (IoT) research and development exercise. Without first settling the quest for what thing is or could be or do, we run the risk of presumptuous visions, or hypes, that can only fail the realities and limits of what is actually possible, leading to customers and consumers confusion as well as market hesitations. Specifically, without a carefully-designed Thing architecture in place, it will be very difficult to find the ""magic"" we are so addicted and accustomed to - programming! Programming the IoT, as we once programmed the mainframe, the workstation, the PC and the mobile devices, is the natural way to realize a fancy IoT scenario or an application. Without Thing architectures and their enablement of new programming models for IoT - we will continue to only envision fancy scenarios but unable to unleash the IoT full potential. This article raises these concerns and provides a view into the future by first looking back into our short history of pervasive computing. The article focuses on the domain of ""Personal"" IoT and will address key new requirements for such Thing architecture. Also, practicing what we preach, we present our ongoing efforts on the Atlas Thing Architecture showing how it supports a variety of thing notions, and how it enables novel models for programmability."
A Vision for a Spot Market for Interdomain Connectivity.,"In this paper we consider an alternative possibility for routing money in the Internet ecosystem based on a spot market for interconnection service that operates alongside, or in addition to, the traditional contract service model. Recent work by others showed that under certain assumptions, enabling transit providers (i.e., networks that carry packets between networks) to sell their excess capacity on a best-effort basis improves both provider profit and consumer surplus. While prior work only focused on pricing strategies, we explore the technical feasibility of such a market in this paper. We consider what is needed to make such a spot market possible and focus on the interaction between technical and economic considerations. In particular, we describe two approaches that demonstrate how economic software defined exchanges (ESDXs) can be used as trusted intermediaries to tie the forwarding service to the flow of money."
Rethinking Home Networks in the Ultrabroadband Era.,"The advent of ultrabroadband Internet connectivity brings a 2-3 orders of magnitude jump in the capacity of access networks (a.k.a. the ""last mile""). Beyond mere capacity increase, this leap represents a qualitative shift in the overall Internet environment. Therefore, we argue that only by seizing the opportunity to re-think the way we structure network applications and services can we realize the full potential ultrabroadband provides. Specifically, with ultrabroadband residential networks, we have the opportunity to re-center our digital lives around our residence, similar to how our physical lives generally center around our homes. To this end, we introduce a new appliance in home networks-a ""home point of presence""-that provides a variety of services to the users in the house regardless of where they are physically located and connected to the network. We illustrate the utility of this appliance by discussing a range of new services that both bring new functionality to the users and improve performance of existing applications."
Providing Cooperative Data Analytics for Real Applications Using Machine Learning.,"This paper presents a data analytics system which determines optimal analytics algorithms by selectively testing a wide range of different algorithms and optimizing parameters using Transformer-Estimator Graphs. Our system is applicable to situations in which multiple clients need to perform calculations on the same data sets. Our system allows clients to cooperate in performing analytics calculations by sharing results and avoiding redundant calculations. Computations may be distributed across multiple nodes, including both client and server nodes. We provide multiple options for dealing with changes to data sets depending upon the data consistency requirements of applications. Another key contribution of our work is the Transformer-Estimator Graph, a system for specifying a wide variety of options to use for machine learning modeling and prediction. We show how Transformer-Estimator Graphs can be used for analyzing time series data. A key feature that we provide for making our system easy to use is solution templates which are customized to problems in specific domains."
"Dependable Public Ledger for Policy Compliance, a Blockchain Based Approach.","The ever increasing amount of personal data accumulated by companies offering innovative services through the cloud, Internet of Things devices and, more recently, social robots has started to alert consumers and legislative authorities. In the advent of the first modern laws trying to protect user privacy, such as the European Union General Data Protection Regulation, it is still unclear what are the tools and techniques that the industry should employ to comply with regulations in a transparent and cost effective manner. We propose an architecture for a public blockchain based ledger that can provide strong evidence of policy compliance. To address scalability concerns, we define a new type of off-chain channel that is based on general state channels and offers verification for information external to the blockchain. We also create a model of the business relationships in a smart home setup that includes a social robot and suggest a sticky policy mechanism to monitor cross-boundary policy compliance."
"Please, do not Decentralize the Internet with (Permissionless) Blockchains!","The old mantra of decentralizing the Internet is coming again with fanfare, this time around the blockchain technology hype. We have already seen a technology supposed to change the nature of the Internet: peer-to-peer. The reality is that peer-to-peer naming systems failed, peer-to-peer social networks failed, and yes, peer-to-peer storage failed as well. In this paper, we will review the research on distributed systems in the last few years to identify the limits of open peer-to-peer networks. We will address issues like system complexity, security and frailty, instability and performance. We will show how many of the aforementioned problems also apply to the recent breed of permissionless blockchain networks. The applicability of such systems to mature industrial applications is undermined by the same properties that make them so interesting for a libertarian audience: namely, their openness, their pseudo-anonymity and their unregulated cryptocurrencies. As such, we argue that permissionless blockchain networks are unsuitable to be the substrate for a decentralized Internet. Yet, there is still hope for more decentralization, albeit in a form somewhat limited with respect to the libertarian view of decentralized Internet: in cooperation rather than in competition with the superpowerful datacenters that dominate the world today. This is derived from the recent surge in interest in byzantine fault tolerance and permissioned blockchains, which opens the door to a world where use of trusted third parties is not the only way to arbitrate an ensemble of entities. The ability of establish trust through permissioned blockchains enables to move the control from the datacenters to the edge, truly realizing the promises of edge-centric computing."
Towards Seamless Configuration Tuning of Big Data Analytics.,"The execution of distributed data processing workloads (such as those running on top of Hadoop or Spark) in cloud environments presents a unique opportunity to explore multiple trade-offs between elasticity (and types of resources being allocated), overall runtime and total costs. However, beyond high-level constraints and objectives, it's not the end-users who should be mainly concerned with those optimizations, but the cloud providers. They have both the vantage point to collect actionable information, economies of scale and position to adjust parameters when dynamic conditions change, in order to fulfil SLOs that go beyond classic measures of latency and throughput. This is at odds with the existing approach of making software (including the interfaces to the cloud and the processing frameworks) as configurable as possible. We propose that rather than configurability, self-tunability (or the illusion of it as far as the end-user is concerned) is a better long-term goal."
Xyreum: A High-Performance and Scalable Blockchain for IIoT Security and Privacy.,"As cyber attacks to Industrial Internet of Things (IIoT) remain a major challenge, blockchain has emerged as a promising technology for IIoT security due to its decentralization and immutability characteristics. Existing blockchain designs, however, introduce high computational complexity and latency challenges which are unsuitable for IIoT. This paper proposes Xyreum, a new high-performance and scalable blockchain for enhanced IIoT security and privacy. Xyreum uses a Time-based Zero-Knowledge Proof of Knowledge (T-ZKPK) with authenticated encryption to perform Mutual Multi-Factor Authentication (MMFA). T-ZKPK properties are also used to support Key Establishment (KE) for securing transactions. Our approach for reaching consensus, which is a blockchain group decision-making process, is based on lightweight cryptographic algorithms. We evaluate our scheme with respect to security, privacy, and performance, and the results show that, compared with existing relevant blockchain solutions, our scheme is secure, privacy-preserving, and achieves a significant decrease in computation complexity and latency performance with high scalability. Furthermore, we explain how to use our scheme to strengthen the security of the REMME protocol, a blockchain-based security protocol deployed in several application domains."
Predicting the Timing and Quality of Responses in Online Discussion Forums.,"We consider the problem of jointly predicting the quality and timing of responses to questions asked in online discussion forums. While prior work has focused on identifying users most likely to answer and/or to provide the highest quality answers to a question, the promptness of the response is also a key factor of user satisfaction. To address this, we propose point process and neural network-based algorithms for three prediction tasks regarding a user's response to a question: whether the user will answer, the net votes that will be received on the answer, and the time that will elapse before the answer. These algorithms learn over a set of 20 features we define for each pair of user and question that quantify both topical and structural aspects of the forums, including discussion post similarities and social centrality measures. Through evaluation on a Stack Overflow dataset consisting of 20,000 question threads, we find that our method outperforms baselines on each prediction task by more than 20%. We also find that the importance of the features varies depending on the task and the amount of historical data available for inference. At the end, we design a question recommendation system that incorporates these predictions to jointly optimize response quality and timing in forums subject to user constraints."
Strengthening the Positive Effect of Viral Marketing.,"In traditional viral marketing, the goal is to reach out to the maximum number of people. However, some studies have demonstrated that spreading a product indiscriminately in a network can cause some counter effect because it may reach people who evaluate it negatively. In this paper, we study how to make use of social networks to avoid negative people so that the 'positive effect' of viral marketing can be maximized, and this optimization problem is called Strengthening the Positive Effect (SPE). SPE has a non-monotone and non-submodular objective function, and it is NP-hard to be approximately solved with any positive factor. Although SPE is almost impossible to solve approximately, we make the pioneer contribution by discovering that: 1) The almost optimal solution is obtainable in some network; 2) For the general network, a polynomial algorithm that yields a multiplicative guarantee is also possible under a reasonable assumption. We test our solution on various realworld social networks with a comprehensive set of experiments. The result affirms that besides its performance analyzability, our solution is more scalable than the current heuristic."
HashCore: Proof-of-Work Functions for General Purpose Processors.,"Over the past five years, the rewards associated with mining Proof-of-Work blockchains have increased substantially. As a result, miners are heavily incentivized to design and utilize Application Specific Integrated Circuits (ASICs) that can compute hashes far more efficiently than existing general purpose hardware. Currently, it is difficult for most users to purchase and operate ASICs due to pricing and availability constraints, resulting in a relatively small number of miners with respect to total user base for most popular cryptocurrencies. In this work, we aim to invert the problem of ASIC development by constructing a Proof-of-Work function for which an existing general purpose processor (GPP, such as an x86 IC) is already an optimized ASIC. In doing so, we will ensure that any would-be miner either already owns an ASIC for the Proof-of-Work system they wish to participate in or can attain one at a competitive price with relative ease. In order to achieve this, we present HashCore, a Proof-of-Work function composed of ""widgets"" generated pseudo-randomly at runtime that each execute a sequence of general purpose processor instructions designed to stress the computational resources of such a GPP. The widgets will be modeled after workloads that GPPs have been optimized for, for example, the SPEC CPU 2017 benchmark suite for x86 ICs, in a technique we refer to as inverted benchmarking. We provide a proof that HashCore is collision-resistant regardless of how the widgets are implemented. We observe that GPP designers/developers essentially create an ASIC for benchmarks such as SPEC CPU 2017. By modeling HashCore after such benchmarks, we create a Proof-of-Work function that can be run most efficiently on a GPP, resulting in a more accessible, competitive, and balanced mining market."
Read-Uncommitted Transactions for Smart Contract Performance.,"Smart contract transactions demonstrate issues of performance and correctness that application programmers must work around. Although the blockchain consensus mechanism approaches ACID compliance, use cases that rely on frequent state changes are impractical due to the block publishing interval of O(10^1) seconds. The effective isolation level is Read-Committed, only revealing state transitions at the end of the block interval. Values read may be stale and not match program order, causing many transactions to fail when a block is committed. This paper perceives the blockchain as a transactional data structure, using this analogy in the development of a new algorithm, Hash-Mark-Set (HMS), that improves transaction throughput by providing a Read-Uncommitted view of state variables. HMS creates a directed acyclic graph (DAG) from the pending transaction pool. The transaction order derived from the DAG is used to provide a Read-Uncommitted view of the data for new transactions, which enter the DAG as they are received. An implementation of HMS is provided, interoperable with Ethereum and ready for use in smart contracts. Over a wide range of transaction mixes, HMS is demonstrated to improve throughput. A side product of the implementation is a new technique, Runtime Argument Augmentation (RAA), that allows smart contracts to communicate with external data services before submitting a transaction. RAA has use cases beyond HMS and can serve as a lightweight replacement for blockchain oracles."
Kaleidoscope: A Crowdsourcing Testing Tool for Web Quality of Experience.,"Today's webpages development cycle consists of constant iterations with the goal to improve user retention, time spent on site, and overall quality of experience. Big companies like Google, Facebook, Amazon, etc. invest a lot of time and money to perform online testing. The prohibitive costs of these approaches are an entry barrier for smaller players. Further, the lack of a substantial user-base can be problematic to ensure statistical significance within a reasonable duration. In this paper we propose Kaleidoscope, an automated tool to evaluate Web features at a large scale, quickly, accurately, and at a reasonable price. Kaleidoscope can test two crucial user-perceived Web features - the style and page loading. As far as we know, it is the first testing tool to replay page loading by controlling visual changes on a webpage. Kaleidoscope allows to concurrently load a webpage in two versions (e.g., different fonts, with vs without ads) that are shown to a participant side-by-side. Further, Kaleidoscope also allows a participant to interact with each webpage version and provide feedback, e.g., respond to a questionnaire previously prepared by an ""experimenter"". Kaleidoscope supports both voluntary and paid testers from FigureEight, a popular crowdsourcing platform. Using hundreds of FigureEight testers, we validate that Kaleidoscope matches the accuracy of trusted in-lab tests while providing results about 12x faster (and arguably at a lower cost) than A/B testing. Finally, we showcase how to use Kaleidoscope's page loading feature to study the user-perceived page load time (uPLT) of a webpage."
Upsampling Inertial Sensor Data from Wearable Smart Devices using Neural Networks.,"Inertial sensor data collected from wearable smart devices such as smartwatches are expected to be used in various smart applications such as video game controllers, hand drawing, hand writing, gestural input devices, human activity recognition, and remote communication using sign language. However, since the maximum sampling rate of inertial sensors in commercial smartwatches is restricted, capturing fine-grained body movements using the low-sampled signals is difficult for these sensors. Therefore, this study proposes a new method for generating high sampling rate signals from the low-sampled signals by upsampling the low-sampled signals using interpolation with an artificial neural network. Because it is impossible to obtain ""non-existent"" data from low-sampled signals according to the information theory, we estimate these data from experience, i.e., using high-sampled signals prepared in advance for training. This is possible because trajectories of a sensor are restricted by the skeletal structure of the body part to which the sensor is attached."
ATOM: Model-Driven Autoscaling for Microservices.,"Microservices based architectures are increasingly widespread in the cloud software industry. Still, there is a shortage of auto-scaling methods designed to leverage the unique features of these architectures, such as the ability to independently scale a subset of microservices, as well as the ease of monitoring their state and reciprocal calls. We propose to address this shortage with ATOM, a model-driven autoscaling controller for microservices. ATOM instantiates and solves at run-time a layered queueing network model of the application. Computational optimization is used to dynamically control the number of replicas for each microservice and its associated container CPU share, overall achieving a fine-grained control of the application capacity at run-time. Experimental results indicate that for heavy workloads ATOM offers around 30%-37% higher throughput than baseline model-agnostic controllers based on simple static rules. We also find that model-driven reasoning reduces the number of actions needed to scale the system as it reduces the number of bottleneck shifts that we observe with model-agnostic controllers."
Associated Task Scheduling Based on Dynamic Finish Time Prediction for Cloud Computing.,"Cloud computing has emerged as an increasingly indispensable and highly demanded platform for various applications, as cloud computing allows for on demand resource provisioning and allocation. The associated tasks composing a job processed by cloud computing need to be executed under ordering constraints for correctness or consistency. The associated tasks are executed on different servers and communication is required to transfer the data between the servers, while the processing capacity of and the communication capacity between different components underlying the cloud computing platform may show great heterogeneity. Therefore, efficient scheduling for the associated tasks is critical for achieving high performance in cloud computing systems. In this paper, we tackle the problem of associated task scheduling for cloud computing with the aim to minimize the makespan of the job, when excessive diversities are present in the computing and communication components. We propose a Dynamic Priority List Scheduling (DPLS) algorithm based on dynamic task finish time prediction. The algorithm dynamically predicts the remaining execution time for each task to be scheduled according to the server allocation of the previously scheduled tasks, and decides the next task to be scheduled and the server allocation based on the previously task scheduling result. We conduct experiments through simulations on randomly generated associated tasks and real-world applications. Experimental results demonstrate that the proposed algorithm is promising."
Chamulteon: Coordinated Auto-Scaling of Micro-Services.,"Nowadays, in order to keep track of the fast changing requirements of Internet applications, auto-scaling is used as an essential mechanism for adapting the number of provisioned resources to the resource demand. The straightforward approach is to deploy a set of common and opensource single-service auto-scalers for each service independently. However, this deployment leads to problems such as bottleneck-shifting and increased oscillations. Existing auto-scalers that scale applications consisting of multiple services are kept closed-source. To face these challenges, we first survey existing auto-scalers and highlight current challenges. Then, we introduce Chamulteon, a redesign of our previously introduced mechanism, which can scale applications consisting of multiple services in a coordinated manner. We evaluate Chamulteon against four different well-cited auto-scalers in four sets of measurement-based experiments where we use diverse environments (VM vs. Docker), real-world traces, and vary the scale of the demanded resources. Overall, Chamulteon achieves the best auto-scaling performance based on established user-oriented and endorsed elasticity metrics."
A Near Optimal Multi-Faced Job Scheduler for Datacenter Workloads.,"As data-parallel applications process more complex data, the dependencies between computation jobs in a multi-stage job also become more complicated. However, most of the existing scheduling solutions primarily rely on total bytes sent (job size) to differentiate jobs where jobs with fewer? bytes sent are prioritized over the larger ones. This approach overlooks the fact that jobs may consist of multiple computation stages, and that the completion of a computation job stage depends on the completion of other jobs' stage. In this paper, we present a coflow scheduler of multi-stage jobs that minimizes the average job completion time. Our solution prioritizes jobs based on the multi-faceted characteristics of multi-stage job structure per stage, instead of total bytes sent. Our experiments show that our approach provides twice the performance of existing solutions on average and by four times in bursty traffic scenario."
Spear: Optimized Dependency-Aware Task Scheduling with Deep Reinforcement Learning.,"Modern data parallel frameworks, such as Apache Spark, are designed to execute complex data processing jobs that contain a large number of tasks, with dependencies between these tasks represented by a directed acyclic graph (DAG). When scheduling these tasks, the ultimate objective is to minimize the makespan of the schedule, which is equivalent to minimizing the job completion time. With task dependencies, however, minimizing the makespan of the schedule is non-trivial, especially when tasks in the DAG have different resource demands with respect to multiple resource types. In this paper, we present Spear, a new scheduling framework designed to minimize the makespan of complex jobs, while considering both task dependencies and heterogeneous resource demands at the same time. Inspired by recent advances in artificial intelligence, Spear applies Monte Carlo Tree Search (MCTS) in the specific context of task scheduling, and trains a deep reinforcement learning model to guide the expansion and rollout steps in MCTS. With deep reinforcement learning, search efficiency can be significantly improved by focusing on more promising branches. With both simulations and experiments using traces from production workloads, we compare the scheduling performance of Spear with state-of-the-art job schedulers in the literature, and Spear can outperform those approaches by up to 20%. Our results have validated our claims that MCTS and deep reinforcement learning can readily be applied to optimize the scheduling of complex jobs with task dependencies."
InstaMeasure: Instant Per-flow Detection Using Large In-DRAM Working Set of Active Flows.,"In the zettabyte era, per-flow measurement becomes more challenging for the data center owing to the increment of both traffic volumes and the number of flows. Also, the swiftness of detection of anomalies (e.g., congestion, link failure, DDoS attack, and so on) becomes paramount. For fast and accurate traffic measurement, managing an accurate working set of active flows (WSAF) from massive volumes of packet influxes at line rates is a key challenge. WSAF is usually located in high-speed but expensive memory, such as TCAM or SRAM, and thus the number of entries to be stored is quite limited. To cope with the scalability issue of WSAF, we propose to use In-DRAM WSAF with scales, and put a compact data structure called FlowRegulator in front of WSAF to compensate for DRAM's slow access time by substantially reducing massive influxes to WSAF without compromising measurement accuracy. To verify its practicability, we further build a per-flow measurement system, called InstaMeasure, on an off-the-shelf Atom (lightweight) processor board. We evaluate our proposed system in a large scale real-world experiment (monitoring our campus main gateway router for 113 hours, and capturing 122.3 million flows). We verify that InstaMeasure can detect heavy hitters (HHs) with 99% accuracy and within 10 ms (detection is faster for heavier HHs) while providing the one million flows record with only tens of MB of DRAM memory. InstaMeasure's various performance metrics are further investigated by the packet trace-driven experiment using one-hour CAIDA dataset, where the target of measurement was all the 78 million L4 flows for one-hour."
Automating System Configuration of Distributed Machine Learning.,"The performance of distributed machine learning systems is dependent on their system configuration. However, configuring the system for optimal performance is challenging and time consuming even for experts due to the diverse runtime factors such as workloads or the system environment. We present cost-based optimization to automatically find a good system configuration for parameter server (PS) machine learning (ML) frameworks. We design and implement Cruise that applies the optimization technique to tune distributed PS ML execution automatically. Evaluation results on three ML applications verify that Cruise automates the system configuration of the applications to achieve good performance with minor reconfiguration costs."
Minimum Makespan Workflow Scheduling for Malleable Jobs with Precedence Constraints and Lifetime Resource Demands.,"Scheduling complex workflows for big data systems is both fundamentally challenging and of great practical importance. Some state-of-the-art schedulers ignore important real-world considerations for the sake of algorithmic tractability, while others are tailored for specific workloads. We consider the preemption version of the Minimum Makespan Workflow Scheduling of Malleable Jobs with Precedence Constraints (MMWS-MP) problem [24], [27] and generalize it to be applicable to a broader range of real-world big data applications. In particular, we formulate MMWSMPL by introducing an additional constraint on the lifetime resource demand, which models constant resource consumption throughout the lifetime of a set of jobs. Practical examples include ApplicationMasters in YARN, port reservations, software licenses, and GPU cycles. We devise two scheduling strategies for MMWS-MPL: (1) LPSched, which takes a linear programming approach, and (2) BoltSched, a mostly greedy heuristic. We prove that LPSched achieves a constant approximation ratio of (2+ε) for any ε>0 and thus serves as a theoretically sound comparison baseline. We empirically evaluate both strategies on synthetic benchmarks and show that BoltSched produces schedules that are nearly as good as LPSched at a fraction (around 10%) of the computational cost."
Catalyst: A Cloud-based Media Processing Framework.,"Massive increases in production and consumption of digital media are motivating cloud ""video encoding as a service."" In this paper, we explore efficient platforms for supporting such services. Specifically, we explore transcoding performance on a software encoder and two hardware-accelerated encoders representing widely different points in the hardware acceleration design space, as well as interference effects when heterogenous encoders are run concurrently. Using results from our exploratory study, we next propose a framework for providing video encoding as a service on a single server, and implement a prototype of the framework in a system called Catalyst. Catalyst accepts transcoding tasks and intelligently uses all available resources, including software and hardware-accelerated encoders, to maximize throughput while respecting task deadlines, and provides a foundational building block for a cluster-based cloud video encoding service. We evaluate Catalyst using a synthetic transcoding workload designed to emulate an IP-TV/Cloud-DVR workload. Evaluation results show that Catalyst can significantly increase throughput while meeting task deadlines compared to naive use of hardware-accelerated encoding."
Distributed Multi-Agent Preference Learning for An IoT-enriched Smart Space.,"There have been several efforts on preference learning in a smart space by means of multi-agent collaborations. Each agent captures a user action or handles part of learning but decision makings are done in a centralized manner. This makes it difficult for a smart space to deal with learning complexity due to the increase and reconfiguration of smart devices. While the complexity is relieved by articulating the learning space, it is not flexible because the articulation procedure needs to be resumed whenever a smart space reconfiguration occurs. In this paper, we propose a distributed multi-agent preference learning architecture which allows a group of physically separate agents to collaborate with each other for learning a user's task preference efficiently in an IoT enriched smart space. For this, the proposed scheme provides four key features: ontology-based knowledge structure for task-driven agent collaboration, knowledge exchange protocol for task-aware causality among agents, Q-learners for observing and learning from user behaviors, and negotiation and acknowledgement protocol for preventing agents from performing disorganized actions. Evaluation results show that the proposed scheme allows smart device agents to learn user preferences in a fully distributed way and outperforms existing approaches in terms of learning speed and system overhead."
"UpKit: An Open-Source, Portable, and Lightweight Update Framework for Constrained IoT Devices.","Updating the software running on constrained IoT devices such as low-power sensors and actuators in a secure and efficient way is an open problem. The limited computational, memory, and storage capabilities of these devices, together with their small energy budget, indeed, restrict the number of features that can be embedded into an update system and make it also difficult to build a generic and compact solution. As a result, existing update systems for constrained IoT devices are often not portable, do not perform a proper verification of the downloaded firmware, or focus only on a single phase of the update process, which exposes them to security threats and calls for new solutions. In this paper we present UpKit, a portable and lightweight software update framework for constrained IoT devices encompassing all phases of the update process: from the generation and signature of a new firmware, to the transmission of the latter to an IoT device, its verification and installation. UpKit employs a novel update architecture that is agnostic to how new firmware images are distributed and that introduces a double-signature process to guarantee the freshness of a new firmware. This, together with an additional verification step, allows also to reject invalid software at an early stage and to prevent an unnecessary reboot of the device. We keep UpKit's design modular and provide an open-source implementation for several operating systems, hardware platforms, as well as cryptographic libraries. We further include support for differential updates and flexible memory slots, which allows to significantly increase the efficiency of the update process. An experimental evaluation shows that UpKit can be used to efficiently update highly-constrained IoT devices, and that it has a comparable memory footprint to state-of-the-art solutions, despite the introduction of several features."
A Population Protocol Model with Interaction Probability Considering Speeds of Agents.,"This paper proposes a new extension of the population protocol (PP) model, the linearly-weighted interaction population protocol (LIPP) model, which introduces weights of agents (or mobile devices) as abstract speeds of agents. The model assumes that the interaction probability between agents is relatively proportional to the weights of the agents, which is almost validated from preliminary simulation results. Each agent can control its weight to adjust its interaction probability. This implies that mobility of agents is semi-passive (not completely passive) since they can change only their abstract speeds. This paper considers how the expected convergence time (measured by the number of interactions) of naive PP protocols for information dissemination, leader election and majority can be improved in the new model by assigning appropriate weighs to agents. The presented results show potential possibility and limitation of the LIPP model."
On Task Assignment for Early Target Inspection in Squads of Aerial Drones.,"We consider the problem of assigning tasks and related trajectories to a fleet of drones, in critical scenarios requiring early anomaly discovery and intervention. Drones visit target points in consecutive trips, with recharging and data offloading in between. We propose a novel metric, called weighted coverage, which generalizes classic notions of coverage, as well as a new notion of accumulative coverage which prioritizes early inspection of target points. We formulate an ILP problem for weighted coverage maximization and show its NP-hardness. We propose an efficient polynomial algorithm with guaranteed approximation. By means of simulations we show that our algorithm performs close to the optimal solution and outperforms a previous approach in terms of several performance metrics, including coverage, average inspection delay, energy consumption, and computation time, under a wide range of application scenarios."
FOCUS: Scalable Search Over Highly Dynamic Geo-distributed State.,"Finding nodes which match certain criteria, based on potentially highly dynamic information, is a critical need in many distributed systems, ranging from cloud management, to network service deployments, to emerging IoT applications. With the increasing scale, dynamicity, and richness of data, existing systems, which typically implement a custom solution based around message queues where nodes push status to a central database, are ill-suited for this purpose. In this paper, we present FOCUS, a general and scalable service which easily integrates into existing and emerging systems to provide this fundamental capability. FOCUS utilizes a gossip-based protocol for nodes to organize into groups based on attributes and current value. With this approach, nodes need not synchronize with a central database, and instead the FOCUS service only needs to query the sub-set of nodes which have the potential to positively match a given query. We show FOCUS's flexibility through an operational example of complex querying for Virtual Network Functions instantiation over cloud sites, and illustrate its ease of integration by replacing the push-based approach in OpenStack's placement service. Our evaluation demonstrates a 5-15× reduction in bandwidth consumption and an ability to scale much better than existing approaches."
Distributed Service Placement in Fog Computing: An Iterative Combinatorial Auction Approach.,"A primary concern in fog computing is how to efficiently allocate limited fog resources to applications with diverse resource requirements. In fog computing, applications that consist of a set of interdependent microservices are mapped to computing and communication devices, referred to as fog nodes. While placement of microservices can be done centrally, the essentially decentralized infrastructure of participating end-user devices motivates the search for distributed solutions. In this paper, we present a distributed placement strategy that seeks to optimize energy consumption and communication costs. We devise a game-theoretic approximation method that is inspired by an iterative combinatorial auction. By properly restricting the types of bids that can be made in an auction, we can avoid the need for a centralized auctioneer. We devise a fully distributed service placement algorithm without central coordination or global state information. The algorithm operates in rounds, where the number of rounds is bounded by the number of applications and the total number of microservices. Numerical examples show that our placement algorithm outperforms existing heuristics in terms of efficiency and network utilization while achieving comparable utilization and load balancing."
DeCloud: Truthful Decentralized Double Auction for Edge Clouds.,"The sharing economy has made great inroads with services like Uber or Airbnb enabling people to share their unused resources with those needing them. The computing world, however, despite its abundance of excess computational resources has remained largely unaffected by this trend, save for few examples like SETI@home. We present DeCloud, a decentralized market framework bringing the sharing economy to on-demand computing where the offering of pay-as-you-go services will not be limited to large companies, but ad hoc clouds can be spontaneously formed on the edge of the network. We design incentive compatible double auction mechanism targeted specifically for distributed ledger trust model instead of relying on third-party auctioneer. DeCloud incorporates innovative matching heuristic capable of coping with the level of heterogeneity inherent for large-scale open systems. Evaluating DeCloud on Google cluster-usage data, we demonstrate that the system has a near-optimal performance from an economic point of view, additionally enhanced by the flexibility of matching."
Quorum Selection for Byzantine Fault Tolerance.,"Systems tolerating arbitrary failures use significant resources to constantly mask omission and timing failures from faulty processes. This paper presents Quorum Selection, a mechanism that allows to select well functioning processes for active participation in a system. Different from previous work, Quorum Selection not only excludes provably faulty processes that deviated from the protocol, but also takes omission and timing failures into account, even if they only affect individual links. We present a system architecture for Quorum Selection, including a novel failure detector that uses expectations to detect omission and timing failures in a Byzantine environment. We investigate how often an adversary may cause the quorum to changes. We show a quadratic lower bound for general Quorum Selection, but we also define a special case of Quorum Selection for leader based systems that requires trying at most 6f quorums."
Maximizing Throughput with Minimum Channel Assignment for Cellular-VANET Het-Nets.,"In this paper, we study the channel assignment problem in cellular-VANET heterogeneous wireless networks. The D2D communication technology can be applied to VANET. Vehicular device-to-device (D2D) network as an underlying network to the cellular network can share the uplink channel resources of the cellular network. Interference as a critical element has an impact on the utilization in channel assignment. To minimize the interference when allocating channels, we present a novel channel assignment algorithm based on reuse distance. Essentially we have limited spectrum resources that can be shared by vehicular transmitters and cellular users in an area, to assign the minimal number of channels to vehicles in a prescribed area is our first concern. Since the interference between co-channel devices is related to their distance, we divide the area to small hexagon regions then use Region-based Channel Assignment Algorithm to assign different channel sets to each region. In this case, three sets of resources can fulfill the channel assignment requirements to all vehicles. We also prove the theoretical guarantee as approximation factor of 3 for the minimal channel assignment problem. To improve the system throughput with limited channel resources in the HetNets, we propose a Local Search Throughput Maximization algorithm to find the vehicular transmitters and cellular users combinations. We prove the optimal approximation factor is (1-ε) and the complexity of our algorithm in each small region. We show the effectiveness and efficiency of proposed algorithm in experiments."
An Optimal Vector Clock Algorithm for Multithreaded Systems.,"Tracking causality (or happened-before relation) between events is useful for many applications such as debugging and recovery from failures. Consider a concurrent system with n threads and m objects. For such systems, either a vector clock of size n is used with one component per thread or a vector clock of size m is used with one component per object. A natural question is whether one can use a vector clock of size strictly less than the minimum of m and n to timestamp events. We give an algorithm in this paper that uses a hybrid of thread and object components. Our algorithm is guaranteed to return the minimum number of components necessary for vector clocks. We first consider the case when the interaction between objects and threads is statically known. This interaction is modeled by a thread-object bipartite graph. Our algorithm is based on finding the maximum bipartite matching of such a graph and then applying König-Egerváry Theorem to compute the minimum vertex cover to determine the optimal number of components necessary for the vector clock. We also propose two mechanisms to compute such an vector clock when computation is revealed in an online fashion. Evaluation on different types of graphs indicates that our offline algorithm generates a size vector clock which is significantly less than the minimum of m and n. These mechanisms are more effective when the underlying bipartite graph is not dense."
"ARES: Adaptive, Reconfigurable, Erasure Coded, Atomic Storage.","Emulating a shared atomic, read/write storage system is a fundamental problem in distributed computing. Replicating atomic objects among a set of data hosts was the norm for traditional implementations (e.g., [6]) in order to guarantee the availability and accessibility of the data despite host failures. As replication is highly storage demanding, recent approaches suggested the use of erasure-codes to offer the same fault-tolerance while optimizing storage usage at the hosts. Initial works focused on a fix set of data hosts. To guarantee longevity and scalability, a storage service should be able to dynamically mask hosts failures by allowing new hosts to join, and failed host to be removed without service interruptions. This work presents the first erasure-code based atomic algorithm, called ARES, which allows the set of hosts to be modified in the course of an execution. ARES is composed of three main components: (i) a reconfiguration protocol, (ii) a read/write protocol, and (iii) a set of data access primitives. The design of ARES is modular and is such to accommodate the usage of various erasure-code parameters on a per-configuration basis. We provide bounds on the latency of read/write operations and analyze the storage and communication costs of the ARES algorithm."
Scaling Longitudinal Functional Health Assessment in Multi-Inhabitant Smarthome.,"Autonomous longitudinal functional health assessment is critically important to support the rehabilitation of older adults in Skilled Nursing Facilities (SNFs). Although the wide availability of commodity smarthome sensors and internet-of-things (IoT) is facilitating continual monitoring of individuals' health-related vital signs and behaviors, missing values, presence of multi-inhabitants and diversity of smarthomes interfere with successful longitudinal assessment and impact the scalability of autonomous health assessments. In this paper, we propose a novel scalable framework to provide health assessments of older adults living in varied smarthome environments. As a critical first step, we propose a novel algorithm to track individuals in a multi-inhabitant smarthome environment. We then propose a novel data curation technique to address missing sensor signals in a multi-modal ambient sensor-assisted environment. Finally, we propose a novel trajectory featurization method inspired Deep Convolutional Neural Network TDCNN, leveraging appropriate samples from a well-labeled source smarthome, to transfer functional health assessment knowledge to unlabeled diverse smarthomes, boosting the scalability of autonomous health assessment. Our evaluation on real SNFs data, collected over 5 months from 95 individuals residing in 9 diverse sensored SNFs environments shows promising results (93% accuracy) with respect to the scalability of our framework."
Are We Referring to the Same x86 64?: Detection of Cache Events in AMD's Zen Micro-architecture.,"The disclosure of attacks based on flawed speculative execution mechanisms exposed cache side-channels as an apparatus to violate process isolation guarantees. Previous work has made Intel's SGX trust model the critical target of state-of-the-art techniques that leverage shared caches to shatter Enclave's safeguard. We demonstrate that porting attack implementations between micro-architectures can become a complex task, dismantling the concept of x86_64 as a whole when talking about covert cache side-channels. Our work describes the shortcomings and alternatives of assessing low-level events within Zen's cache hierarchy. We disclose the effects of the implemented Second Level cache pre-fetcher and how it tampers the ability of algorithms to discover minimal eviction sets, for which we coin the term ZenAccess."
A Self-Stabilizing Algorithm for Constructing ST-Reachable Directed Acyclic Graph When lS| ≤ 2 and |T| ≤ 2.,"In this paper, we introduce a new graph structure named an ST-reachable directed acyclic graph which is a directed acyclic graph (DAG) that guarantees reachability from every sender to every target (i.e., a directed path exists). When an arbitrary connected undirected graph G=(V,E) and two sets of the vertices, senders S (⊂ V) and targets T (⊂ V), are given, we consider construction of a minimal ST-reachable DAG by changing some undirected edges to arcs and removing the remaining edges. This implies that every node in T is reachable from every node in S on the constructed ST-reachable DAG. In particular, our goals are (1) to find the necessary and sufficient condition that an ST-reachable DAG can be constructed, and (2) to design a self-stabilizing algorithm for constructing a minimal ST-reachable DAG (if exists). In this paper, we present the necessary and sufficient condition that a minimal ST-reachable DAG can be constructed when S ≤ 2 and |T| ≤ 2, and propose a self-stabilizing algorithm to construct an ST-reachable DAG (if exists) when an arbitrary connected undirected graph, S (|S| ≤ 2) and T (|T| ≤ 2) are given. Moreover, our proposed algorithm can detect the non-existence of ST-reachable DAG if there exists no ST-reachable DAG of the given graph and two sets of vertices, S and T."
A Distributed Synchronous SGD Algorithm with Global Top-k Sparsification for Low Bandwidth Networks.,"Distributed synchronous stochastic gradient descent (S-SGD) with data parallelism has been widely used in training large-scale deep neural networks (DNNs), but it typically requires very high communication bandwidth between computational workers (e.g., GPUs) to exchange gradients iteratively. Recently, Top-k sparsification techniques have been proposed to reduce the volume of data to be exchanged among workers and thus alleviate the network pressure. Top-k sparsification can zero-out a significant portion of gradients without impacting the model convergence. However, the sparse gradients should be transferred with their indices, and the irregular indices make the sparse gradients aggregation difficult. Current methods that use AllGather to accumulate the sparse gradients have a communication complexity of O(kP), where P is the number of workers, which is inefficient on low bandwidth networks with a large number of workers. We observe that not all top-k gradients from P workers are needed for the model update, and therefore we propose a novel global Top-k (gTop-k) sparsification mechanism to address the difficulty of aggregating sparse gradients. Specifically, we choose global top-k largest absolute values of gradients from P workers, instead of accumulating all local top-k gradients to update the model in each iteration. The gradient aggregation method based on gTop-k sparsification, namely gTopKAllReduce, reduces the communication complexity from O(kP) to O(k log P). Through extensive experiments on different DNNs, we verify that gTop-k S-SGD has nearly consistent convergence performance with S-SGD, and it has only slight degradations on generalization performance. In terms of scaling efficiency, we evaluate gTop-k on a cluster with 32 GPU machines which are interconnected with 1 Gbps Ethernet. The experimental results show that our method achieves 2.7-12× higher scaling efficiency than S-SGD with dense gradients and 1.1-1.7× improvement than the existing Top-k S-SGD."
