"Timely, Reliable, and Cost-Effective Internet Transport Service Using Dissemination Graphs.","Emerging applications such as remote manipulation and remote robotic surgery require communication that is both timely and reliable, but the Internet natively supports only communication that is either completely reliable with no timeliness guarantees (e.g. TCP) or timely with best-effort reliability (e.g. UDP). We present an overlay transport service that can provide highly reliable communication while meeting stringent timeliness guarantees (e.g. 130ms round-trip latency across the US) over the Internet. To enable routing schemes that can support the necessary timeliness and reliability, we introduce dissemination graphs, providing a unified framework for specifying routing schemes ranging from a single path, to multiple disjoint paths, to arbitrary graphs. We conduct an extensive analysis of real-world network data, finding that a routing approach using two disjoint paths performs well in most cases, and that cases where two disjoint paths do not perform well typically involve problems around a source or destination. Based on this analysis, we develop a timely dissemination-graph-based routing method that can add targeted redundancy in problematic areas of the network. This approach can cover over 99% of the performance gap between a traditional single-path approach and an optimal (but prohibitively expensive) scheme, while two dynamic disjoint paths cover about 70% of this gap, and two static disjoint paths cover about 45%. This performance improvement is obtained at a cost increase of about 2% over two disjoint paths."
Pronto: Efficient Test Packet Generation for Dynamic Network Data Planes.,"Computer networks are becoming increasingly complex today and thus prone to various network faults. Traditional testing tools (e.g., ping, traceroute) that often involve substantial manual effort to uncover faults are inefficient. This paper focuses on fault detection of the network data plane using test packets. Existing solutions of test packet generation either take very long time (e.g., more than one hour) to complete or generate too many test packets that may hurt regular traffic. In this paper, we present Pronto, an automated test packet generation tool that generates test packets to exercise data plane rules in the entire network in a short time (e.g., several seconds) and can quickly react to rule changes due to network dynamics. In addition, Pronto minimizes the number of test packets by allowing a packet to test multiple rules at different switches. The performance evaluation using two real network data plane rule sets shows that Pronto is faster than a recently developed tool by more than two orders of magnitude. Pronto can update the probes for rule changes using less than 1ms while existing methods have no such update function."
Agar: A Caching System for Erasure-Coded Data.,"Erasure coding is an established data protection mechanism. It provides high resiliency with low storage overhead, which makes it very attractive to storage systems developers. Unfortunately, when used in a distributed setting, erasure coding hampers a storage system's performance, because it requires clients to contact several, possibly remote sites to retrieve their data. This has hindered the adoption of erasure coding in practice, limiting its use to cold, archival data. Recent research showed that it is feasible to use erasure coding for hot data as well, thus opening new perspectives for improving erasure-coded storage systems. In this paper, we address the problem of minimizing access latency in erasure-coded storage. We propose Agar-a novel caching system tailored for erasure-coded content. Agar optimizes the contents of the cache based on live information regarding data popularity and access latency to different data storage sites. Our system adapts a dynamic programming algorithm to optimize the choice of data blocks that are cached, using an approach akin to ""Knapsack"" algorithms. We compare Agar to the classical Least Recently Used and Least Frequently Used cache eviction policies, while varying the amount of data cached between a data chunk and a whole replica of the object. We show that Agar can achieve 16% to 41% lower latency than systems that use classical caching policies."
High Performance Recovery for Parallel State Machine Replication.,"State machine replication is a fundamental approach to high availability. Despite the vast literature on the topic, relatively few studies have considered the issues involved in recovering faulty replicas. Recovering a replica requires (a) retrieving and installing an up-to-date replica checkpoint, and (b) restoring and re-executing the log of commands not reflected in the checkpoint. Parallel techniques to state machine replication render recovery particularly challenging since throughput under normal execution (i.e., in the absence of failures) is very high. Consequently, the log of commands that need to be applied until the replica is available is typically large, which delays recovery. In this paper, we present two techniques to optimize recovery in parallel state machine replication. The first technique allows new commands to execute concurrently with the execution of logged commands, before replicas are completely updated. The second technique introduces on-demand state recovery, which allows segments of a checkpoint to be recovered concurrently."
On Data Parallelism of Erasure Coding in Distributed Storage Systems.,"Deployed in various distributed storage systems, erasure coding has demonstrated its advantages of low storage overhead and high failure tolerance. Typically in an erasure-coded distributed storage system, systematic maximum distance seperable (MDS) codes are chosen since the optimal storage overhead can be achieved and meanwhile data can be read directly without decoding operations. However, data parallelism of existing MDS codes is limited, because we can only read data from some specific servers in parallel without decoding operations. In this paper, we propose Carousel codes, designed to allow data to be read from an arbitrary number of servers in parallel without decoding, while preserving the optimal storage overhead of MDS codes. Furthermore, Carousel codes can achieve the optimal network traffic to reconstruct an unavailable block. We have implemented a prototype of Carousel codes on Apache Hadoop. Our experimental results have demonstrated that Carousel codes can make MapReduce jobs finish with almost 50% less time and reduce data access latency significantly, with a comparable throughput in the encoding and decoding operations and no additional sacrifice of failure tolerance or the network overhead to reconstruct unavailable data."
MeteorShower: Minimizing Request Latency for Majority Quorum-Based Data Consistency Algorithms in Multiple Data Centers.,"With the increasing popularity of serving and storing data in multiple data centers, we investigate the efficiency of majority quorum-based data consistency algorithms under this scenario. Because of the failure-prone nature of distributed storage systems, majority quorum-based data consistency algorithms become one of the most widely adopted approaches. In this paper, we propose the MeteorShower framework, which provides fault-tolerant read/write key-value storage service across multiple data centers with sequential consistency guarantees. A major feature is that most read operations are executed locally within a single data center. This results in lowering read latency from hundreds of milliseconds to tens of milliseconds. The data consistency algorithm in MeteorShower augments majority quorum-based algorithms. Thus, it keeps all the desirable properties of majority quorums, such as fault tolerance, balanced load, etc. An implementation of MeteorShower on top of Cassandra is deployed and evaluated in multiple data centers using the Google Cloud Platform. Evaluations of MeteorShower framework have shown that it can consistently serve read requests without paying the communication delays among replicas maintained in multiple data centers. As a result, we are able to improve the latency of read requests from hundreds of milliseconds to tens of milliseconds while achieving the same latency on write requests and the same fault tolerance guarantee. Thus, MeteorShower is optimized for read intensive workloads."
LSbM-tree: Re-Enabling Buffer Caching in Data Management for Mixed Reads and Writes.,"LSM-tree has been widely used in data management production systems for write-intensive workloads. However, as read and write workloads co-exist under LSM-tree, data accesses can experience long latency and low throughput due to the interferences to buffer caching from the compaction, a major and frequent operation in LSM-tree. After a compaction, the existing data blocks are reorganized and written to other locations on disks. As a result, the related data blocks that have been loaded in the buffer cache are invalidated since their referencing addresses are changed, causing serious performance degradations. In order to re-enable high-speed buffer caching during intensive writes, we propose Log-Structured buffered-Merge tree (simplified as LSbM-tree) by adding a compaction buffer on disks, to minimize the cache invalidations on buffer cache caused by compactions. The compaction buffer efficiently and adaptively maintains the frequently visited data sets. In LSbM, strong locality objects can be effectively kept in the buffer cache with minimum or without harmful invalidations. With the help of a small on-disk compaction buffer, LSbM achieves a high query performance by enabling effective buffer caching, while retaining all the merits of LSM-tree for write-intensive data processing, and providing high bandwidth of disks for range queries. We have implemented LSbM based on LevelDB. We show that with a standard buffer cache and a hard disk, LSbM can achieve 2x performance improvement over LevelDB. We have also compared LSbM with other existing solutions to show its strong effectiveness."
Incremental Topology Transformation for Publish/Subscribe Systems Using Integer Programming.,"Distributed overlay-based publish/subscribe systems provide a selective, scalable, and decentralized approach to data dissemination. Due to the dynamic communication flows between data producers and consumers, the overlay topology of such systems can become inefficient over time and therefore requires adaptation to the existing load. Existing studies propose algorithms to design overlay topologies which are optimized for specific workloads. However, the problem of generating a plan to incrementally transform the current topology to an optimized one has been largely ignored. In this paper, we present IPITT, an approach based on integer programming for the incremental topology transformation (ITT) problem. Given the current topology and a target topology, IPITT generates a transformation plan with a minimal number of steps in order to lessen service disruption. Furthermore, we introduce a plan execution mechanism and evaluate our approach on an existing publish/subscribe system. Based on our evaluation, IPITT can reduce plan computation time by a factor of 10 and generates plans with an execution time up to 55% shorter than those of existing approaches."
milliScope: A Fine-Grained Monitoring Framework for Performance Debugging of n-Tier Web Services.,"Modern distributed systems are often considered to be black boxes that greatly limit the potential to understand behaviors at the level of detail necessary to diagnose some of the most important types of performance problems. Recently researchers have found abnormal response time delays, one to two orders of magnitude longer than the average response time, that exist in short periods and cause economic loss for service providers. These very short bottlenecks are hard to detect due to their short life spans and their variety of possible reasons. In this paper, we propose milliScope (mScope), the first millisecond-granularity software-based resource and event monitoring for distributed systems that achieves both performance, low overhead at high frequency, and high accuracy matched with other firmware monitoring tool. More specifically, milliScope is a fine-grained monitoring framework to collaborate multiple mScopeMonitors for event and resource monitoring to reconstruct the flow of each client request and profile execution performance in a distributed system. We utilize the resource mScopeMonitors for system resource monitoring, and we develop our own event mScopeMonitors to identify the execution boundary in a lightweight, precise and systematic methodology. The semantic and syntactic of these monitoring logs with arbitrary formats are enriched by our multistage data transformation tool, mScopeDataTransformer, which unifies the diverse monitoring logs into a dynamic data warehouse, mScopeDB, for advanced analysis. We conduct several illustrative scenarios in which milliScope successfully diagnoses the response time anomalies caused by very short bottlenecks using a representative web application benchmark (RUBBoS)."
Stark: Optimizing In-Memory Computing for Dynamic Dataset Collections.,"Emerging distributed in-memory computing frameworks, such as Apache Spark, can process a huge amount of cached data within seconds. This remarkably high efficiency requires the system to well balance data across tasks and ensure data locality. However, it is challenging to satisfy these requirements for applications that operate on a collection of dynamically loaded and evicted datasets. The dynamics may lead to time-varying data volume and distribution, which would frequently invoke expensive data re-partition and transfer operations, resulting in high overhead and large delay. To address this problem, we present Stark, a system specifically designed for optimizing in-memory computing on dynamic dataset collections. Stark enforces data locality for transformations spanning multiple datasets (e.g., join and cogroup) to avoid unnecessary data replications and shuffles. Moreover, to accommodate fluctuating data volume and skewed data distribution, Stark delivers elasticity into partitions to balance task execution time and reduce job makespan. Finally, Stark achieves bounded failure recovery latency by optimizing the data checkpointing strategy. Evaluations on a 50-server cluster show that Stark reduces the job makespan by 4X and improves system throughput by 6X compared to Spark."
CRESON: Callable and Replicated Shared Objects over NoSQL.,"In a Cloud environment, the ability to share and persist objects simplifies the design of applications. Storing objects in a NoSQL database ensures their availability and provides scalability to applications. When Object-NoSQL Mapping is performed at the client side, objects that are accessed by several clients are repeatedly converted between their in-memory and serialized representations. This negatively impacts performance and increases replication costs. In this paper, we describe the design of CRESON, a system supporting callable objects over NoSQL, in which application objects are mapped and instantiated directly on the storage nodes. CRESON supports composition by reference and ensures strong consistency. Objects are replicated and maintained coherent using State Machine Replication. The implementation of CRESON leverages the support of a listenable key-value store (LKVS), a novel NoSQL storage abstraction that we introduce in this paper. We discuss the performance and complexity of CRESON with the example of the portage of a personal cloud storage service, initially developed using an object-relational mapping over a sharded PostgreSQL database. Our results show that CRESON offers a simpler programming experience both in terms of learning time and lines of code, while performing better on average and being more scalable."
Virtualized Network Coding Functions on the Internet.,"Network coding is a fundamental tool that enables higher network capacity and lower complexity in routing algorithms, by encouraging the mixing of information flows in the middle of a network. Implementing network coding in the core Internet is subject to practical concerns, since Internet routers are often overwhelmed by packet forwarding tasks, leaving little processing capacity for coding operations. Inspired by the recent paradigm of network function virtualization, we propose implementing network coding as a new network function, and deploying such coding functions in geo-distributed cloud data centers, to practically enable network coding on the Internet. We target multicast sessions (including unicast flows as special cases), strategically deploy relay nodes (network coding functions) in selected data centers between senders and receivers, and embrace high bandwidth efficiency brought by network coding with dynamic coding function deployment. We design and implement the network coding function on typical virtual machines, featuring efficient packet processing. We propose an efficient algorithm for coding function deployment, scaling in and out, in the presence of system dynamics. Real-world implementation on Amazon EC2 and Linode demonstrates significant throughput improvement and higher robustness of multicast via coding functions as well as efficiency of the dynamic deployment and scaling algorithm."
Consensus Robustness and Transaction De-Anonymization in the Ripple Currency Exchange System.,"Distributed financial systems are radically changing the way we do business and spend our money. Ripple, in particular, is unique in its kind. It is built on consensus and trust among its users and it allows to exchange both fiat currencies and goods over its network. It does so by storing the accounts of its users, their balances, and all the transactions in a distributed ledger, publicly accessible. In this paper we perform an in-depth study of the Ripple exchange system and its public distributed ledger. We analyze payments, the structure of payment paths, and the role of the entities in the system such as Gateways (the equivalent of banks) and Market Makers. We also analyze the internal stream of events and show that Ripple relies on a surprisingly small number of active validators, raising concerns on the actual robustness and fairness of the system. Moreover, we consider the degree of anonymity that Ripple is able to guarantee. By examining the first three years of Ripple history (more than 500 GB worth of data), we show that even approximate information on a single payment can uncover, with incredible accuracy, the entire financial life of the user. For example, anyone who overhears our order of a Latte at our favourite bar can easily get complete and unlimited access to our balance, our previous and future payments, our monthly income, as well as critical information about the places where we shop and the people we trust."
Learning Privacy Habits of PDS Owners.,"The concept of Personal Data Storage (PDS) has recently emerged as an alternative and innovative way of managing personal data w.r.t. the service-centric one commonly used today. The PDS offers a unique logical repository, allowing individuals to collect, store, and give access to their data to third parties. The research on PDS has so far mainly focused on the enforcement mechanisms, that is, on how user privacy preferences can be enforced. In contrast, the fundamental issue of preference specification has been so far not deeply investigated. In this paper, we do a step in this direction by proposing different learning algorithms that allow a fine-grained learning of the privacy aptitudes of PDS owners. The learned models are then used to answer third party access requests. The extensive experiments we have performed show the effectiveness of the proposed approach."
City-Hunter: Hunting Smartphones in Urban Areas.,"The security issue of public WiFi is gaining more and more concern. By listening to probe requests, an adversary can obtain the SSID list of the APs to which a smartphone previously connected, and utilizes this information to trick the smartphone into associating to it. However, with the enhancement of security level, most smartphones now do not proactively disclose their SSID lists, making these attacks obsolete. In this paper, we propose City-Hunter, an attacker that can lure nearby smartphones without knowing their SSID information. City-Hunter establishes and maintains an SSID database by integrating both offline and online information. Meanwhile, it smartly chooses some SSIDs to hit a smartphone according to the past record and freshness. We evaluate the performance of City-Hunter in different public places. The results demonstrate that City-Hunter is able to successfully hit 12% ~ 18% smartphones without knowing their SSID information, which is about 4 ~ 8 times improvement compared to the similar attacks like KARMA and MANA."
When Seeing Isn't Believing: On Feasibility and Detectability of Scapegoating in Network Tomography.,"Network tomography is a vital tool to estimate link qualities from end-to-end network measurements. An implicit assumption in network tomography is that observed measurements indeed reflect the aggregate of link performance (i.e., seeing is believing). However, it is not guaranteed today that there exists no anomaly (e.g., malicious autonomous systems and insider threats) in large-scale networks. Malicious nodes can intentionally manipulate link metrics via delaying or dropping packets to affect measurements. Will such an assumption render a vulnerability when facing attackers? The problem is of essential importance in that network tomography is developed towards effective network diagnostics and failure recovery. In this paper, we demonstrate that the vulnerability is real and propose a new attack strategy, called scapegoating, in which malicious nodes can substantially damage a network (e.g., delaying packets) and at the same time maliciously manipulate end-to-end measurement results such that a legitimate node is misleadingly identified as the root cause of the damage (thereby becoming a scapegoat) under network tomography. We formulate three basic scapegoating approaches and show under what conditions attacks can be successful. We also reveal conditions to detect such attacks. Our theoretical and experimental results show that simply trusting measurements leads to scapegoating vulnerabilities. Thus, existing methods should be revisited accordingly for security in various applications."
You Can Hear But You Cannot Steal: Defending Against Voice Impersonation Attacks on Smartphones.,"Voice, as a convenient and efficient way of information delivery, has a significant advantage over the conventional keyboard-based input methods, especially on small mobile devices such as smartphones and smartwatches. However, the human voice could often be exposed to the public, which allows an attacker to quickly collect sound samples of targeted victims and further launch voice impersonation attacks to spoof those voice-based applications. In this paper, we propose the design and implementation of a robust software-only voice impersonation defense system, which is tailored for mobile platforms and can be easily integrated with existing off-the-shelf smart devices. In our system, we explore magnetic field emitted from loudspeakers as the essential characteristic for detecting machine-based voice impersonation attacks. Furthermore, we use a state-of-the-art automatic speaker verification system to defend against human imitation attacks. Finally, our evaluation results show that our system achieves simultaneously high accuracy (100%) and low equal error rates (EERs) (0%) in detecting the machine-based voice impersonation attack on smartphones."
Flow Reconnaissance via Timing Attacks on SDN Switches.,"When encountering a packet for which it has no matching forwarding rule, a software-defined networking (SDN) switch requests an appropriate rule from its controller; this request delays the routing of the flow until the controller responds. We show that this delay gives rise to a timing side channel in which an attacker can test for the recent occurrence of a target flow by judiciously probing the switch with forged flows and using the delays they encounter to discern whether covering rules were previously installed in the switch. We develop a Markov model of an SDN switch to permit the attacker to select the best probe (or probes) to infer whether a target flow has recently occurred. Our model captures practical challenges related to rule evictions to make room for other rules; rule timeouts due to inactivity; the presence of multiple rules that apply to overlapping sets of flows; and rule priorities. We show that our model enables detection of target flows with considerable accuracy in many cases."
A Study of Long-Tail Latency in n-Tier Systems: RPC vs. Asynchronous Invocations.,"Long-tail latency of web-facing applications continues to be a serious problem. Most of the previously published research addresses two classes of long latency problems: uneven workloads such as web search, and resource saturation in single nodes. We describe an experimental study of a third class of long tail latency problems that are specific to distributed systems: Cross-Tier Queue Overflow (CTQO) due to a combination of millibottlenecks (with sub-second duration) and tightly-coupled servers in n-tier systems (e.g., Apache, Tomcat, and MySQL) using RPC-style request-response communications. Our experiments show that the appearance of millibottlenecks (e.g., created by short workload bursts) in one server often causes another server (which has no saturated resources) in the synchronous invocation chain to fill up its queues (CTQO) and drop packets, creating very long response time queries. CTQO can be reduced or avoided by replacing the server dropping packets with an asynchronous server. In synchronous n-tier system experiments, long tail latency due to CTQO can be reproduced consistently at utilization as low as 43%. In contrast, when all n-tier servers are replaced by asynchronous versions, CTQO and consequent dropped packets remain absent at utilization levels as high as 83%, despite the same millibottlenecks."
Rain or Shine? - Making Sense of Cloudy Reliability Data.,"Cloud datacenters must ensure high availability for the hosted applications and failures can be the bane of datacenter operators. Understanding the what, when and why of failures can help tremendously to mitigate their occurrence and impact. Failures can, however, depend on numerous spatial and temporal factors spanning hardware, workloads, support facilities, and even the environment. One has to rely on failure data from the field to quantify the influence of these factors on failures. Towards this goal, we collect failures data along with many parameters that might influence failures from two large production datacenters with very diverse characteristics. We show that multiple factors simultaneously affect failures, and these factors may interact in non-trivial ways. This makes conventional approaches that study aggregate characteristics or single parameter influences, rather inaccurate. Instead, we build a multi-factor analysis framework to systematically identify influencing factors, quantify their relative impact, and help in more accurate decision making for failure mitigation. We demonstrate this approach for three important decisions: spare capacity provisioning, comparing the reliability of hardware for vendor selection, and quantifying flexibility in datacenter climate control for cost-reliability trade-offs."
Right-Sizing Geo-distributed Data Centers for Availability and Latency.,"We show cloud developers how to right size data center (DC) capacity for geo-distributed applications deployed on several multi-megawatt DCs, possibly also using many smaller edge DCs. Note that capacity considerations for a geo-distributed infrastructure do not decompose into individual DC capacity planning. When edge DCs are used, heterogeneous availability and costs affect the capacity split between the edge and core DCs. Non-uniform spatial distribution of clients and interdependence between latency and availability constraints make it non-trivial to provision the right capacity at each DC. We develop a geo-distributed capacity planning framework to capture the key factors that influence capacity, ranging from application demand patterns, latency and availability requirements, DC cost-availability trade-offs, and data replication overheads. We apply our framework to a realistic application and DC infrastructure setting to gather insights into how capacity should be provisioned and allocated across DCs for a representative set of requirements and costs."
Performance Driven Resource Sharing Markets for the Small Cloud.,"Small-scale clouds (SCs) often suffer from resource under-provisioning during peak demand, leading to inability to satisfy service level agreements (SLAs) and consequent loss of customers. One approach to address this problem is for a set of autonomous SCs to share resources among themselves in a cost-induced cooperative fashion, thereby increasing their individual capacities (when needed) without having to significantly invest in more resources. In this context, a central problem is how to properly share resources for a price in order to achieve profitable service, while maintaining customer SLAs. To address this problem, we propose the SC-Share framework that utilizes two interacting models: (i) a stochastic performance model that estimates the achieved performance characteristics under given SLA requirements, and (ii) a market-based game-theoretic model that (as shown empirically) converges to efficient resource sharing decisions at market equilibrium. Our results include extensive evaluations that illustrate the utility of the proposed framework."
Fault-Scalable Virtualized Infrastructure Management.,"Large-scale virtualized datacenters require considerable automation in infrastructure management in order to operate efficiently. Automation is impaired, however, by the fact that deployments are prone to multiple types of subtle faults due to hardware failures, software bugs, misconfiguration, crashes, performance degraded hardware, etc. Existing Infrastructure-as-a-Service (IaaS) management stacks incorporate little to no resilience measures to shield end users from such cloud providerlevel failures and poor performance. This paper proposes and evaluates extensions to IaaS stacks that mask faults in a fault-agnostic manner while ensuring that the overheads can be proportional to observed failure rates. We also demonstrate that infrastructure automation services and end-user applications can use service-specific knowledge, together with our new interface, to achieve better outcomes."
DeltaCFS: Boosting Delta Sync for Cloud Storage Services by Learning from NFS.,"Cloud storage services, such as Dropbox, iCloud Drive, Google Drive, and Microsoft OneDrive, have greatly facilitated users' synchronizing files across heterogeneous devices. Among them, Dropbox-like services are particularly beneficial owing to the delta sync functionality that strives towards greater network-level efficiency. However, when delta sync trades computation overhead for network-traffic saving, the tradeoff could be highly unfavorable under some typical workloads. We refer to this problem as the abuse of delta sync. To address this problem, we propose DeltaCFS, a novel file sync framework for cloud storage services by learning from the design of conventional NFS (Network File System). Specifically, we combine delta sync with NFS-like file RPC in an adaptive manner, thus significantly cutting computation overhead on both the client and server sides while preserving the network-level efficiency. DeltaCFS also enables a neat design for guaranteeing causal consistency and fine-grained version control of files. In our FUSE-based prototype system (which is open-source), DeltaCFS outperforms Dropbox by generating up to 11x less data transfer and up to 100x less computation overhead under concerned workloads."
Cachier: Edge-Caching for Recognition Applications.,"Recognition and perception based mobile applications, such as image recognition, are on the rise. These applications recognize the user's surroundings and augment it with information and/or media. These applications are latency-sensitive. They have a soft-realtime nature - late results are potentially meaningless. On the one hand, given the compute-intensive nature of the tasks performed by such applications, execution is typically offloaded to the cloud. On the other hand, offloading such applications to the cloud incurs network latency, which can increase the user-perceived latency. Consequently, edge computing has been proposed to let devices offload intensive tasks to edge servers instead of the cloud, to reduce latency. In this paper, we propose a different model for using edge servers. We propose to use the edge as a specialized cache for recognition applications and formulate the expected latency for such a cache. We show that using an edge server like a typical web cache, for recognition applications, can lead to higher latencies. We propose Cachier, a system that uses the caching model along with novel optimizations to minimize latency by adaptively balancing load between the edge and the cloud, by leveraging spatiotemporal locality of requests, using offline analysis of applications, and online estimates of network conditions. We evaluate Cachier for image-recognition applications and show that our techniques yield 3x speedup in responsiveness, and perform accurately over a range of operating conditions. To the best of our knowledge, this is the first work that models edge servers as caches for compute-intensive recognition applications, and Cachier is the first system that uses this model to minimize latency for these applications."
Content Centric Peer Data Sharing in Pervasive Edge Computing Environments.,"The proliferation and daily congregation of modern mobile devices have created abundant opportunities for peer edge devices to share valuable data with each other. The short contact durations, relatively small sharing sizes, and uncertain data availability, demand agile, light weight peer based data sharing. In this paper, we propose Peer Data Sharing (PDS) that enables edge devices to discover which data exist in nearby peers, and retrieve interested data robustly and efficiently. PDS uses novel lingering queries, mixedcast and en-route message rewriting techniques to minimize redundant transmissions and maximize opportunistic overhearing thus caching in data discovery and retrieval. Extensive evaluations based on an Android prototype show that PDS discovers and retrieves almost 100% data in tens of seconds, and remains robust despite wireless contention, simultaneous consumer requests and user mobility."
FLARE: Coordinated Rate Adaptation for HTTP Adaptive Streaming in Cellular Networks.,"Fog computing is an emerging architecture that aims to run applications on multiple devices that lie on a continuum from cloud servers to personal user smartphones. These architectures allow applications to optimize over the information stored at and functionalities run on each device, based on individual device capabilities. We demonstrate the benefits of this approach for mobile video streaming. Existing HAS (HTTP adaptive streaming) techniques often suffer from problems like unstable video quality and suboptimal resource utilization. We find that a lack of coordination prevents both clientand network-side HAS techniques from solving them. However, our fog approach can exploit existing telecommunication APIs, which expose network capabilities to applications, in order to coordinate between clients and the network. Our coordinated HAS solution, FLARE, optimizes the total utility of all clients in a cell while maintaining stable video quality and supporting user- and device-specific needs. We implement FLARE on a commodity LTE femtocell and use the implementation to conduct the first comparison of HAS players on an LTE femtocell. By conducting extensive experiments using the ns-3 simulator, we also demonstrate that FLARE (i) enhances the average video bitrate, (ii) achieves stable video quality, and (iii) balances the throughput of simultaneous video and data flows, compared to other representative HAS solutions."
Networked Drone Cameras for Sports Streaming.,"A network of drone cameras can be deployed to cover live events, such as high-action sports game played on a large field, but managing networked drone cameras in real-time is challenging. Distributed approaches yield suboptimal solutions from lack of coordination but coordination with a centralized controller incurs round-trip latencies of several hundreds of milliseconds over a wireless channel. We propose a fog-networking based system architecture to automatically coordinate a network of drones equipped with cameras to capture and broadcast the dynamically changing scenes of interest in a sports game. We design both optimal and practical algorithms to balance the tradeoff between two metrics: coverage of the most important scenes and streamed video bitrate. To compensate for network round-trip latencies, the centralized controller uses a predictive approach to predict which locations the drones should cover next. The controller maximizes video bitrate by associating each drone to an optimally matched server and dynamically re-assigns drones as relay nodes to boost the throughput in low-throughput scenarios. This dynamic assignment at centralized controller occurs at slower time-scale permitted by round-trip latencies, while the predictive approach and drones' local decision ensures that the system works in real-time. Experimental results over tens of flights on the field suggest our system can achieve really good performance, for example, 8 drones can achieve a tradeoff of 94% coverage and (on average) 2K video support at 20 Mbps by optimizing between coverage and throughput. By dynamically allocating drones to cover the game or act as relays, our system also demonstrates a 2x gain over systems maximizing static coverage alone that achieves only 9 Mbps video throughput."
Chronus: Consistent Data Plane Updates in Timed SDNs.,"Software-Defined Networks (SDNs) introduce interesting new opportunities in how network routes can be defined, verified, and changed over time. Yet despite the logically-centralized perspective offered, an SDN still needs to be considered a distributed system: rule updates communicated from the controller to the individual switches traverse an asynchronous network and may arrive out-of-order, and hence lead to (temporary or permanent) inconsistencies. Accordingly, the consistent network update problem has recently received much attention. Motivated by the advent of tightly synchronized SDNs, we in this paper initiate the study of algorithms for consistent network updates in “timed SDNs”-SDNs in which individual node updates can be scheduled at specific times. This paper presents Chronus, which is based on provably congestion- and loop-free update scheduling algorithms, and avoids the flow table space headroom required by existing two-phase update approaches. We formulate the Minimum Update Time Problem (MUTP) as an optimization program. We propose a tree algorithm to check the feasibility and a greedy algorithm to find a update sequence in polynomial time. Extensive experiments on Mininet and numerical simulations show that Chronus can substantially reduce transient congestion by 75% and save over 60% of the rules compared to the state of the art."
"Distributed Deep Neural Networks Over the Cloud, the Edge and End Devices.","We propose distributed deep neural networks (DDNNs) over distributed computing hierarchies, consisting of the cloud, the edge (fog) and end devices. While being able to accommodate inference of a deep neural network (DNN) in the cloud, a DDNN also allows fast and localized inference using shallow portions of the neural network at the edge and end devices. When supported by a scalable distributed computing hierarchy, a DDNN can scale up in neural network size and scale out in geographical span. Due to its distributed nature, DDNNs enhance sensor fusion, system fault tolerance and data privacy for DNN applications. In implementing a DDNN, we map sections of a DNN onto a distributed computing hierarchy. By jointly training these sections, we minimize communication and resource usage for devices and maximize usefulness of extracted features which are utilized in the cloud. The resulting system has built-in support for automatic sensor fusion and fault tolerance. As a proof of concept, we show a DDNN can exploit geographical diversity of sensors to improve object recognition accuracy and reduce communication cost. In our experiment, compared with the traditional method of offloading raw sensor data to be processed in the cloud, DDNN locally processes most sensor data on end devices while achieving high accuracy and is able to reduce the communication cost by a factor of over 20x."
Dynamic Control of Flow Completion Time for Power Efficiency of Data Center Networks.,"Data center network (DCN) can consume a significant amount of power (e.g., 10% to 20%) in large-scale data centers. To reduce the power consumption of DCN, traffic consolidation has been recently proposed as an effective approach to reduce the number of DCN devices in use. However, existing consolidation approaches do not sufficiently consider the flow completion time (FCT) requirement. On one hand, missing the FCT deadlines can cause serious violation of service-level agreement, especially for delay-sensitive networking services, such as web search and E-commerce. On the other hand, keeping all the devices on to make FCTs much shorter than the desired requirements is unnecessary because 1) users may not be able to perceive the difference, and 2) such a greedy strategy can lead to unnecessarily high DCN power consumption and thus more electricity costs. In this paper, we propose FCTcon, a dynamic FCT control strategy for DCN power optimization. FCTcon is designed rigorously based on control theory to dynamically control the FCT of delay-sensitive traffic flows exactly to requirements, such that the desired FCT performance is guaranteed while the maximum amount of DCN power savings can be achieved. Results from both hardware experiments and simulation evaluations demonstrate that, compared to the state-of-the-art DCN power optimization schemes, FCTcon can improve the DCN FCT performance, while achieving nearly the same or even more power savings. Consequently, FCTcon can result in more than 22.0% to 62.2% extra net profits for a data center with 50K servers."
On Energy-Efficient Congestion Control for Multipath TCP.,"Multipath TCP (MPTCP) enables transmission via multiple routes for an end-to-end connection to improve resource usage of regular TCP. Due to the increasing concern in green computing, there has been significant interest in designing energy-efficient multipath transport. For existing MPTCP congestion control algorithms, the research community still lacks a comprehensive understanding of which components in such an algorithm play the fundamental role in energy efficiency, how various algorithms compare against each other from energy-consuming perspective, or whether there exist potentially better solutions for energy saving. In this paper, we take a first step to answer these questions. Based on the MPTCP Linux kernel experiments, we first summarize that the energy consumption is related to three aspects: average throughput, path delay and different network scenarios. In order to bridge congestion control to the three aspects, we analyze the existing algorithms and capture the essential parameters of multipath congestion control model related to MPTCP's energy-efficiency. Then we design a window increase factor to shift traffic to low-delay energy-efficient paths. We further extend this design by using an energy-aware compensative parameter to fit the general hierarchical Internet topology. We evaluate the performance of existing multipath congestion control algorithms and our proposed algorithm in different network scenarios. The results successfully validate the improved energy efficiency of our design."
A Mechanism for Cooperative Demand-Side Management.,"Demand-side management (DSM) is an important theme in studies of the Smart Grid and offers the possibility of leveling power consumption with its attendant benefits of reducing capital expenses. This paper develops an algorithmic mechanism that reduces peak total power consumption and encourages prosocial behavior, such as expressing flexibility in one's power consumption and reporting preferences truthfully. The objective is to provide a tractable, budget-balanced mechanism that promotes truth-telling from households. The resulting mechanism is theoretically and empirically proven to be ex ante budget-balanced, weakly Pareto-efficient, and weakly Bayesian incentive-compatible. A simulation study verifies that the mechanism could largely reduce the computational complexity that the optimal allocation requires, while maintaining approximately the same performance. A user study with 20 subjects further shows the effectiveness of the mechanism in preventing participants from defecting and incentivizing them to reveal flexible preferences."
A Hierarchical Framework of Cloud Resource Allocation and Power Management Using Deep Reinforcement Learning.,"Automatic decision-making approaches, such as reinforcement learning (RL), have been applied to (partially) solve the resource allocation problem adaptively in the cloud computing system. However, a complete cloud resource allocation framework exhibits high dimensions in state and action spaces, which prohibit the usefulness of traditional RL techniques. In addition, high power consumption has become one of the critical concerns in design and control of cloud computing systems, which degrades system reliability and increases cooling cost. An effective dynamic power management (DPM) policy should minimize power consumption while maintaining performance degradation within an acceptable level. Thus, a joint virtual machine (VM) resource allocation and power management framework is critical to the overall cloud computing system. Moreover, novel solution framework is necessary to address the even higher dimensions in state and action spaces. In this paper, we propose a novel hierarchical framework for solving the overall resource allocation and power management problem in cloud computing systems. The proposed hierarchical framework comprises a global tier for VM resource allocation to the servers and a local tier for distributed power management of local servers. The emerging deep reinforcement learning (DRL) technique, which can deal with complicated control problems with large state space, is adopted to solve the global tier problem. Furthermore, an autoencoder and a novel weight sharing structure are adopted to handle the high-dimensional state space and accelerate the convergence speed. On the other hand, the local tier of distributed server power managements comprises an LSTM based workload predictor and a model-free RL based power manager, operating in a distributed manner. Experiment results using actual Google cluster traces show that our proposed hierarchical framework significantly saves the power consumption and energy usage than the baseline while achieving no severe latency degradation. Meanwhile, the proposed framework can achieve the best trade-off between latency and power/energy consumption in a server cluster."
SunChase: Energy-Efficient Route Planning for Solar-Powered EVs.,"Electric vehicles (EVs) play a significant role in the current transportation systems. The main factor that affects the acceptance of existing EV models is the range anxiety problem caused by limited charging stations and long recharge times. Recently, the solar-powered EV has drawn many attentions due to being free of charging limitations. However, the solarpowered EVs may still struggle with the limited use because of unpredictable solar availability. For example, shadings causedby buildings and trees also possibly decrease the solar panel cell efficiency. To address this, we propose a route planning method for solar-powered EVs to balance the energy harvesting and consumption subject to time constraint. The idea behindour solution is to offer power-aware optimal routing, which maximizes the on-road energy input given solar availability on each road segment. We first build a solar access estimation model using 3D geographic data and then employ a multi-criteriasearch method to generate a set of Pareto candidate routes. In order to reduce the size of the set, we leverage the bisect kmeans clustering algorithm to extract the most representative Pareto routes with better solar availability. In the evaluation, wedeveloped a validation platform on the vehicle and leveraged mobile sensing techniques to examine our proposed model in real road environments. We conducted simulations to evaluate our proposed route planning algorithm using real life scenarios. Experimental results demonstrate that our solar input model is robust to real road scenarios, and the routing algorithm has great potential to provide efficient services for solar-powered EV in the future."
Persistent Traffic Measurement Through Vehicle-to-Infrastructure Communications.,"Measuring point traffic volume and point-to-point traffic volume in a road system has important applications in transportation engineering. The connected vehicle technologies integrate wireless communications and computers into transportation systems, allowing wireless data exchanges between vehicles and road-side equipment, and enabling large-scale, sophisticated traffic measurement. This paper investigates the problems of persistent point traffic measurement and persistent point-to-point traffic measurement, which were not adequately studied in the prior art, particularly in the context of intelligent vehicular networks. We propose two novel estimators for privacy-preserving persistent traffic measurement: one for point traffic and the other for point-to-point traffic. The estimators are mathematically derived from the join result of traffic records, which are produced by the electronic roadside units with privacy-preserving data structures. We evaluate our estimation methods using simulations based on both real transportation traffic data and synthetic data. The numerical results demonstrate the effectiveness of the proposed methods in producing high measurement accuracy and allowing accuracy-privacy tradeoff through parameter setting."
TagBreathe: Monitor Breathing with Commodity RFID Systems.,"Breath monitoring helps assess the general personal health and gives clues to chronic diseases. Yet current breath monitoring technologies are inconvenient and intrusive. For instance, typical breath monitoring devices need to attach nasal probes or chest bands to users. Wireless sensing technologies have been applied to monitor breathing using radio waves without physical contact. Those wireless sensing technologies however require customized radios which are not readily available. More importantly, due to interference, such technologies do not work well with multiple users. With multiple users in presence, the detection accuracy of existing systems decreases dramatically. In this paper, we propose to monitor users' breathing using commercial-off-the-shelf (COTS) RFID systems. In our system, passive lightweight RFID tags are attached to users' clothes and backscatter radio waves, and commodity RFID readers report low level data (e.g., phase values). We track periodic body movement due to inhaling and exhaling by analyzing the low level data reported by commodity readers. To enhance the measurement robustness, we synthesize data streams from an array of multiple tags to improve the monitoring accuracy. Our design follows the standard EPC protocol which arbitrates collisions in the presence of multiple tags. We implement a prototype the breath monitoring system with commodity RFID systems. The experiment results show that the prototype system can simultaneously monitor breathing with high accuracy even with the presence of multiple users."
Double-Edged Sword: Incentivized Verifiable Product Path Query for RFID-Enabled Supply Chain.,"Querying the path information of individual products in a supply chain is key to many applications. RFID (Radio-Frequency IDentification) is a main technology to enable product path information query today. With RFID technology, supply chain participants can efficiently track products in transit and record their production information in databases. In this paper, we investigate the following question: how can we conduct privacy-preserving product path information query with verifiability on an RFID-enabled distributed supply chain? We address this question with Double Edged(DE)-Sword, an incentivized verifiable query system. DE-Sword introduces a novel double-edged reputation incentive mechanism to encourage supply chain participants to behave; and couples it with cryptographic primitives and careful protocol design. We evaluate DE-Sword through security analysis and performance experiments. The security analysis shows that DE-Sword guarantees both verifiability and privacy. The experiment results show that DE-Sword achieves low overhead in RFID-enabled supply chain applications."
Towards Accurate Corruption Estimation in ZigBee Under Cross-Technology Interference.,"Cross-Technology Interference affects the operation of low-power ZigBee networks, especially under severe WiFi interference. Accurate corruption estimation is very important to improve the resilience of ZigBee transmissions. However, there are many limitations in existing approaches such as low accuracy, high overhead, and requiring hardware modification. In this paper, we propose an accurate corruption estimation approach, AccuEst, which utilizes per-byte SINR (Signal-to-Interference-and-Noise Ratio) to detect corruption. We combine the use of pilot symbols with per-byte SINR to improve corruption detection accuracy, especially in highly noisy environments (i.e., noise and interference are at the same level). In addition, we design an adaptive pilot instrumentation scheme to strike a good balance between accuracy and overhead. We implement AccuEst on the TinyOS 2.1.1/TelosB platform and evaluate its performance through extensive experiments. Results show that AccuEst improves corruption detection accuracy by 78.6% on average compared with state-of-the-art approach (i.e., CARE) in highly noisy environments. In addition, AccuEst reduces pilot overhead by 53.7% on average compared to the traditional pilot-based approach. We implement AccuEst in a coding-based transmission protocol, and results show that with AccuEst, the packet delivery ratio is improved by 20.3% on average."
Unseen Activity Recognitions: A Hierarchical Active Transfer Learning Approach.,"Human activity recognition (AR) is an essential element for user-centric and context-aware applications. While previous studies showed promising results using various machine learning algorithms, most of them can only recognize the activities that were previously seen in the training data. We investigate the challenges of improving the recognition of unseen daily activities in smart home environment, by better exploiting the hierarchical taxonomy of complex daily activities. We first (a) design a hierarchical representation of complex activity taxonomy in terms of human-readable semantic attributes, and (b) develop a hierarchy of classifiers which incorporates a cluster tree built on the domain knowledge from training samples. Though this model is rich in recognizing complex activities that are previously seen in training data, it is not well versed to recognize unseen complex activities without new training samples. To tackle this challenge, we extend Hierarchical Active Transfer Learning (HATL) approach that exploits semantic attribute cluster structure of complex activities shared between seen (source) and unseen (target) activity domains. Our approach employs transfer and active learning to help label target domain unlabeled data by spawning the most effective queries. We evaluated our approach with two real-time smart home systems (IRB #HP-00064387) which corroborates radical improvements in recognizing unseen complex activities."
RFIPad: Enabling Cost-Efficient and Device-Free In-air Handwriting Using Passive Tags.,"An important function of smart environments is the ubiquitous access of computing devices. In public areas such as hospitals, libraries, and airports, people may want to interact with nearby computing systems to get information, such as directions to a hospital room, locations of books, and flight departure/arrival information. Touch screen based displays and kiosks, which are commonly used today, may incur extra hardware cost or even possible germ and bacteria infection. This work provides a new solution: users can make queries and inputs by performing in-air handwriting to an array of passive RFID tags, named RFIPad. This input method does not require human hands to carry any device and hence is convenient for applications in public areas. Besides the mobile and contactless property, this system is a cost-efficient extension to current RFID systems: an existing reader can monitor multiple RFIPads while performing its regular applications such as identification and tracking. We implement a prototype of RFIPad using commercial off-the-shelf UHF RFID devices. Experimental results show that RFIPad achieves >91% accuracy in recognizing basic touch-screen operations and English letters."
Robust Incentive Tree Design for Mobile Crowdsensing.,"With the proliferation of smart mobile devices (smart phone, tablet, and wearable), mobile crowdsensing becomes a powerful sensing and computation paradigm. It has been put into application in many fields, such as spectrum sensing, environmental monitoring, healthcare, and so on. Driven by promising incentives, the power of the crowd grants crowdsensing an advantage in mobilizing users who perform sensing tasks with the embedded sensors on the smart devices. Auction is one of the commonly adopted crowdsensing incentive mechanisms to incentivize users for participation. However, it does not consider the incentive for user solicitation, where in crowdsensing, such incentive would ease the tension when there is a lack of crowdsensing users. To deal with this issue, we aim to design an auction-based incentive tree to offer rewards to users for both participation and solicitation. Meanwhile, we want the incentive mechanism to be robust against dishonest behavior such as untruthful bidding and sybil attacks, to eliminate malicious price manipulations. We design RIT as a Robust Incentive Tree mechanism for mobile crowdsensing which combines the advantages of auctions and incentive trees. We prove that RIT is truthful and sybil-proof with probability at least H, for any given H ∈ (0, 1). We also prove that RIT satisfies individual rationality, computational efficiency, and solicitation incentive. Simulation results of RIT further confirm our analysis."
WearLock: Unlocking Your Phone via Acoustics Using Smartwatch.,"Smartphone lock screens are implemented to reduce the risk of data loss or compromise given the fact that increasing amount of person data are accessible on smartphones nowadays. Unfortunately, many smartphone users abandon lock screens due to the inconvenience of unlocking their phones many times a day. With the wide adoption of wearables, token-based approaches have gained popularity in simplifying unlocking and retaining security at the same time. To this end, we propose to take advantage of the smartwatch for easy smartphone unlocking. In this paper, we have designed WearLock, a system that uses acoustic tones as tokens to automate the unlocking securely. We build a sub-channel selection and an adaptive modulation in the acoustic modem to maximize unlocking success rate against ambient noise only when those two devices are nearby. We leverage the motion sensor on the smartwatch to reduce the unlock frequency. We offload smartwatch tasks to the smartphone to speed up computation and save energy. We have implemented the WearLock prototype and conducted extensive evaluations. Results achieved a low average bit error rate (BER) as 8% in various experiments. Compared to traditional manual personal identification numbers (PINs) entry, WearLock achieves at least 18% unlock speedup without any manual effort."
Modeling Mobile Code Acceleration in the Cloud.,"The quality of service of a mobile application is critical to ensure user satisfaction. Techniques have been proposed to accomplish adaptation of quality of service dynamically. However, there is still a limited understanding about how to provide a utility model for code execution. One key challenge is modeling the level of quality in the code execution that can be provisioned by the cloud. Since the allocation of cloud resources has a cost, it is important to optimize cloud usage. We propose a software-defined networking approach that allows modeling and controlling code acceleration of a mobile application deployed across multiple type of devices. By segregating the computational requirements of the mobile application into groups, we were able to define the acceleration needed by each group of devices. As the computational requirements of a device can change across time, a mobile device can be re-assigned to another group based on demand. Our SDN approach implements a model that allows the system to predict workload based on acceleration groups. Evaluating our system in a real testbed showed that it is possible to predict workload and allocate optimal resources to handle that workload with 87.5% accuracy."
E-Android: A New Energy Profiling Tool for Smartphones.,"As the limited battery lifetime remains a major factor restricting the applicability of a smartphone, significant research efforts have been devoted to understand the energy consumption in smartphones. Existing energy modeling methods can account energy drain in a fine-grained manner and provide well designed human-battery interfaces for users to characterize energy usage of every app in smartphones. However, in this paper, we demonstrate that there are still pitfalls in current Android energy modeling approaches, leaving collateral energy consumption unaccounted. The existence of collateral energy consumption becomes a serious energy bug. In particular, those energy bugs could be exploited to launch a new class of energy attacks, which deplete battery life and sidestep the supervision of current energy accounting. To unveil collateral energy bugs, we propose E-Android to accurately profile energy consumption of a smartphone in a comprehensive manner. E-Android monitors collateral energy related events and maintains energy consumption maps for relevant apps. We evaluate the effectiveness of E-Android under six different collateral energy attacks and two normal scenarios, and compare the results with those of Android. While Android fails to disclose collateral energy bugs, E-Android can accurately profile energy consumption and reveal the existence of energy bugs with minor overhead."
Local and Low-Cost White Space Detection.,"White spaces are portions of the TV spectrum that are allocated but not used locally. Ifaccurately detected, white spaces offer a valuable new opportunity for highspeed wireless communications. We propose a new method for white space detection that allows a node to actlocally, based on a centrally constructed model, and at low cost, whiledetecting more spectrum opportunities than best known approaches. Weleverage two ideas. First, we demonstrate that low-cost spectrum monitoringhardware can offer ""good enough"" detection capabilities. Second, we develop amodel that combines locally-measured signal features and location to more efficiently detect white space availability. We incorporate these ideas into the design,implementation, and evaluation of a complete system we call Waldo. We deployWaldo on a laptop in the Atlanta metropolitan area in the US covering 700 km2. Our results show that usingsignal features, in addition to location, can improve detection accuracy by up to10x for some channels. We also deploy Waldo on an Android smartphone,demonstrating the feasibility of real-time white space detection with efficientuse of smartphone resources."
General Analysis of Incentive Mechanisms for Peer-to-Peer Transmissions: A Quantum Game Perspective.,"The peer-to-peer transmission is a mainstream in challenged network environments. Yet, the free rider phenomenon in peer-to peer transmissions presses a need for incentive mechanisms to stimulate contributions of data transmission. As a result, it is imperative to answer the questions: whether and to what extent an incentive mechanism can invoke such contributions? To answer these questions, we employ an n-player continuous quantum game model to analyze extrinsic incentive mechanisms (promoting cooperative behaviors by offering rewards), and use the quantum prisoner's dilemma model to analyze intrinsic incentive mechanisms (encouraging reciprocal cooperation by exploiting internal bounds). To the best of our knowledge, we are the first to analyze incentive mechanisms for peer-to-peer transmissions from a quantum game perspective. Such a perspective is adopted because the extended strategy space in the quantum game broadens the range for searching optimal strategies and the introduction of entanglement makes the proposed analytical frameworks more practical due to the consideration of the peers' relationships in decision-making. Our proposed quantum game-based analytical frameworks are generic because they are compatible with classic game-based schemes. Our analytical results can provide straightforward insights on evaluating the potential of incentive mechanisms and can serve as important references for designing new incentive mechanisms."
High-Performance and Resilient Key-Value Store with Online Erasure Coding for Big Data Workloads.,"Distributed key-value store-based caching solutions are being increasingly used to accelerate Big Data applications on modern HPC clusters. This has necessitated incorporating fault-tolerance capabilities into high-performance key-value stores such as Memcached that are otherwise volatile in nature. In-memory replication is being used as the primary mechanism to ensure resilient data operations. However, this incurs increased network I/O with high remote memory requirements. On the other hand, Erasure Coding is being extensively explored for enabling data resilience, while achieving better storage efficiency. In this paper, we first perform an in-depth modeling-based analysis of the performance trade-offs of In-Memory Replication and Erasure Coding schemes for key-value stores, and explore the possibilities of employing Online Erasure Coding for enabling resilience in high-performance key-value stores for HPC clusters. We then design a non-blocking API-based engine to perform efficient Set/Get operations by overlapping the encoding/decoding involved in enabling Erasure Coding-based resilience with the request/response phases, by leveraging RDMA on high performance interconnects. Performance evaluations show that the proposed designs can outperform synchronous RDMA-based replication by about 2.8×, and can improve YCSB throughput and average read/write latencies by about 1.34× - 2.6× over asynchronous replication for larger key-value pair sizes (>16KB). We also demonstrate its benefits by incorporating it into a hybrid and resilient key-value store-based burst-buffer system over Lustre for accelerating Big Data I/O on HPC clusters."
Modeling and Analyzing Latency in the Memcached system.,"Memcached is a widely used in-memory caching solution in large-scale searching scenarios. The most pivotal performance metric in Memcached is latency, which is affected by various factors including the workload pattern, the service rate, the unbalanced load distribution and the cache miss ratio. To quantitate the impact of each factor on latency, we establish a theoretical model for the Memcached system. Specially, we formulate the unbalanced load distribution among Memcached servers by a set of probabilities, capture the burst and concurrent key arrivals at Memcached servers in form of batching blocks, and add a cache miss processing stage. Based on this model, algebraic derivations are conducted to estimate latency in Memcached. The latency estimation is validated by intensive experiments. Moreover, we obtain a quantitative understanding of how much improvement of latency performance can be achieved by optimizing each factor and provide several useful recommendations to optimal latency in Memcached."
Speculative Slot Reservation: Enforcing Service Isolation for Dependent Data-Parallel Computations.,"Priority scheduling is a fundamental tool to provide service isolation for different jobs in shared clusters. Ideally, the performance of a high-priority job should not be dragged down by another with a lower priority. However, we show in this paper that simply assigning a high priority provides no isolation for jobs with dependent computations. A job, even receiving the highest priority, may give up compute slots to another before proceeding to the downstream computation, which is because of barrier, i.e., that the downstream computation cannot start until all the upstream tasks have completed. Such an interruption of execution inevitably results in a significant delay. In this paper, we propose speculative slot reservation that judiciously reserves slots for downstream computations, so as to retain service isolation for high-priority jobs. To mitigate the utilization loss due to slot reservation, we analyze the trade-off between utilization and isolation, and expose a tunable knob to navigate the trade-off. We also propose a complementary straggler mitigation strategy that uses the reserved slots to run extra copies of slow tasks. We have implemented speculative slot reservation in Spark. Evaluations based on both cluster deployment and trace-driven simulations show that our approach enforces strict service isolation for high-priority jobs, without slowing down the other jobs with a lower priority."
Optimizing Shuffle in Wide-Area Data Analytics.,"As increasingly large volumes of raw data are generated at geographically distributed datacenters, they need to be efficiently processed by data analytic jobs spanning multiple datacenters across wide-area networks. Designed for a single datacenter, existing data processing frameworks, such as Apache Spark, are not able to deliver satisfactory performance when these wide-area analytic jobs are executed. As wide-area networks interconnecting datacenters may not be congestion free, there is a compelling need for a new system framework that is optimized for wide-area data analytics. In this paper, we design and implement a new proactive data aggregation framework based on Apache Spark, with a focus on optimizing the network traffic incurred in shuffle stages of data analytic jobs. The objective of this framework is to strategically and proactively aggregate the output data of mapper tasks to a subset of worker datacenters, as a replacement to Spark's original passive fetch mechanism across datacenters. It improves the performance of wide-area analytic jobs by avoiding repetitive data transfers, which improves the utilization of inter-datacenter links. Our extensive experimental results using standard benchmarks across six Amazon EC2 regions have shown that our proposed framework is able to reduce job completion times by up to 73%, as compared to the existing baseline implementation in Spark."
Job Scheduling without Prior Information in Big Data Processing Systems.,"Job scheduling plays an important role in improving the overall system performance in big data processing frameworks. Simple job scheduling policies, such as Fair and FIFO scheduling, do not consider job sizes and may degrade the performance when jobs of varying sizes arrive. More elaborate job scheduling policies make the convenient assumption that jobs are recurring, and complete information about their sizes is available from their prior runs. In this paper, we design and implement an efficient and practical job scheduler for big data processing systems to achieve better performance even without prior information about job sizes. The superior performance of our job scheduler originates from the design of multiple level priority queues, where jobs are demoted to lower priority queues if the amount of service consumed so far reaches a certain threshold. In this case, jobs in need of a small amount of service can finish in the topmost several levels of queues, while jobs that need a large amount of service to complete are moved to lower priority queues to avoid head-of-line blocking. Our new job scheduler can effectively mimic the shortest job first scheduling policy without knowing the job sizes in advance. To demonstrate its performance, we have implemented our new job scheduler in YARN, a popular resource manager used by Hadoop/Spark, and validated its performance with both experiments on real datasets and large-scale trace-driven simulations. Our experimental and simulation results have strongly confirmed the effectiveness of our design: our new job scheduler can reduce the average job response time of the Fair scheduler by up to 45%."
Distributed Load Balancing in Key-Value Networked Caches.,"Modern web services rely on a network of distributed cache servers to efficiently deliver content to users. Load imbalance among cache servers can substantially degrade content delivery performance. Due to the skewed and dynamic nature of real-world workloads, cache servers that serve viral content experience higher load as compared to other cache servers. We propose a novel distributed load balancing protocol called Meezan to address the load imbalance among cache servers. Meezan replicates popular objects to mitigate skewness and adjusts hash space boundaries in response to load dynamics in a novel way. Our theoretical analysis shows that Meezan achieves near perfect load balancing for a wide range of operating parameters. Our trace driven simulations shows that Meezan reduces load imbalance by up to 52% as compared to prior solutions."
Cognitive Context-Aware Distributed Storage Optimization in Mobile Cloud Computing: A Stable Matching Based Approach.,"Mobile cloud storage (MCS) is being extensively used nowadays to provide data access services to various mobile platforms such as smart phones and tablets. For cross-platform mobile apps, MCS is a foundation for sharing and accessing user data as well as supporting seamless user experience in a mobile cloud computing environment. However, the mobile usage of smart phones or tablets is quite different from legacy desktop computers, in the sense that each user has his/her own mobile usage pattern. Therefore, it is challenging to design an efficient MCS that is optimized for individual users. In this paper, we investigate a distributed MCS system whose performance is optimized by exploiting the fine-grained context information of every mobile user. In this distributed system, lightweight storage servers are deployed pervasively, such that data can be stored closer to its user. We systematically optimize the data access efficiency of such a distributed MCS by exploiting three types of user context information: mobility pattern, network condition, and data access pattern. We propose two optimization formulations: a centralized one based on mixed-integer linear programming (MILP), and a distributed one based on stable matching. We then develop solutions to both formulations. Comprehensive simulations are performed to evaluate the effectiveness of the proposed solutions by comparing them against their counterparts under various network and context conditions."
Fair Caching Algorithms for Peer Data Sharing in Pervasive Edge Computing Environments.,"Edge devices (e.g., smartphones, tablets, connected vehicles, IoT nodes) with sensing, storage and communication resources are increasingly penetrating our environments. Many novel applications can be created when nearby peer edge devices share data. Caching can greatly improve the data availability, retrieval robustness and latency. In this paper, we study the unique issue of caching fairness in edge environment. Due to distinct ownership of peer devices, caching load balance is critical. We consider fairness metrics and formulate an integer linear programming problem, which is shown as summation of multiple Connected Facility Location (ConFL) problems. We propose an approximation algorithm leveraging an existing ConFL approximation algorithm, and prove that it preserves a 6.55 approximation ratio. We further develop a distributed algorithm where devices exchange data reachability and identify popular candidates as caching nodes. Extensive evaluation shows that compared with existing wireless network caching algorithms, our algorithms significantly improve data caching fairness, while keeping the contention induced latency similar to the best existing algorithms."
Latency-Driven Cooperative Task Computing in Multi-user Fog-Radio Access Networks.,"Fog computing is emerging as one promising solution to meet the increasing demand for ultra-low latency services in wireless networks. Taking a forward-looking perspective, we propose a Fog-Radio Access Network (F-RAN) model, which utilizes the existing infrastructure, e.g., small cells and macro base stations, to achieve the ultra-low latency by joint computing across multiple F-RAN nodes and near-range communications at the edge. We treat the low latency design as an optimization problem, which characterizes the tradeoff between communication and computing across multiple F-RAN nodes. Since this problem is NP-hard, we propose a latency-driven cooperative task computing algorithm with one-for-all concept for simultaneous selection of the F-RAN nodes to serve with proper heterogeneous resource allocation for multi-user services. Considering the limited heterogeneous resources shared among all users, we advocate the one-for-all strategy for every user taking other's situation into consideration and seek for a ""win-win"" solution. The numerical results show that the low latency services can be achieved by F-RAN via latency-driven cooperative task computing."
Approximation and Online Algorithms for NFV-Enabled Multicasting in SDNs.,"Multicasting is a fundamental functionality of networks for many applications including online conferencing, event monitoring, video streaming, and system monitoring in data centers. To ensure multicasting reliable, secure and scalable, a service chain consisting of network functions (e.g., firewalls, Intrusion Detection Systems (IDSs), and transcoders) usually is associated with each multicast request. Such a multicast request is referred to as an NFV-enabled multicast request. In this paper we study NFV-enabled multicasting in a Software-Defined Network (SDN) with the aims to minimize the implementation cost of each NFV-enabled multicast request or maximize the network throughput for a sequence of NFV-enabled requests, subject to network resource capacity constraints. We first formulate novel NFV-enabled multicasting and online NFV-enabled multicasting problems. We then devise the very first approximation algorithm with an approximation ratio of 2K for the NFV-enabled multicasting problem if the number of servers for implementing the network functions of each request is no more than a constant K (1). We also study dynamic admissions of NFV-enabled multicast requests without the knowledge of future request arrivals with the objective to maximize the network throughput, for which we propose an online algorithm with a competitive ratio of O(log n) when K = 1, where n is the number of nodes in the network. We finally evaluate the performance of the proposed algorithms through experimental simulations. Experimental results demonstrate that the proposed algorithms outperform other existing heuristics."
Distributed Auctions for Task Assignment and Scheduling in Mobile Crowdsensing Systems.,"With the emergence of Mobile Crowdsensing Systems (MCSs), many auction schemes have been proposed to incentivize mobile users to participate in sensing activities. However, in most of the existing work, the heterogeneity of MCSs has not been fully exploited. To tackle this issue, in this paper, we study the joint problem of sensing task assignment and scheduling while considering partial fulfillment, attribute diversity, and price diversity. We first elaborately model the problem as a reverse auction and design a distributed auction framework. Then, based on this framework, we propose two distributed auction schemes, cost-preferred auction scheme (CPAS) and time schedule-preferred auction scheme (TPAS), which differ on the methods of task scheduling, winner determination, and payment computation. We further rigorously prove that both CPAS and TPAS can achieve computational-efficiency, individual-rationality, budget-balance, and truthfulness. Finally, the simulation results validate the effectiveness of both CPAS and TPAS in terms of sensing task's allocation efficiency, mobile user's working time utilization and utility, and truthfulness."
Effective Mobile Data Trading in Secondary Ad-hoc Market with Heterogeneous and Dynamic Environment.,"Advances in smartphone technologies enable mobile data subscribers to resell their data allowance to other users, creating a secondary data market. The trading environment of this secondary data market is dynamic and ad-hoc: buyers and sellers join and leave the market at all times, changing the trading landscape constantly. The amount of data demanded and offered at any point in time also vary. These conditions make determining a fair transaction price, and matching buyers to sellers difficult in practice. Prior schemes utilize global description of the network and market forces to achieve good performance, but the implementation requires a high overhead cost. In this paper, we present DataMart, a data pricing and user matching platform for trading in this dynamic, ad-hoc and heterogeneous market that works in distributed manner without needing global information. Using insights from real world traces, we demonstrate via simulation that our pricing scheme is converging and consistent with the law of demand and supply. Further, our user matching scheme achieves comparable performance to the optimal solution. We implement a prototype on Android platform, and the experiment results confirm the effectiveness of DataMart."
Kalis - A System for Knowledge-Driven Adaptable Intrusion Detection for the Internet of Things.,"In this paper, we introduce Kalis, a self-adapting, knowledge-driven expert Intrusion Detection System able to detect attacks in real time across a wide range of IoT systems. Kalis does not require changes to existing IoT software, can monitor a wide variety of protocols, has no performance impact on applications on IoT devices, and enables collaborative security scenarios. Kalis is the first comprehensive approach to intrusion detection for IoT that does not target individual protocols or applications, and adapts the detection strategy to the specific network features. Extensive evaluation shows that Kalis is effective and efficient in detecting attacks to IoT systems."
Fuzzy Extractors for Biometric Identification.,"Fuzzy extractor provides key generation from biometrics and other noisy data. The generated key is seamlessly usable for any cryptographic applications because its information entropy is sufficient for security. Biometric authentication offers natural and passwordless user authentication in various systems where fuzzy extractors can be used for biometric information security. Typically, a biometric system operates in two modes: verification and identification. However, existing fuzzy extractors does not support efficient user identification. In this paper, we propose a succinct fuzzy extractor scheme which enables efficient biometric identification as well as verification that it satisfies the security requirements. We show that the proposed scheme can be easily used in both verification and identification modes. To the best of our knowledge, we propose the first fuzzy extractor based biometric identification protocol. The proposed protocol is able to identify a user with constant computational cost rather than linear-time computation required by other fuzzy extractor schemes. We also provide security analysis of proposed schemes to show their security levels. The implementation shows that the performance of proposed identification protocol is constant and it is close to that of verification protocols."
Smartphone Privacy Leakage of Social Relationships and Demographics from Surrounding Access Points.,"While the mobile users enjoy the anytime anywhere Internet access by connecting their mobile devices through Wi-Fi services, the increasing deployment of access points (APs) have raised a number of privacy concerns. This paper explores the potential of smartphone privacy leakage caused by surrounding APs. In particular, we study to what extent the users' personal information such as social relationships and demographics could be revealed leveraging simple signal information from APs without examining the Wi-Fi traffic. Our approach utilizes users' activities at daily visited places derived from the surrounding APs to infer users' social interactions and individual behaviors. Furthermore, we develop two new mechanisms: the Closeness-based Social Relationships Inference algorithm captures how closely people interact with each other by evaluating their physical closeness and derives fine-grained social relationships, whereas the Behavior-based Demographics Inference method differentiates various individual behaviors via the extracted activity features (e.g., activeness and time slots) at each daily place to reveal users' demographics. Extensive experiments conducted with 21 participants' real daily life including 257 different places in three cities over a 6-month period demonstrate that the simple signal information from surrounding APs have a high potential to reveal people's social relationships and infer demographics with an over 90% accuracy when using our approach."
EV-Matching: Bridging Large Visual Data and Electronic Data for Efficient Surveillance.,"Visual (V) surveillance systems are extensively deployed and becoming the largest source of big data. On the other hand, electronic (E) data also plays an important role in surveillance and its amount increases explosively with the ubiquity of mobile devices. One of the major problems in surveillance is to determine human objects' identities among different surveillance scenes. Traditional way of processing big V and E datasets separately does not serve the purpose well because V data and E data are imperfect alone for information gathering and retrieval. Matching human objects in the two datasets can merge the good of the two for efficient large-scale surveillance. Yet such matching across two heterogeneous big datasets is challenging. In this paper, we propose an efficient set of parallel algorithms, called EV-Matching, to bridge big E and V data. We match E and V data based on their spatiotemporal correlation. The EV-Matching algorithms are implemented on Apache Spark to further accelerate the whole procedure. We conduct extensive experiments on a large synthetic dataset under different settings. Results demonstrate the feasibility and efficiency of our proposed algorithms."
Adaptive Reconnaissance Attacks with Near-Optimal Parallel Batching.,"In assessing privacy on online social networks, it is important to investigate their vulnerability to reconnaissance strategies, in which attackers lure targets into being their friends by exploiting the social graph in order to extract victims' sensitive information. As the network topology is only partially revealed after each successful friend request, attackers need to employ an adaptive strategy. Existing work only considered a simple strategy in which attackers sequentially acquire one friend at a time, which causes tremendous delay in waiting for responses before sending the next request, and which lack the ability to retry failed requests after the network has changed. In contrast, we investigate an adaptive and parallel strategy, of which attackers can simultaneously send multiple friend requests in batch and recover from failed requests by retrying after topology changes, thereby significantly reducing the time to reach the targets and greatly improving robustness. We cast this approach as an optimization problem, Max-Crawling, and show it inapproximable within (1 - 1/e + ε). We first design our core algorithm P
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">M</sub>
-AReST which has an approximation ratio of (1 - e
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">-(1-1/e)</sup>
) using adaptive monotonic submodular properties. We next tighten our algorithm to provide a nearoptimal solution, i.e. having a ratio of (1 - 1/e), via a two-stage stochastic programming approach. We further establish the gap bound of (1 - e
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">-(1-1/e)2</sup>
) between batch strategies versus the optimal sequential one. We experimentally validate our theoretical results, finding that our algorithm performs nearoptimally in practice and that this is robust under a variety of problem settings."
Achieving Strong Privacy in Online Survey.,"Thanks to the proliferation of Internet access and modern digital and mobile devices, online survey has been flourishing into data collection of marketing, social, financial and medical studies. However, traditional data collection methods in online survey suffer from serious privacy issues. Existing privacy protection techniques are not adequate for online survey for lack of strong privacy. In this paper, we propose a practical strong privacy online survey scheme SPS based on a novel data collection technique called dual matrix masking (DM2), which guarantees the correctness of the tallying results with low computation overhead, and achieves universal verifiability, robustness and strong privacy. We also propose a more robust scheme RSPS, which incorporates multiple distributed survey managers. The RSPS scheme preserves the nice properties of SPS, and further achieves robust strong privacy against joint collusion attack. Through extensive analyses, we demonstrate our proposed schemes can be efficiently applied to online survey with accuracy and strong privacy."
Service Overlay Forest Embedding for Software-Defined Cloud Networks.,"Network Function Virtualization (NFV) on Software-Defined Networks (SDN) can effectively optimize the allocation of Virtual Network Functions (VNFs) and the routing of network flows simultaneously. Nevertheless, most previous studies on NFV focus on unicast service chains and thereby are not scalable to support a large number of destinations in multicast. On the other hand, the allocation of VNFs has not been supported in the current SDN multicast routing algorithms. In this paper, therefore, we make the first attempt to tackle a new challenging problem for finding a service forest with multiple service trees, where each tree contains multiple VNFs required by each destination. Specifically, we formulate a new optimization, named Service Overlay Forest (SOF), to minimize the total cost of all allocated VNFs and all multicast trees in the forest. We design a new 3ρ
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">ST</sub>
-approximation algorithm to solve the problem, where ρ
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">ST</sub>
 denotes the best approximation ratio of the Steiner Tree problem, and the distributed implementation of the algorithm is also presented. Simulation results on real networks for data centers manifest that the proposed algorithm outperforms the existing ones by over 25%. Moreover, the implementation of an experimental SDN with HP OpenFlow switches indicates that SOF can significantly improve the QoE of the Youtube service."
Joint Optimization of Chain Placement and Request Scheduling for Network Function Virtualization.,"Compared with executing Network Functions (NFs) on dedicated hardwares, the recent trend of Network Function Virtualization (NFV) holds the promise for operators to flexibly deploy software-based NFs on commodity servers. However, virtual NFs (VNFs) are normally ""chained"" together to provide a specific network service. Thus, an efficient scheme is needed to place the VNF chains across the network and effectively schedule requests to service instances, which can maximize the average resource utilization of each node in service and simultaneously minimize the average response latency of each request. To this end, we formulate first VNF chains placement problem as a variant of bin-packing problem, which is NP-hard, and we model request scheduling problem based on the key concepts from open Jackson network. To jointly optimize the performance of NFV, we propose a priority-driven weighted algorithm to improve resource utilization and a heuristic algorithm to reduce response latency. Through extensive trace-driven simulations, we show that our methods can indeed enhance performance in diverse scenarios. In particular, we can improve the average resource utilization by 33.4% and can reduce the average total latency by 19.9% as compared with the state-of-the-art methods."
BIG Cache Abstraction for Cache Networks.,"In this paper, we advocate the notion of ""BIG"" cache as an innovative abstraction for effectively utilizing the distributed storage and processing capacities of all servers in a cache network. The ""BIG"" cache abstraction is proposed to partly address the problem of (cascade) thrashing in a hierarchical network of cache servers, where it has been known that cache resources at intermediate servers are poorly utilized, especially under classical cache replacement policies such as LRU. We lay out the advantages of ""BIG"" cache abstraction and make a strong case both from a theoretical standpoint as well as through simulation analysis. We also develop the dCLIMB cache algorithm to minimize the overheads of moving objects across distributed cache boundaries and present a simple yet effective heuristic for addressing the cache allotment problem in the design of ""BIG"" cache abstraction."
Distributed QR Decomposition Framework for Training Support Vector Machines.,"Support Vector Machines (SVM) belong to a class of supervised machine learning algorithms with applications in classification and regression analysis. SVM training is modeled as a convex optimization problem that is computationally tedious and has large memory requirements. Specifically, it is a quadratic programming problem which scales rapidly with the training set size rather than the dimensionality of the feature space. In this work, we first present a novel QR decomposition framework (QRSVM) to efficiently model and solve a large scale SVM problem by capitalizing on low-rank representations of the full kernel matrix rather than solving the problem as a sequence of smaller sub-problems. The low-rank structure of the kernel matrix is leveraged to transform the dense matrix into one with a sparse and separable structure. The modified SVM problem requires significantly lesser memory and computation. Our approach scales linearly with the training set size which makes it applicable to large datasets. This motivates towards our another contribution; exploring a distributed QRSVM framework to solve large-scale SVM classification problems in parallel across a cluster of computing nodes. We also derive an optimal step size for fast convergence of the dual ascent method which is used to solve the quadratic programming problem."
Distributively Computing Random Walk Betweenness Centrality in Linear Time.,"Betweenness centrality of a node represents its influence over the spread of information in the network. It is normally defined as the ratio of the number of shortest paths passing through the node among all shortest paths. However, the spread of information may not just pass through the shortest paths which is captured by a new measure of betweenness centrality based on random walks [1]. The random walk betweenness centrality of a node means how often it is traversed by a random walk between all pairs of other nodes. In this paper, we propose an O(n log n) time distributed randomized approximation algorithm for calculating each node's random walk betweenness centrality with an approximation ratio (1-ϵ) where n is the number of nodes and ϵ is an arbitrarily small constant between 0 and 1. Our distributed algorithm is designed under the widely used CONGEST model, where each edge can only transfer O(log n) bits in each round. To our best knowledge, this is the first distributed algorithm for computing the random walk betweenness centrality. Moreover, we give a non-trivial lower bound for distributively computing the exact random walk betweenness centrality under the CONGEST model, which is Ω(n\log n +D) where D is the network diameter. This means exactly computing random walk betweenness cannot be done in sublinear time."
DeGPar: Large Scale Topic Detection Using Node-Cut Partitioning on Dense Weighted Graphs.,"Topic Detection (TD) refers to automatic techniques for locating topically related material in web documents. Nowadays, massive amounts of documents are generated by users of Online Social Networks (OSNs), in form of very short text, tweets and snippets of news. While topic detection, in its traditional form, is applied to a few documents containing a lot of information, the problem has now changed to dealing with massive number of documents with very little information. The traditional solutions, thus, fall short either in scalability (due to huge number of input items) or sparsity (due to insufficient information per input item). In this paper we address the scalability problem by introducing an efficient and scalable graph based algorithm for TD on short texts, leveraging dimensionality reduction and clustering techniques. We first, compress the input set of documents into a dense graph, such that frequent cooccurrence patterns in the documents create multiple dense topological areas in the graph. Then, we partition the graph into multiple dense sub-graphs, each representing a topic. We compare the accuracy and scalability of our solution with two state-of-the-art solutions (including the standard LDA, and BiTerm). The results on two widely used benchmark datasets show that our algorithm not only maintains a similar or better accuracy, but also performs by an order of magnitude faster than the state-of-the-art approaches."
Networked Stochastic Multi-armed Bandits with Combinatorial Strategies.,"In this paper, we investigate a largely extended version of classical MAB problem, called networked combinatorial bandit problems. In particular, we consider the setting of a decision maker over a networked bandits as follows: each time a combinatorial strategy, e.g., a group of arms, ischosen, and the decision maker receives a rewardresulting from her strategy and also receives a side bonusresulting from that strategy for each arm's neighbor. This is motivated by many real applications such as on-line social networks where friends can provide their feedback on shared content, therefore if we promote a product to a user, we can also collect feedback from her friends on that product. To this end, we consider two types of side bonus in this study: side observation and side reward. Upon the number of arms pulled at each time slot, we study two cases: single-play and combinatorial-play. Consequently, this leaves us four scenarios to investigate in the presence of side bonus: Single-play with Side Observation, Combinatorial-play with Side Observation, Single-play with Side Reward, and Combinatorial-play with Side Reward. For each case, we present and analyze a series of zero regret polices where the expect of regret over time approaches zero as time goes to infinity. Extensive simulations validate the effectiveness of our results."
Computability of Perpetual Exploration in Highly Dynamic Rings.,"We consider systems made of autonomous mobile robots evolving in highly dynamic discrete environment i.e., graphs where edges may appear and disappear unpredictably without any recurrence, stability, nor periodicity assumption. Robots are uniform (they execute the same algorithm), they are anonymous (they are devoid of any observable ID), they have no means allowing them to communicate together, they share no common sense of direction, and they have no global knowledge related to the size of the environment. However, each of them is endowed with persistent memory and is able to detect whether it stands alone at its current location. A highly dynamic environment is modeled by a graph such that its topology keeps continuously changing over time. In this paper, we consider only dynamic graphs in which nodes are anonymous, each of them is infinitely often reachable from any other one, and such that its underlying graph (i.e., the static graph made of the same set of nodes and that includes all edges that are present at least once over time) forms a ring of arbitrary size. In this context, we consider the fundamental problem of perpetual exploration: each node is required to be infinitely often visited by a robot. This paper analyzes the computability of this problem in (fully) synchronous settings, i.e., we study the deterministic solvability of the problem with respect to the number of robots. We provide three algorithms and two impossibility results that characterize, for any ring size, the necessary and sufficient number of robots to perform perpetual exploration of highly dynamic rings."
Locally Self-Adjusting Skip Graphs.,"We present a distributed self-adjusting algorithm for skip graphs that minimizes the average routing costs between arbitrary communication pairs by performing topological adaptation to the communication pattern. Our algorithm is fully decentralized, conforms to the CONGEST model (i.e. uses O(logn) bit messages), and requires O(logn) bits of memory for each node, where n is the total number of nodes. Upon each communication request, our algorithm first establishes communication by using the standard skip graph routing, and then locally and partially reconstructs the skip graph topology to perform topological adaptation. We propose a computational model for such algorithms, as well as a yardstick (working set property) to evaluate them. Our working set property can also be used to evaluate self-adjusting algorithms for other graph classes where multiple tree-like subgraphs overlap (e.g. hypercube networks). We derive a lower bound of the amortized routing cost for any algorithm that follows our model and serves an unknown sequence of communication requests. We show that the routing cost of our algorithm is at most a constant factor more than the amortized routing cost of any algorithm conforming to our computational model. We also show that the expected transformation cost for our algorithm is at most a logarithmic factor more than the amortized routing cost of any algorithm conforming to our computational model."
Online to Offline Business: Urban Taxi Dispatching with Passenger-Driver Matching Stability.,"In the Online to Offline (O2O) taxi business (e.g., Uber), the interests of passengers, taxi drivers, and the company may not align with one another, since taxis do not belong to the company. To balance these interests, this paper studies the taxi dispatch problem for the O2O taxi business. The interests of passengers and taxi drivers are modeled. For non-sharing taxi dispatches (multiple passenger requests cannot share a taxi), a stable marriage approach is proposed. It can deal with unequal numbers of passenger requests and taxis through matching them to dummy partners. Given dummy partners, stable matchings are proved to exist. Three rules are presented to find out all possible stable matchings. For sharing taxi dispatches (multiple passenger requests can share a taxi), passenger requests are packed through solving a maximum set packing problem. Packed passenger requests are regarded as a single request for matching taxis. Extensive real data-driven experiments demonstrate how well our approach performs. The proposed algorithms have a limited performance gap to the literature in terms of the dispatch delay and the passenger satisfaction, but they significantly improve upon existing algorithms in terms of the taxi satisfaction."
An Optimization Framework for Online Ride-Sharing Markets.,"Taxi services and product delivery services are instrumental for our modern society. Thanks to the emergence of sharing economy, ride-sharing services such as Uber, Didi, Lyft and Google's Waze Rider are becoming more ubiquitous and grow into an integral part of our everyday lives. However, the efficiency of these services are severely limited by the sub-optimal and imbalanced matching between the supply and demand. We need a generalized framework and corresponding efficient algorithms to address the efficient matching, and hence optimize the performance of these markets. Existing studies for taxi and delivery services are only applicable in scenarios of the one-sided market. In contrast, this work investigates a highly generalized model for the taxi and delivery services in the market economy (abbreviated as""taxi and delivery market"") that can be widely used in two-sided markets. Further, we present efficient online and offline algorithms for different applications. We verify our algorithm with theoretical analysis and trace-driven simulations under realistic settings."
Fast and Accurate Tracking of Population Dynamics in RFID Systems.,"RFID systems have been widely deployed for various applications such as supply chain management, indoor localization, inventory control, and access control. This paper deals with the fundamental problem of estimating the number of arriving and departing tags between any two time instants in dynamically changing RFID tag populations, which is needed in many applications such as warehouse monitoring and privacy sensitive RFID systems. In this paper, we propose a dynamic tag estimation scheme, namely DTE, that can achieve arbitrarily high required reliability, is compliant with the C1G2 standard, and works in single as well as multiple-reader environment. DTE uses the standardized frame slotted Aloha protocol and utilizes the number of slots that change their values in corresponding Aloha frames at the two time instants to estimate the number of arriving and departing tags. It is easy to deploy because it neither requires modification to tags nor to the communication protocol between tags and readers. We have extensively evaluated and compared DTE with the only prior scheme, ZDE, that can estimate the number of arriving and departing tags. Unfortunately, ZDE can not achieve arbitrarily high required reliability. In contrast, our proposed scheme always achieves the required reliability. For example, for a tag population containing 10 4 tags, a required reliability of 95%, and a required confidence interval of 5%, DTE takes 5.12 seconds to achieve the required reliability whereas ZDE achieves a reliability of only 66% in the same amount of time."
Robust Indoor Wireless Localization Using Sparse Recovery.,"With the multi-antenna design of WiFi interfaces, phased array has become a promising mechanism for accurate WiFi localization. State-of-the-art WiFi-based solutions using AoA (Angle-of-Arrival), however, face a number of critical challenges. First, their localization accuracy degrades dramatically when the Signal-to-Noise Ratio (SNR) becomes low. Second, they do not fully utilize coherent processing across all available domains. In this paper, we present ROArray, a Robust Array based system that accurately localizes a target even with low SNRs. In the spatial domain, ROArray can produce sharp AoA spectrums by parameterizing the steering vector based on a sparse grid. Then, to expand into the frequency domain, it jointly estimates the ToAs (Time-of-Arrival) and AoAs of all the paths using multi-subcarrier OFDM measurements. Furthermore, through multi-packet fusion, ROArray is enabled to perform coherent estimation across the spatial, frequency, and time domains. Such coherent processing not only increases the virtual aperture size, which enlarges the number of maximum resolvable paths, but also improves the system robustness to noise. Our implementation using off-the-shelf WiFi cards demonstrates that, with low SNRs, ROArray significantly outperforms state-of-the-art solutions in terms of localization accuracy; when medium or high SNRs are present, it achieves comparable accuracy."
Max-Min Fair Resource Allocation in HetNets: Distributed Algorithms and Hybrid Architecture.,"We study the resource allocation problem in RAN-level integrated HetNets. This emerging HetNets paradigm allows for dynamic traffic splitting across radio access technologies for each client, and then for aggregating the traffic inside the network to improve the overall resource utilization. We focus on the max-min fair service rate allocation across the clients, and study the properties of the optimal solution. Based on the analysis, we design a low complexity distributed algorithm that tries to achieve max-min fairness. We also design a hybrid network architecture that leverages opportunistic centralized network supervision to augment the distributed solution. We analyze the performance of our proposed algorithms and prove their convergence. We also derive conditions under which the outcome is optimal. When the conditions are not satisfied, we provide constant upper and lower bounds on the optimality gap. Finally, we study the convergence time of our distributed solution and show that leveraging appropriate policies in its design significantly reduces the convergence time."
Optimization of Full-View Barrier Coverage with Rotatable Camera Sensors.,"In all the researches in wireless sensor networks, cameras are increasingly utilized for their surveillance capabilities. In this paper, we elaborately discuss about the problem of Full-View Barrier Coverage with Rotatable Camera Sensors (FBR), including weakly and strongly connected versions. FBR is proven to be NP-hard in this paper by reducing Group Steiner Tree problem to it. Our goal is to reduce sensor number when guaranteeing the surveillance capabilities at the same time. Correspondingly, we introduce a novel weighed graph structure called Full-View Barrier Graph. We transform weak version problem into a pseudo one-dimension one and propose W-GraProj algorithm with the help of dynamic programming; in strong version problem, we introduce two centralized algorithms (S-Dijkstra, S-Thorup), respectively aiming to save sensor num-ber and to reduce time complexity. Moreover, we rigorously analyze the correctness and time complexity for each algorithm. In addition, the mass number of experiments are conducted to validate the efficiency of all algorithms, which prove that our structures and algorithms can construct a full-view barrier with fewer camera sensors compared with previous researches."
Communication through Symbol Silence: Towards Free Control Messages in Indoor WLANs.,"Efficient design of wireless networks benefits from the exchange of control messages. However, control message itself consumes scarce channel resources. In this paper, we propose CoS (Communication through symbol Silence), a novel communication strategy that conveys control messages for free without consuming extra channel resources. CoS inserts silence symbols in data packets and leverages the intervals between inserted silence symbols to encode information. The silence symbols can be located by energy detection at the granularity of symbols and the intervals are interpreted into transmitted control messages. Based on our key insights that the channel code is under-utilized in current wireless networks and the distribution of symbol errors within a data packet is predictable in indoor wireless transmissions, the symbols erased by silence symbols are recovered by the coding redundancy that is originally used to correct symbol errors. A rate adaptation scheme is designed to dynamically adjust the rate of free control messages according to channel conditions so that the transmission of free control messages does not harm the original data throughput. We implement CoS on our software defined radio platform to validate the feasibility of CoS. The extensive results show that the control messages are delivered with close to 100% accuracy in a large SNR range. In addition, we measure the achievable capacity of free control messages in various channel conditions."
Secure Connectivity of Wireless Sensor Networks Under Key Predistribution with on/off Channels.,"Security is an important issue in wireless sensor networks (WSNs), which are often deployed in hostile environments. The q-composite key predistribution scheme has been recognized as a suitable approach to secure WSNs. Although the q-composite scheme has received much attention in the literature, there is still a lack of rigorous analysis for secure WSNs operating under the q-composite scheme in consideration of the unreliability of links. One main difficulty lies in analyzing the network topology whose links are not independent. Wireless links can be unreliable in practice due to the presence of physical barriers between sensors or because of harsh environmental conditions severely impairing communications. In this paper, we resolve the difficult challenge and investigate k-connectivity in secure WSNs operating under the q-composite scheme with unreliable communication links modeled as independent on/off channels, where k-connectivity ensures connectivity despite the failure of any (k - 1) sensors or links, and connectivity means that any two sensors can find a path in between for secure communication. Specifically, we derive the asymptotically exact probability and a zero-one law for k-connectivity. We further use the theoretical results to provide design guidelines for secure WSNs. Experimental results also confirm the validity of our analytical findings."
iUpdater: Low Cost RSS Fingerprints Updating for Device-Free Localization.,"While most existing indoor localization techniques are device-based, many emerging applications such as intruder detection and elderly monitoring drive the needs of device-free localization, in which the target can be localized without any device attached. Among the diverse techniques, received signal strength (RSS) fingerprint-based methods are popular because of the wide availability of RSS readings in most commodity hardware. However, current fingerprint-based systems suffer from high human labor cost to update the fingerprint database and low accuracy due to the large degree of RSS variations. In this paper, we propose a fingerprint-based device-free localization system named iUpdater to significantly reduce the labor cost and increase the accuracy. We present a novel self-augmented regularized singular value decomposition (RSVD) method integrating the sparse attribute with unique properties of the fingerprint database. iUpdater is able to accurately update the whole database with RSS measurements at a small number of reference locations, thus reducing the human labor cost. Furthermore, iUpdater observes that although the RSS readings vary a lot, the RSS differences between both the neighboring locations and adjacent wireless links are relatively stable. This unique observation is applied to overcome the short-term RSS variations to improve the localization accuracy. Extensive experiments in three different environments over 3 months demonstrate the effectiveness and robustness of iUpdater."
Influence Maximization in a Many Cascades World.,"Online Social Networks (OSNs) are widely utilized in viral marketing campaigns exploiting the word-of-mouth effect. Various propagation models have been proposed to describe the way cascades unfold in OSNs. Based on the existing propagation models, several studies address the problem of influence maximization, where the objective is to identify an appropriate subset of users to initiate the spread of a contagion. However, existing approaches ignore an important factor in the propagation process, i.e., the correlation of multiple contagions simultaneously cascading in the social network and how these affect the users' decisions regarding the adoption of a contagion. Although recent works look into either the competition or the complementarity among a pair of contagions, a uniform model that describes the propagation of multiple cascades with varying types and degrees of correlations is lacking. This work constitutes the first attempt to fill this gap. We formulate a novel propagation model, the Correlated Contagions Dynamic Linear Threshold (CCDLT), that considers the correlation of many contagions in either competitive or complementary manner. Our proposed model allows for different degrees of competition/complementarity among cascades. We further consider that users may dynamically switch states regarding the contagion they promote during the propagation process, based on the influence of their neighborhoods. We then design a greedy seed selection algorithm that identifies the appropriate subset of users to participate in a specific contagion in order to maximize its spread and we formally prove that it approximates the best solution at a ratio of 1 - 1/e. Through an extensive experimental evaluation we demonstrate the superiority of our approach over existing schemes."
Expertise-Aware Truth Analysis and Task Allocation in Mobile Crowdsourcing.,"Mobile crowdsourcing has received considerable attention as it enables people to collect and share large volume of data through their mobile devices. Since the accuracy of the collected data is usually hard to ensure, researchers have proposed techniques to identify truth from noisy data by inferring and utilizing the reliability of users, and allocate tasks to users with higher reliability. However, they neglect the fact that a user may only have expertise on some problems (in some domains), but not others. Neglecting this expertise diversity may cause two problems: low estimation accuracy in truth analysis and ineffective task allocation. To address these problems, we propose an Expertise-aware Truth Analysis and Task Allocation (ETA
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
) approach, which can effectively infer user expertise and then allocate tasks and estimate truth based on the inferred expertise. ETA
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
 relies on a novel semantic analysis method to identify the expertise domains of the tasks and user expertise, an expertise-aware truth analysis solution to estimate truth and learn user expertise, and an expertise-aware task allocation method to maximize the probability that tasks are allocated to users with the right expertise while ensuring the work load does not exceed the processing capability at each user. Experimental results based on two real-world datasets demonstrate that ETA
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
 significantly outperforms existing solutions."
MELODY: A Long-Term Dynamic Quality-Aware Incentive Mechanism for Crowdsourcing.,"Crowdsourcing allows requesters to allocate tasks to a group of workers on the Internet to make use of their collective intelligence. Quality control is a key design objective in incentive mechanisms for crowdsourcing as requesters aim at obtaining answers of high quality under a given budget. However, when measuring workers' long-term quality, existing mechanisms either fail to utilize workers' historical information, or treat workers' quality as stable and ignore its temporal characteristics, hence performing poorly in a long run. In this paper we propose MELODY, a long-term dynamic quality-aware incentive mechanism for crowdsourcing. MELODY models interaction between requesters and workers as reverse auctions that run continuously. In each run of MELODY, we design a truthful, individual rational, budget feasible and quality-aware algorithm for task allocation with polynomial-time computation complexity and O(1) performance ratio. Moreover, taking into consideration the long-term characteristics of workers' quality, we propose a novel framework in MELODY for quality inference and parameters learning based on Linear Dynamical Systems at the end of each run, which takes full advantage of workers' historical information and predicts their quality accurately. Through extensive simulations, we demonstrate that MELODY outperforms existing work in terms of both quality estimation (reducing estimation error by 17.6% ~ 24.2%) and social performance (increasing requester's utility by 18.2% ~ 46.6%) in long-term scenarios."
The Strong Link Graph for Enhancing Sybil Defenses.,"The sybil problem is a fundamental problem in distributed systems and online social networks (OSNs). The basic problem is that an attacker can easily create multiple identities in a distributed or open online system. Popular and effective sybil defenses are usually based on properties of the network structure. However, most defenses assume that it is hard for the attacker to make many connections to honest users. However, this assumption can be invalid in real OSNs which decreases the effectiveness of many sybil defenses. We propose a graph transformation, the strong link graph, to mitigate such attacks by reducing the effect ofa large number of attack edges. Our preliminary experiments show indeed that when the attacker has many attack edges, existing algorithms such as SybilLimit, SybilRank and Gatekeeper are ineffective. After the strong link graph is applied, it deletes many of the attack edges, restoring the effectiveness of the sybil defenses."
Mechanism Design for Mobile Crowdsensing with Execution Uncertainty.,"Mobile crowdsensing has emerged as a promising paradigm for data collection due to increasingly pervasive and powerful mobile devices. There have been extensive research works that propose incentive mechanisms for crowdsensing, but they all make the assumption that mobile users will positively complete the allocated sensing tasks. In this paper, we consider a new and practical scenario of crowdsensing, where a user may fail to complete the task with a certain probability. It is an important and emerging issue for the incentive mechanisms to ensure fault tolerance for each sensing task under such unreliable scenarios. We design reverse auctions to model the strategic interaction between the platform and mobile users, in which users' probability of success and cost to perform the tasks are private information. Considering the task execution uncertainty, the goal of the auction mechanism is to minimize the social cost of user recruitment, while guaranteeing the tasks to be completed with high probability. We prove that minimizing the social cost is an NP-hard problem, and design computationally efficient mechanisms that achieve good approximation ratio and economic-robust properties, e.g., strategy-proofness. We conduct extensive simulations to evaluate the performance of our mechanisms based on a real data set. The evaluation results show that our mechanisms outperform the heuristic algorithm and approach to the optimal solution."
Towards Scalable and Dynamic Social Sensing Using A Distributed Computing Framework.,"With the rapid growth of online social media and ubiquitous Internet connectivity, social sensing has emerged as a new crowdsourcing application paradigm of collecting observations (often called claims) about the physical environment from humans or devices on their behalf. A fundamental problem in social sensing applications lies in effectively ascertaining the correctness of claims and the reliability of data sources without knowing either of them a priori, which is referred to as truth discovery. While significant progress has been made to solve the truth discovery problem, some important challenges have not been well addressed yet. First, existing truth discovery solutions did not fully solve the dynamic truth discovery problem where the ground truth of claims changes over time. Second, many current solutions are not scalable to large-scale social sensing events because of the centralized nature of their truth discovery algorithms. Third, the heterogeneity and unpredictability of the social sensing data traffic pose additional challenges to the resource allocation and system responsiveness. In this paper, we developed a Scalable Streaming Truth Discovery (SSTD) solution to address the above challenges. In this paper, we developed a Scalable Streaming Truth Discovery (SSTD) solution to address the above challenges. In particular, we first developed a dynamic truth discovery scheme based on Hidden Markov Models (HMM) to effectively infer the evolving truth of reported claims. We further developed a distributed framework to implement the dynamic truth discovery scheme using Work Queue in HTCondor system. We also integrated the SSTD scheme with an optimal workload allocation mechanism to dynamically allocate the resources (e.g., cores, memories) to the truth discovery tasks based on their computation requirements. We evaluated SSTD through real world social sensing applications using Twitter data feeds. The evaluation results on three real-world data traces (i.e., Boston Bombing, Paris Shooting and College Football) show that the SSTD scheme is scalable and outperforms the state-of-the-art truth discovery methods in terms of both effectiveness and efficiency."
Phoenix: A Constraint-Aware Scheduler for Heterogeneous Datacenters.,"Today's datacenters are increasingly becoming diverse with respect to both hardware and software architectures in order to support a myriad of applications. These applications are also heterogeneous in terms of job response times and resource requirements (eg., Number of Cores, GPUs, Network Speed) and they are expressed as task constraints. Constraints are used for ensuring task performance guarantees/Quality of Service(QoS) by enabling the application to express its specific resource requirements. While several schedulers have recently been proposed that aim to improve overall application and system performance, few of these schedulers consider resource constraints across tasks while making the scheduling decisions. Furthermore, latencycritical workloads and short-lived jobs that typically constitute about 90% of the total jobs in a datacenter have strict QoS requirements, which can be ensured by minimizing the tail latency through effective scheduling. In this paper, we propose Phoenix, a constraint-aware hybrid scheduler to address both these problems (constraint awareness and ensuring low tail latency) by minimizing the job response times at constrained workers. We use a novel Constraint Resource Vector (CRV) based scheduling, which in turn facilitates reordering of the jobs in a queue to minimize tail latency. We have used the publicly available Google traces to analyze their constraint characteristics and have embedded these constraints in Cloudera and Yahoo cluster traces for studying the impact of traces on system performance. Experiments with Google, Cloudera and Yahoo cluster traces across 15,000 worker node cluster shows that Phoenix improves the 99th percentile job response times on an average by 1.9× across all three traces when compared against a state-of-the-art hybrid scheduler. Further, in comparison to other distributed scheduler like Hawk, it improves the 90
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">th</sup>
 and 99
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">th</sup>
 percentile job response times by 4.5× and 5× respectively."
Dual Scaling VMs and Queries: Cost-Effective Latency Curtailment.,"Wimpy virtual instances equipped with small numbers of cores and RAM are popular public and private cloud offerings because of their low cost for hosting applications. The challenge is how to run latency-sensitive applications using such instances, which trade off performance for cost. In this study, we analytically and experimentally show that simultaneously scaling resources at coarse granularity and workloads, i.e., submitting multiple query clones to different servers, at fine granularity can overcome the performance disadvantages of wimpy VM instances and achieve stringent latency targets that are even lower than the average execution times of wimpy servers. To such an end, we first derive a closed-form analysis for the latency under any given VM provisioning and query replication level, considering cloning policies that can (not) terminate outstanding clones with (without) an overhead. Validated on trace-driven simulations, our analysis is able to accurately predict the latency and efficiently search for the optimal number of VMs and clones. Secondly, we develop a dual elastic scaler, DuoScale, that dynamically scales VMs and clones according to the workload dynamics so as to achieve the target latency in a cost-effective manner. The effectiveness of DuoScale lies on the observation that the application performance only scales sub-linearly with increasing vertical or horizontal resource provisioning, i.e., resources per VM or number of VMs. We evaluate DuoScale against VM-only scaling strategies via extensive trace-driven simulations as well as experimental results on a cloud test-bed. Our results show that DuoScale is able to achieve the stringent target latency by using clones on wimpy VMs with cost savings up to 50%, compared to scaling brawny VMs that have better performance at a higher unit cost."
A Framework for Enabling Security Services Collaboration Across Multiple Domains.,"Collaboration among Security Service Functions (SSF) is expected to become as essential to SECaaS (SECurity as a Service) systems as elasticity is to IaaS (Infrastructure as a Service). The virtualization opens new era in network security as new security appliances can be created on demand in appropriate places in the network. At the same time, the increasing size and diversity of attacks make it necessary to come up with new approaches for more efficient and more resilient security mechanisms. In this paper, we propose a new framework leveraging SDN (Software Defined Networking) and SFC (Service Function Chaining) to enhance the collaboration among different SSFs to mitigate large scale attacks. We describe a framework that allows SSFs from different domains to negotiate and dynamically control the amount of resources allocated for collaboration, in what we call a ""best-effort"" collaboration mode. This SSF collaboration framework creates a distributed mitigation system for handling large scale attacks in a dynamic and scalable manner. The efficiency and feasibility of this framework is experimentally assessed, showing that our approach incurs low overhead, increases the amount of traffic treated by SSFs and reduces the dropped traffic due to the lack of resources from the security mechanisms."
Group Clustering Using Inter-Group Dissimilarities.,"Various systems have natural groupings. For instance in large scale distributed system, we can have groups of virtual and/or physical devices. A system can also have groups of time series datasets collected at different time intervals. Such groups are usually characterized by multidimensional metrics (features) set. Clustering such groups using their multidimensional datasets has various applications, such as identifying different performance levels for anomaly detection and load balancing. Traditional algorithms focus on clustering a single time series dataset and not on such groups with multidimensional metrics datasets. In this paper, we present the design, implementation and analysis of two sets of group clustering algorithms. The first set is called one-to-many as it generates clusters of groups by comparing each group against all other groups. The second set of algorithms is called pairwise as it generates the clusters of groups using pairwise group dissimilarity matrix. Both sets of algorithms first generate group dissimilarity weights using metric ranking algorithms. We implemented the group clustering algorithms by extending a well known machine learning package and using a front-end visualizer.We validated the clustering algorithms using real world datasets on the VMware vSAN product. Experimental results show that 7 out of the 8 proposed algorithms can generate expected clusters in at least 4 out of the 6 detailed experiments. In 5 out of the 6 experiments, 3 out of the 8 proposed algorithms can generate the expected clusters. One of the pairwise algorithms can generate the expected clusters in all 6 of the 6 experiments."
Comprehensive Measurement and Analysis of the User-Perceived I/O Performance in a Production Leadership-Class Storage System.,"With the increase of the scale and intensity of the parallel I/O workloads generated by those scientific applications running on high performance computing facilities, understanding the I/O dynamics, especially the root cause of the I/O performance variability and degradation in HPC environment, have become extremely critical to the HPC community. In this paper, we run extensive I/O measuring tests on a production leadership-class storage system to capture the performance variabilities of large-scale parallel I/O. Analyzing these results and its statistic correlation revealed some valuable insights into the characteristics of the storage system and the root cause of I/O performance variability. Further, we leverage these findings and propose an I/O middleware design refactoring which can improve the performance of the parallel I/O by optimizing the data striping and placement. Our preliminary evaluation results demonstrate the proposed approach can reduce the average per-process write latency by at least 80% and the maximum per-process write latency by at least 20%."
On the Limits of Subsampling of Location Traces.,"Location data collection at a societal scale is increasingly becoming common - examples of this are call and data detail records in telecommunication companies, GPS samples collected by car companies, and GPS samples from mobile devices in mapping companies (e.g., Google, Microsoft). Such large scale mobility datasets have applications in urban planning, network planning, surveillance, and real-time traffic estimations. This paper addresses the problem of subsampling location traces while preserving the amount of information present in such datasets. We present a novel subsampling technique that is based on a hierarchical geographical encoding mechanism (geohash), that allows for efficient spatial cluster sampling. We analyze this subsampling technique through various information theoretic measures to quantify the total ""amount"" of information in a dataset from a location trace perspective and evaluate these metrics in the context of two large scale mobility datasets from telecommunication companies - one is that of call detail records and the second is that of data detail records. We show that subsampling data in both these cases by as much as 75% does not significantly reduce the total amount of information, i.e. the dataset can be used similar to the original version. This paves way for the creation of better space and CPU efficient models that can support various applications reliant on collective location traces."
SOM-TC: Self-Organizing Map for Hierarchical Trajectory Clustering.,"Trajectory clustering techniques help discover interesting insights from moving object data, including common routes for people and vehicles, anomalous sub-trajectories, etc. Existing trajectory clustering techniques fail to take in to account the uncertainty present in location data. In this paper, we investigate the problem of clustering trajectory data and propose a novel algorithm for clustering similar full and sub-trajectories together while modeling uncertainty in this data. We describe the necessary pre-processing techniques for clustering trajectory data, namely techniques to discretize raw location data using Possible World semantics to capture the inherent uncertainty in location data, and to segment full trajectories in to meaningful sub-trajectories. As a baseline, we extend the well known K-means algorithm to cluster trajectory data. We then describe and evaluate a new trajectory clustering algorithm, SOM-TC (Self-Organizing Map Based Trajectory Clustering), that is inspired from the self-organizing map technique and is at least 4x faster than the baseline K-means and current density based clustering approaches."
Processing Encrypted and Compressed Time Series Data.,"Numerous applications, e.g., in the industrial sector, produce large amounts of time-series data, which must be stored and made available for distributed processing. While outsourcing data storage and processing to third-party service providers offers many benefits, it raises data privacy issues. In light of this problem, techniques have been proposed to share only encrypted data with the remote service provider, yet the capability to run meaningful queries over the data is preserved. However, timeseries data is typically compressed at the server to save space, which is not easily possible when dealing with encrypted data. Moreover, data must be compressed in such a way that queries can still be executed efficiently. As a first step in this direction, we present an approach that preserves data privacy, enables compression at the server, and supports querying of the stored data. Our evaluation using realworld time-series data shows that our compression mechanism can reduce the required space drastically. Moreover, the median running time of all considered queries increases marginally, implying that compression can be introduced without sacrificing performance of query execution."
Calvin Constrained - A Framework for IoT Applications in Heterogeneous Environments.,"Calvin is an IoT framework for application development, deployment and execution in heterogeneous environments, that includes clouds, edge resources, and embedded or constrained resources. Inside Calvin, all the distributed resources are viewed as one environment by the application. The framework provides multi-tenancy and simplifies development of IoT applications, which are represented using a dataflow of application components (named actors) and their communication. The idea behind Calvin poses similarity with the serverless architecture and can be seen as Actor as a Service instead of Function as a Service. This makes Calvin very powerful as it does not only scale actors quickly but also provides an easy actor migration capability. In this work, we propose Calvin Constrained, an extension to the Calvin framework to cover resource-constrained devices. Due to limited memory and processing power of embedded devices, the constrained side of the framework can only support a limited subset of the Calvin features. The current implementation of Calvin Constrained supports actors implemented in C as well as Python, where the support for Python actors is enabled by using MicroPython as a statically allocated library, by this we enable the automatic management of state variables and enhance code re-usability. As would be expected, Python-coded actors demand more resources over C-coded ones. We show that the extra resources needed are manageable on current off-the-shelve micro-controller-equipped devices when using the Calvin framework."
Privacy Preserving User-Based Recommender System.,"With the rapid development of the social networks, Collaborative Filtering (CF)-based recommender systems have been increasingly prevalent and become widely accepted by users. The CF-based techniques generate recommendations by collecting privacy sensitive data from users. Usually, the users are sensitive to disclosure of personal information and, consequently, there are unavoidable security concerns since private information can be easily misused by malicious third parties. In order to protect against breaches of personal information, it is necessary to obfuscate user information by means of an efficient encryption technique while simultaneously generating the recommendation by making true information inaccessible to service providers. Therefore, we propose a privacy preserving user-based CF technique based on homomorphic encryption, which is capable of determining similarities among users followed by generating recommendations without revealing any private information. We introduce different semi-honest parties to preserve privacy and to carry out intermediate computations for generating recommendations. We implement our method on publicly available datasets and show that our method is practical as well as achieves high level of security for users without compromising the recommendation accuracy."
Privacy Preserving Optimization of Participatory Sensing in Mobile Cloud Computing.,"With the rapid growth of mobile cloud computing, participatory sensing emerges as a new paradigm to explore our physical world at an unprecedented ne granularity by recruiting the pervasive sensor-enabled smart phones. While extensive optimization has been performed in the literature to coordinate the sensing activity of the cloud-based sensing server (or platform) and the participating smart phones so as to maximize the efficiency of participatory sensing, the privacy issue in the optimization has been largely overlooked. In this paper, we propose a novel privacy-preserving optimization framework that allows both the cloud-based platform and mobile users to share data for the formulation and solution of the optimization, but without revealing sensitive information that may lead to privacy leakage of each other. Our method is built upon a privacy-preserving version of the well-known NP-hard weighted set-coverage problem. To accommodate privacy requirements in this framework, our solution uses a modified bloom filter along with a Dife-Hellman-type exchange protocol among all participants for data aggregation, sharing, and presentation. Through extensive simulation we evaluate the privacy strength of the proposed approach and also verify its effectiveness and low overhead."
SPHINX: A Password Store that Perfectly Hides Passwords from Itself.,"Password managers (aka stores or vaults) allow a user to store and retrieve (usually high-entropy) passwords for her multiple password-protected services by interacting with a “device” serving the role of the manager (e.g., a smartphone or an online third-party service) on the basis of a single memorable (low-entropy) master password. Existing password managers work well to defeat offline dictionary attacks upon web service compromise, assuming the use of high-entropy passwords is enforced. However, they are vulnerable to leakage of all passwords in the event the device is compromised, due to the need to store the passwords encrypted under the master password and/or the need to input the master password to the device (as in smartphone managers). Evidence exists that password managers can be attractive attack targets. In this paper, we introduce a novel approach to password management, called SPHINX, which remains secure even when the password manager itself has been compromised. In SPHINX, the information stored on the device is information theoretically independent of the user's master password - an attacker breaking into the device learns no information about the master password or the user's site-specific passwords. Moreover, an attacker with full control of the device, even at the time the user interacts with it, learns nothing about the master password - the password is not entered into the device in plaintext form or in any other way that may leak information on it. Unlike existing managers, SPHINX produces strictly high-entropy passwords and makes it compulsory for the users to register these randomized passwords with the web services, hence fully defeating offline dictionary attack upon service compromise. The design and security of SPHINX is based on the device-enhanced PAKE model of Jarecki et al. that provides the theoretical basis for this construction and is backed by rigorous cryptographic proofs of security. While SPHINX is suitable for different device and online platforms, in this paper, we report on its concrete instantiation on smartphones given their popularity and trustworthiness as password managers (or even two-factor authentication). We present the design, implementation and performance evaluation of SPHINX, offering prototype browser plugins, smartphone apps and transparent device-client communication. Based on our inspection analysis, the overall user experience of SPHINX improves upon current managers. We also report on a lab-based usability study of SPHINX, which indicates that users' perception of SPHINX security and usability is high and satisfactory when compared to regular password-based authentication. Finally, we discuss how SPHINX may be extended to an online service for the purpose of back-up or as an independent password manager."
When Smart TV Meets CRN: Privacy-Preserving Fine-Grained Spectrum Access.,"Dynamic spectrum sharing techniques applied in the UHF TV band have been developed to allow secondary WiFi transmission in areas with active TV users. This technique of dynamically controlling the exclusion zone enables vastly increasing secondary spectrum re-use, compared to the ""TV white space"" model where TV transmitters determine the exclusion zone and only ""idle"" channels can be re-purposed. However, in current such dynamic spectrum sharing systems, the sensitive operation parameters of both primary TV users (PUs) and secondary users (SUs) need to be shared with the spectrum database controller (SDC) for the purpose of realizing efficient spectrum allocation. Since such SDC server is not necessarily operated by a trusted third party, those current systems might cause essential threatens to the privacy requirement from both PUs and SUs. To address this privacy issue, this paper proposes a privacy-preserving spectrum sharing system between PUs and SUs, which realizes the spectrum allocation decision process using efficient multi-party computation (MPC) technique. In this design, the SDC only performs secure computation over encrypted input from PUs and SUs such that none of the PU or SU operation parameters will be revealed to SDC. The evaluation of its performance illustrates that our proposed system based on efficient MPC techniques can perform dynamic spectrum allocation process between PUs and SUs efficiently while preserving users' privacy."
Revisiting Security Risks of Asymmetric Scalar Product Preserving Encryption and Its Variants.,"Cloud computing has emerged as a compelling vision for managing data and delivering query answering capability over the internet. This new way of computing also poses a real risk of disclosing confidential information to the cloud. Searchable encryption addresses this issue by allowing the cloud to compute the answer to a query based on the cipher texts of data and queries. Thanks to its inner product preservation property, the asymmetric scalar-product-preserving encryption (ASPE) has been adopted and enhanced in a growing number of works toperform a variety of queries and tasks in the cloud computingsetting. However, the security property of ASPE and its enhancedschemes has not been studied carefully. In this paper, we show acomplete disclosure of ASPE and several previously unknownsecurity risks of its enhanced schemes. Meanwhile, efficientalgorithms are proposed to learn the plaintext of data and queriesencrypted by these schemes with little or no knowledge beyondthe ciphertexts. We demonstrate these risks on real data sets."
An Adversary-Centric Behavior Modeling of DDoS Attacks.,"Distributed Denial of Service (DDoS) attacks are some of the most persistent threats on the Internet today. The evolution of DDoS attacks calls for an in-depth analysis of those attacks. A better understanding of the attackers' behavior can provide insights to unveil patterns and strategies utilized by attackers. The prior art on the attackers' behavior analysis often falls in two aspects: it assumes that adversaries are static, and makes certain simplifying assumptions on their behavior, which often are not supported by real attack data. In this paper, we take a data-driven approach to designing and validating three DDoS attack models from temporal (e.g., attack magnitudes), spatial (e.g., attacker origin), and spatiotemporal (e.g., attack inter-launching time) perspectives. We design these models based on the analysis of traces consisting of more than 50,000 verified DDoS attacks from industrial mitigation operations. Each model is also validated by testing its effectiveness in accurately predicting future DDoS attacks. Comparisons against simple intuitive models further show that our models can more accurately capture the essential features of DDoS attacks."
Anti-Malicious Crowdsourcing Using the Zero-Determinant Strategy.,"Crowdsourcing is a promising paradigm to accomplish a complex task via eliciting services from a large group of contributors. However, recent observations indicate that the success of crowdsourcing is being threatened by the malicious behaviors of the contributors. In this paper, we analyze the malicious attack problem using an iterated prisoner's dilemma (IPD) game and propose a zero-determinant (ZD) strategy based scheme by rewarding a worker's cooperation or penalizing the defection for enticing his final cooperation. Both theoretical analysis and simulation study indicate that the proposed algorithm has two attractive characteristics: 1) the requestor can incentivize the worker to keep on cooperating by only increasing the short-term payment; and 2) the proposed algorithm is fair, so the requestor cannot arbitrarily penalize an innocent worker to increase her payoff even though she can dominate the game. To the best of our knowledge, we are the first to use the ZD strategy to stimulate both players to cooperate in an IPD game. Moreover, our proposed algorithm is not restricted to solve the problem of the malicious crowdsourcing - it can be employed to tackle any problem that can be formulated by an IPD game."
JPR: Exploring Joint Partitioning and Replication for Traffic Minimization in Online Social Networks.,"A scalable storage system becomes more important today for online social networks (OSNs) as the volume of user data increases rapidly. Key-value store uses consistent hashing to save data in a distributed manner. As a defacto standard, it has been widely used in production environments of many OSNs. However, the random nature of hashing always leads to high inter-server traffic. Recently, partitioning and replication are respectively proposed in many existing works where the former aims to minimize the inter-server read traffic and the latter aims to optimize the inter-server write traffic. Nevertheless, the separated manners of optimization cannot efficiently reduce the traffic. Because the inter-server read traffic is changed during replication. In this paper, we suggest that performing partitioning and replication simultaneously could provide probability to further optimize traffic. Then we formulate the problem as a revised graph partitioning with overlaps, since overlaps partitioning naturally corresponds to replication. To solve the problem, we propose a Joint Partitioning and Replication (JPR) scheme. Through extensive experiments with a real world Facebook trace, we evaluate that JPR significantly reduces inter-server traffic with slightly sacrificing storage cost compared to hashing, and preserves a good load balancing across servers as well."
Optimizing Source Selection in Social Sensing in the Presence of Influence Graphs.,"This paper addresses the problem of choosing the right sources to solicit data from in sensing applications involving broadcast channels, such as those crowdsensing applications where sources share their observations on social media. The goal is to select sources such that expected fusion error is minimized. We assume that soliciting data from a source incurs a cost and that the cost budget is limited. Contrary to other formulations of this problem, we focus on the case where some sources influence others. Hence, asking a source to make a claim affects the behavior of other sources as well, according to an influence model. The paper makes two contributions. First, we develop an analytic model for estimating expected fusion error, given a particular influence graph and solution to the source selection problem. Second, we use that model to search for a solution that minimizes expected fusion error, formulating it as a zero-one integer non-linear programming (INLP) problem. To scale the approach, the paper further proposes a novel reliability-based pruning heuristic (RPH) and a similarity-based lossy estimation (SLE) algorithm that significantly reduce the complexity of the INLP algorithm at the cost of a modest approximation. The analytically computed expected fusion error is validated using both simulations and real-world data from Twitter, demonstrating a good match between analytic predictions and empirical measurements. It is also shown that our method outperforms baselines in terms of resulting fusion error."
Dynamic Contract Design for Heterogenous Workers in Crowdsourcing for Quality Control.,"Crowdsourcing sites heavily rely on paid workers to ensure completion of tasks. Yet, designing a pricing strategies able to incentivize users' quality and retention is non trivial. Existing payment strategies either simply set a fixed payment per task without considering changes in workers' behaviors, or rule out poor quality responses and workers based on coarse criteria. Hence, task requesters may be investing significantly in work that is inaccurate or even misleading. In this paper, we design a dynamic contract to incentivize high-quality work. Our proposed approach offers a theoretically proven algorithm to calculate the contract for each worker in a cost-efficient manner. In contrast to existing work, our contract design is not only adaptive to changes in workers' behavior, but also adjusts pricing policy in the presence of malicious behavior. Both theoretical and experimental analysis over real Amazon review traces show that our contract design can achieve a near optimal solution. Furthermore, experimental results demonstrate that our contract design 1) can promote high-quality work and prevent malicious behavior, and 2) outperforms the intuitive strategy of excluding all malicious workers in terms of the requester's utility."
Joint Request Balancing and Content Aggregation in Crowdsourced CDN.,"Recent years have witnessed a new content delivery paradigm named crowdsourced CDN, in which devices deployed at edge network can prefetch contents and provide content delivery service. Crowdsourced CDN offers high-quality experience to end-users by reducing their content access latency and alleviates the load of network backbone by making use of network and storage resources at millions of edge devices. In such paradigm, redirecting content requests to proper devices is critical for user experience. The uniqueness of request redirection in such crowdsourced CDN lies that: on one hand, the bandwidth capacity of the crowdsourced CDN devices is limit, hence devices located at a crowded place can be easily overwhelmed when serving nearby user requests; on the other hand, contents requested in one device can be significantly different from another one, making request redirection strategies used in conventional CDNs which only aim to balance request loads ineffective. In this paper, we explore request redirection strategies that take both workload balance of devices and content requested by users into consideration. Our contributions are as follows. First, we conduct measurement studies, coving 1.8M users watching 0.4M videos, to understand request patterns in crowdsourced CDN. We observe that the loads of nearby devices can be very different and the contents requested at nearby devices can also be significantly different. These observations lead to our design for request balancing at nearby devices. Second, we formulate the request redirection problem by taking both the content access latency and the content replication cost into consideration, and propose a request balancing and content aggregation solution. Finally, we evaluate the performance of our design using trace-driven simulations, and observe our scheme outperforms the traditional strategy in terms of many metrics, e.g., we observe a content access latency reduction by 50% over traditional mechanisms such as the Nearest/Random request routing scheme."
Shrink: A Breast Cancer Risk Assessment Model Based on Medical Social Network.,"Breast cancer risk assessment model can assess whether a people is at a high risk of developing breast cancer disease or not and confirm a breast cancer high-risk group. Because the etiology of breast cancer disease is different in different country and region, the existing risk assessment model is only adaptive to certain countries and regions. And the parameters of these models are fixed, so these models have poor generality. Aiming at these problems, the paper puts forward a new breast cancer risk assessment model named as Shrink. Using the idea of social network, Shrink constructs a medical social network to show the similarity among people, and uses group division algorithm to divide the network into breast cancer high-risk group and low-risk group. The parameters of this model can be set according to the needs of the breast census, and these parameters can be directly acquired through questionnaire, therefore Shrink has good generality. Moreover, under the uncertain classification standard, Shrink adopts a new classification method to discover breast cancer high-risk group. Based on the real data from questionnaires, we make experiments in Matlab, and obtain the evaluation index of the model. The experiment proves that the model itself has good evaluation result and is better than classic Gail model."
Opportunistic Energy Sharing Between Power Grid and Electric Vehicles: A Game Theory-Based Pricing Policy.,"Electric vehicles (EVs) have great potential to reduce dependency on fossil fuels. The recent surge in the development of online EV (OLEV) will help to address the drawbacks associated with current generation EVs, such as the heavy and expensive batteries. OLEVs are integrated with the smart grid of power infrastructure through a wireless power transfer system (WPT) to increase the driving range of the OLEV. However, the integration of OLEVs with the grid creates a tremendous load for the smart grid. The demand of a power grid changes over time and the price of power is not fixed throughout the day. There should be some congestion avoidance and load balancing policy implications to ensure quality of services for OLEVs. In this paper, first, we conduct an analysis to show the existence of unpredictable power load and congestion because of OLEVs. We use the Simulation for Urban MObility tool and hourly traffic counts of a road section of the New York City to analyze the amount of energy OLEVs can receive at different times of the day. Then, we present a game theory based on a distributed power schedule framework to find the optimal schedule between OLEVs and smart grid. In the proposed framework, OLEVs receive the amount of power charging from the smart grid based on a power payment function which is updated using best response strategy. We prove that the updated power requests converge to the optimal power schedule. In this way, the smart grid maximizes the social welfare of OLEVs, which is defined as mixed consideration of total satisfaction and its power charging cost. Finally, we verify the performance of our proposed pricing policy under different scenarios in a simulation study."
Energy Efficient Object Detection in Camera Sensor Networks.,"A wireless camera network can provide situation awareness information (e.g., humans in distress) in scenarios such as disaster recovery. If such camera sensors are battery operated, sending raw video feeds back to a central controller can be expensive in terms of energy consumption. Further, if all cameras were to use the optimal processing algorithm for object decision, they may also expend unnecessary energy. Stated otherwise, cameras that capture the same objects may not all have to use the optimal algorithm to achieve a desired accuracy, and this can save processing energy costs. In this paper, our objective is to design and implement a framework that can support coordination among cameras to deliver highly accurate detection of objects in an energy efficient way. The framework, which we call EECS (for energy efficient camera sensors), estimates the detection accuracy and energy costs incurred (both the processing and communication costs are taken into account) with each detection algorithm for each camera, and comes up with a choice of cameras for sending information pertaining to the object of interest. This set of cameras and the video processing algorithms that they must use, are chosen so as to minimize the energy expenditures, given a desired detection accuracy. We implement EECS on a camera network built with smartphones, and demonstrate that it reduces the energy consumption by up to 40% while ensuring a object detection accuracy of over 86%."
DeepOpp: Context-Aware Mobile Access to Social Media Content on Underground Metro Systems.,"Accessing online social media content on underground metro systems is a challenge due to the fact that passengers often lose connectivity for large parts of their commute. As the oldest metro system in the world, the London underground represents a typical transportation network with intermittent Internet connectivity. To deal with disruption in connectivity along the sub-surface and deep-level underground lines on the London underground, we have designed a context-aware mobile system called DeepOpp that enables efficient offline access to online social media by prefetching and caching content opportunistically when signal availability is detected. DeepOpp can measure, crowdsource and predict signal characteristics such as strength, bandwidth and latency; it can use these predictions of mobile network signal to activate prefetching, and then employ an optimization routine to determine which social content should be cached in the system given real-time network conditions and device capacities. DeepOpp has been implemented as an Android application and tested on the London Underground; it shows significant improvement over existing approaches, e.g. reducing the amount of power needed to prefetch social media items by 2.5 times. While we use the London Underground to test our system, it is equally applicable in New York, Paris, Madrid, Shanghai, or any other urban underground metro system, or indeed in any situation in which users experience long breaks in connectivity."
PhaseBeat: Exploiting CSI Phase Data for Vital Sign Monitoring with Commodity WiFi Devices.,"Vital signs, such as respiration and heartbeat, are useful to health monitoring since such signals provide important clues of medical conditions. Effective solutions are needed to provide contact-free, easy deployment, low-cost, and long-term vital sign monitoring. In this paper, we present PhaseBeat to exploit channel state information (CSI) phase difference data to monitor breathing and heartbeat with commodity WiFi devices. We provide a rigorous analysis of the CSI phase difference data with respect to its stability and periodicity. Based on the analysis, we design and implement the PhaseBeat system with off-the-shelf WiFi devices, and conduct an extensive experimental study to validate its performance. Our experimental results demonstrate the superior performance of PhaseBeat over existing approaches in various indoor environments."
REX: Rapid Ensemble Classification System for Landslide Detection Using Social Media.,"We study the problem of using Social Media to detect natural disasters, of which we are interested in a special kind, namely landslides. Employing information from Social Media presents unique research challenges, as there exists a considerable amount of noise due to multiple meanings of the search keywords, such as ""landslide"" and ""mudslide"". To tackle these challenges, we propose REX, a rapid ensemble classification system which can filter out noisy information by implementing two key ideas: (I) a new method for constructing independent classifiers that can be used for rapid ensemble classification of Social Media texts, where each classifier is built using randomized Explicit Semantic Analysis; and (II) a self-correction approach which takes advantage of the observation that the majority label assigned to Social Media texts belonging to a large event is highly accurate. We perform experiments using real data from Twitter over 1.5 years to show that REX classification achieves 0.98 in F-measure, which outperforms the standard Bag-of-Words algorithm by an average of 0.14 and the state-of-the-art Word2Vec algorithm by 0.04. We also release the annotated datasets used in the experiments as a contribution to the research community containing 282k labeled items."
Toward An Integrated Approach to Localizing Failures in Community Water Networks.,"We present a cyber-physical-human distributed computing framework, AquaSCALE, for gathering, analyzing and localizing anomalous operations of increasingly failure-prone community water services. Today, detection of pipe breaks/leaks in water networks takes hours to days. AquaSCALE leverages dynamic data from multiple information sources including IoT (Internet of Things) sensing data, geophysical data, human input, and simulation/modeling engines to create a sensor-simulation-data integration platform that can accurately and quickly identify vul-nerable spots. We propose a two-phase workflow that begins with robust simulation methods using a commercial grade hydraulic simulator - EPANET, enhanced with the support for IoT sensor and pipe failure modelings. It generates a profile of anomalous events using diverse plug-and-play machine learning techniques. The profile then incorporates with external observations (NOAA weather reports and twitter feeds) to rapidly and reliably isolate broken water pipes. We evaluate the two-phase mechanism in canonical and real-world water networks under different failure scenarios. Our results indicate that the proposed approach with offline learning and online inference can locate multiple simultaneous pipe failures at fine level of granularity (individual pipeline level) with high level of accuracy with detection time reduced by orders of magnitude (from hours/days to minutes)."
MobiQoR: Pushing the Envelope of Mobile Edge Computing Via Quality-of-Result Optimization.,"Mobile edge computing aims at improving application response time and energy efficiency by deploying data processing at the edge of the network. Due to the proliferation of Internet of Things and interactive applications, the ever-increasing demand for low latency calls for novel approaches to further pushing the envelope of mobile edge computing beyond existing task offloading and distributed processing mechanisms. In this paper, we identify a new tradeoff between Quality-of-Result (QoR) and service response time in mobile edge computing. Our key idea is motivated by the observation that a growing set of edge applications involving media processing, machine learning, and data mining can tolerate some level of quality loss in the computed result. By relaxing the need for highest QoR, significant improvement in service response time can be achieved. Toward this end, we present a novel optimization framework, MobiQoR, which minimizes service response time and app energy consumption by jointly optimizing the QoR of all edge nodes and the offloading strategy. The proposed MobiQoR is prototyped using Parse, an open source mobile back-end tool, on Android smartphones. Using representative applications including face recognition and movie recommendation, our evaluation with real-world datasets shows that MobiQoR reduces response time and energy consumption by up to 77% (in face recognition) and 189.3% (in movie recommendation) over existing strategies under the same level of QoR relaxation."
Truthful Auctions for User Data Allowance Trading in Mobile Networks.,"User data allowance trading emerges as a promising practice in mobile data networks since it can help mobile networks to attract more users. However, to date, there is no study on user data allowance trading in mobile networks. In this paper, we develop a truthful framework that allows users to bid for data allowance. We focus on preventing price cheating, guaranteeing fairness, and minimizing trading maintenance cost in trading. We formulate the data trading process as a double auction problem and develop algorithms to solve the problem. In particular, we use a uniform price auction based on a competitive equilibrium to defend against price cheating and provide fair-ness. Meanwhile, we leverage linear programming to minimize trading maintenance cost. We conduct extensive simulations to demonstrate the performance of the proposed mechanism. The simulation results show that our trading mechanism is truthful and fair, while incurring a minimized maintenance cost."
Online Resource Allocation for Arbitrary User Mobility in Distributed Edge Clouds.,"As clouds move to the network edge to facilitate mobile applications, edge cloud providers are facing new challenges on resource allocation. As users may move and resource prices may vary arbitrarily, %and service delays are heterogeneous, resources in edge clouds must be allocated and adapted continuously in order to accommodate such dynamics. In this paper, we first formulate this problem with a comprehensive model that captures the key challenges, then introduce a gap-preserving transformation of the problem, and propose a novel online algorithm that optimally solves a series of subproblems with a carefully designed logarithmic objective, finally producing feasible solutions for edge cloud resource allocation over time. We further prove via rigorous analysis that our online algorithm can provide a parameterized competitive ratio, without requiring any a priori knowledge on either the resource price or the user mobility. Through extensive experiments with both real-world and synthetic data, we further confirm the effectiveness of the proposed algorithm. We show that the proposed algorithm achieves near-optimal results with an empirical competitive ratio of about 1.1, reduces the total cost by up to 4x compared to static approaches, and outperforms the online greedy one-shot optimizations by up to 70%."
Leveraging Target k-Coverage in Wireless Rechargeable Sensor Networks.,"Energy remains a major hurdle in running computation-intensive tasks on wireless sensors. Recent efforts have been made to employ a Mobile Charger (MC) to deliver wireless power to sensors, which provides a promising solution to the energy problem. Most of previous works in this area aim at maintaining perpetual network operation at the expense of high operating cost of MC. In the meanwhile, it is observed that due to low cost of wireless sensors, they are usually deployed at high density so there is abundant redundancy in their coverage in the network. For such networks, it is possible to take advantage of the redundancy to reduce the energy cost. In this paper, we relax the strictness of perpetual operation by allowing some sensors to temporarily run out of energy while still maintaining target k-coverage in the network at lower cost of MC. We first establish a theoretical model to analyze the performance improvements under this new strategy. Then we organize sensors into load-balanced clusters for target monitoring by a distributed algorithm. Next, we propose a charging algorithm named λ-GTSP Charging Algorithm to determine the optimal number of sensors to be charged in each cluster to maintain k-coverage in the network and derive the route for MC to charge them. We further generalize the algorithm to encompass mobile targets as well. Our extensive simulation results demonstrate significant improvements of network scalability and cost saving that MC can extend charging capability over 2-3 times with a reduction of 40% of moving cost without sacrificing the network performance."
Reducing Cellular Signaling Traffic for Heartbeat Messages via Energy-Efficient D2D Forwarding.,"Mobile Instant Messaging (IM) apps, such as WhatsApp and WeChat, frequently send heartbeat messages to remote servers to maintain always-online status. Periodic heartbeat messages are small in size, but their transmissions incur heavy signaling traffic to frequently establish and release communication channels between base stations and smartphones, known as signaling storm. Meanwhile, smartphones also need to activate cellular data communication module frequently for transmitting short heartbeat messages, resulting in substantial energy consumption. To address these issues, we propose a Device-to-Device (D2D) based heartbeat relaying framework, in order to reduce signaling traffic and energy consumption in heartbeat transmission. The framework selects the smartphones as relays to opportunistically collect heartbeat messages from nearby smartphones using energy-efficient D2D communication. The collected heartbeat messages are transmitted to the BS in an aggregated manner to reduce cellular signaling traffic. Based on the periods and the expiration time of the collected heartbeat messages, the framework schedules the transmissions of collected heartbeat messages to minimize signaling and energy consumption while satisfying time constrains. We implement and evaluate our solution on Android smartphones. The results from real-world experiments show that our solution achieves more than 50% signaling traffic reduction and up to 36% energy saving."
k-Protected Routing Protocol in Multi-hop Cognitive Radio Networks.,"In cognitive radio networks (CRNs), the established communication sessions between secondary users (SUs) may be affected or even get interrupted because the SUs need to relinquish the spectrum when the licensed users (PUs) appear and reclaim the spectrum/channel. On detecting PU activities, the SUs on the affected links either switch to another available idle spectrum using the same link or the SUs seek for an alternative path/link. In either approach, the ongoing session is destined to experience delay or even gets interrupted, which is intolerable to quality of service-sensitive applications such as multimedia streaming or audio/video conferencing. In this paper, we study the problem of establishing k-protected routes in CRNs. A k-protected route consists of a set of main links with preassigned backup spectrum and backup paths and is guaranteed to sustain from k PU appearances without being interrupted. For a CRN, we find a k-protected route for each session request and maximize the number of sessions that can be supported. We propose both centralized and distributed k-protected routing algorithms for this problem. Simulation results show that our k-protected routing protocol outperforms existing opportunistic spectrum switching approaches in terms of delay and interruption rate."
Multi-resource Load Balancing for Virtual Network Functions.,"Middleboxes are widely deployed to perform various network functions to ensure security and improve performance. The recent trend of Network Function Virtualization (NFV) makes it easy for operators to deploy software implementations of these network functions on commodity servers. However, virtual network functions consume different amounts of resources when processing packets. Thus a multi-resource load balancing (MRLB) mechanism is needed to efficiently utilize server resources. MRLB problem in the context of NFV is fundamentally different from multi-resource allocation problems, as well as traditional single-resource load balancing and multi-resource load balancing problems in task scheduling. In this paper, we tackle the MRLB problem in NFV by first proposing dominant load-the load of the most stressed resource on a server-as the load balancing metric. We then formulate the MRLB problem as an optimization to minimize the maximum dominant load of all NFV servers given the demand. Based on proximal Jacobian ADMM, we propose an efficient algorithm to solve the problem in large scale settings. Through extensive trace-driven simulations and prototype experiments on a testbed, we show that our MRLB algorithm with dominant load performs significantly better and faster than benchmarking algorithms."
"Learning from Failure Across Multiple Clusters: A Trace-Driven Approach to Understanding, Predicting, and Mitigating Job Terminations.","In large-scale computing platforms, jobs are prone to interruptions and premature terminations, limiting their usability and leading to significant waste in cluster resources. In this paper, we tackle this problem in three steps. First, we provide a comprehensive study based on log data from multiple large-scale production systems to identify patterns in the behaviour of unsuccessful jobs across different clusters and investigate possible root causes behind job termination. Our results reveal several interesting properties that distinguish unsuccessful jobs from others, particularly w.r.t. resource consumption patterns and job configuration settings. Secondly, we design a machine learning-based framework for predicting job and task terminations. We show that job failures can be predicted relatively early with high precision and recall, and also identify attributes that have strong predictive power of job failure. Finally, we demonstrate in a concrete use case how our prediction framework can be used to mitigate the effect of unsuccessful execution using an effective task-cloning policy that we propose."
RBAY: A Scalable and Extensible Information Plane for Federating Distributed Datacenter Resources.,"While many institutions, whether industrial, academic, or governmental, satisfy their computing needs through public cloud providers, many others still manage their own resources, often as geographically distributed datacenters. Spare capacity from these geographically distributed datacenters could be offered to others, provided there were a mechanism to discover, and then request these resources. Unfortunately, single datacenter administrators tend not to cooperate due to issues of scalability, diverse administrative policies, and site-specific monitoring infrastructure. This paper describes RBAY, an integrated information plane that enables secure and scalable sharing between geographically distributed datacenters. RBAY's key design features are twofold. First, RBAY employs a decentralized `hierarchical aggregation tree' structure to seamlessly aggregate spare resources from geographically distributed datacenters to a global information plane. Second, RBAY attaches to each participating server a `admin-customized' handler, which follows site-specific policy to expose, hide, add, remove resources to RBAY, and thus fulfill the task of `which resource to expose to whom, when, and how'. An experimental evaluation on eight real-world geo-distributed sites demonstrates RBAY's rapid response to composite queries, as well as its extensible, scalable, and lightweight nature."
Task-aware TCP in Data Center Networks.,"In modern data centers, many flow-based and task-based schemes have been proposed to speed up the data transmission in order to provide fast, reliable services for millions of users. However, existing flow-based schemes treat all flows in isolation, contributing less to or even hurting user experience due to the stalled flows. Other prevalent task-based approaches, such as centralized and decentralized scheduling, are sophisticated or unable to share task information. In this work, we first reveal that relinquishing bandwidth of leading flows to the stalled ones effectively reduces the task completion time. We further present the design and implementation of a general supporting scheme that shares the flow-tardiness information through a receiver-driven coordination. Our scheme can be flexibly and widely integrated with the state-of-the-art TCP protocols designed for data centers, while making no modification on switches. Through the testbed experiments and simulations of typical data center applications, we show that our scheme reduces the task completion time by 70% and 50% compared with the flow-based protocols (e.g. DCTCP, L2DCT) and task-based scheduling (e.g. Baraat), respectively. Moreover, our scheme also outperforms other approaches by 18% to 25% in prevalent topologies of data center."
Limitations of Load Balancing Mechanisms for N-Tier Systems in the Presence of Millibottlenecks.,"The scalability of n-tier systems relies on effective load balancing to distribute load among the servers of the same tier. We found that load balancing mechanisms (and some policies) in servers used in typical n-tier systems (e.g., Apache and Tomcat) have issues of instability when very long response time (VLRT) requests appear due to millibottlenecks, very short bottlenecks that last only tens to hundreds of milliseconds. Experiments with standard n-tier benchmarks show that during millibottlenecks, some load balancing policy/mechanism combinations make the mistake of sending new requests to the node(s) suffering from millibottlenecks, instead of the idle nodes as load balancers are supposed to do. Several of these mistakes are due to the implicit assumptions made by load balancing policies and mechanisms on the stability of system state. Our study shows that appropriate remedies at policy and mechanism levels can avoid these mistakes during millibottlenecks and remove the VLRT requests, thus improving the average response time by a factor of 12."
Performance Analysis of Cloud Computing Centers Serving Parallelizable Rendering Jobs Using M/M/c/r Queuing Systems.,"Performance analysis is crucial to the successful development of cloud computing paradigm. And it is especially important for a cloud computing center serving parallelizable application jobs, for determining a proper degree of parallelism could reduce the mean service response time and thus improve the performance of cloud computing obviously. In this paper, taking the cloud based rendering service platform as an example application, we propose an approximate analytical model for cloud computing centers serving parallelizable jobs using M/M/c/r queuing systems, by modeling the rendering service platform as a multi-station multi-server system. We solve the proposed analytical model to obtain a complete probability distribution of response time, blocking probability and other important performance metrics for given cloud system settings. Thus this model can guide cloud operators to determine a proper setting, such as the number of servers, the buffer size and the degree of parallelism, for achieving specific performance levels. Through extensive simulations based on both synthetic data and real-world workload traces, we show that our proposed analytical model can provide approximate performance prediction results for cloud computing centers serving parallelizable jobs, even those job arrivals follow different distributions."
Evaluation of Deep Learning Frameworks Over Different HPC Architectures.,"Recent advances in deep learning have enabled researchers across many disciplines to uncover new insights about large datasets. Deep neural networks have shown applicability to image, time-series, textual, and other data, all of which are available in a plethora of research fields. However, their computational complexity and large memory overhead requires advanced software and hardware technologies to train neural networks in a reasonable amount of time. To make this possible, there has been an influx in development of deep learning software that aim to leverage advanced hardware resources. In order to better understand the performance implications of deep learning frameworks over these different resources, we analyze the performance of three different frameworks, Caffe, TensorFlow, and Apache SINGA, over several hardware environments. This includes scaling up and out with single-and multi-node setups using different CPU and GPU technologies. Notably, we investigate the performance characteristics of NVIDIA's state-of-the-art hardware technology, NVLink, and also Intel's Knights Landing, the most advanced Intel product for deep learning, with respect to training time and utilization. To our best knowledge, this is the first work concerning deep learning bench-marking with NVLink and Knights Landing. Through these experiments, we provide analysis of the frameworks' performance over different hardware environments in terms of speed and scaling. As a result of this work, better insight is given towards both using and developing deep learning tools that cater to current and upcoming hardware technologies."
On Achieving Efficient Data Transfer for Graph Processing in Geo-Distributed Datacenters.,"Graph partitioning is important for optimizing the performance and communication cost of large graph processing jobs. Recently, many graph applications such as social networks store their data on geo-distributed datacenters (DCs) to provide services worldwide with low latency. This raises new challenges to existing graph partitioning methods, due to the costly Wide Area Network (WAN) usage and the multi-levels of network heterogeneities in geo-distributed DCs. In this paper, we propose a geo-aware graph partitioning method named G-Cut, which aims at minimizing the inter-DC data transfer time of graph processing jobs in geo-distributed DCs while satisfying the WAN usage budget. G-Cut adopts two novel optimization phases which address the two challenges in WAN usage and network heterogeneities separately. G-Cut can be also applied to partition dynamic graphs thanks to its light-weight runtime overhead. We evaluate the effectiveness and efficiency of G-Cut using realworld graphs with both real geo-distributed DCs and simulations. Evaluation results show that G-Cut can reduce the inter-DC data transfer time by up to 58% and reduce the WAN usage by up to 70% compared to state-of-the-art graph partitioning methods with a low runtime overhead."
GBooster: Towards Acceleration of GPU-Intensive Mobile Applications.,"The performance of GPUs on mobile devices is generally the bottleneck of multimedia mobile applications (e.g., 3D games and virtual reality). Previous attempts to tackle the issue mainly migrate GPU computation to servers residing in remote cloud centers. However, the costly network delay is especially undesirable for highly-interactive multimedia applications since a fast response time is critical for user experience. In this paper, we propose GBooster, a system that accelerates multimedia mobile applications by transparently offloading GPU tasks onto neighboring multimedia devices such as Smart TVs and Gaming Consoles. Specifically, GBooster intercepts and redirects system graphics calls by utilizing the Dynamic Linker Hooking technique, which requires no modification of the applications and the mobile systems. In addition, a major concern for offloading is the high energy consumption incurred by network transmissions. To address this concern, GBooster is designed to intelligently switch between the low-power Bluetooth and the high-throughput WiFi based on the traffic demand. We implement GBooster on the Android system and evaluate its performance. The results demonstrate that it can boost applications' frame rates by up to 85%. In terms of power consumption, GBooster can preserve up to 70% energy compared with local execution."
Scaling k-Nearest Neighbours Queries (The Right Way).,"Recently parallel / distributed processing approaches have been proposed for processing k-Nearest Neighbours (kNN) queries over very large (multidimensional) datasets aiming to ensure scalability. However, this is typically achieved at the expense of efficiency. With this paper we offer a novel approach that alleviates the performance problems associated with state of the art methods. The essence of our approach, which differentiates it from related research, rests on (i) adopting a coordinator-based distributed processing algorithm, instead of those employed over data-parallel executionengines (such as Hadoop/MapReduce or Spark), and (ii) on a way to organize data, to structure computation, and to index the stored datasets that ensures that only a very small number of data items are retrieved from the underlying data store, communicated over the network, and processed by the coordinatorfor every kNN query. Our approach also pays special attention to ensuring scalability in addition to low query processing times. Overall, kNN queries can be processed in just tens of milliseconds (as opposed to the tens of) seconds required by state of the art. We have implemented our approach, usinga NoSQL DB (HBase) as the data store, and we compare it against the state-of-the-art: the Hadoop-based Spatial Hadoop (SHadoop) and the Spark-based Simba methods. We employ different datasets of various sizes, showcasing the contributed performance advantages. Our approach outperforms the stateof the art, by 2-3 orders of magnitude, and consistently for dataset sizes ranging from hundreds of millions to hundreds of billions of data points. We also show that the key constituent performance overheads incurred during query processing (such as the number of data items retrieved from the data store, the required network bandwidth, and the processing time at the coordinator) scale very well, ensuring the overall scalability of the approach."
Parallelizing Big De Bruijn Graph Construction on Heterogeneous Processors.,"De Bruijn graph construction is the first step in de novo assemblers to connect input reads into a complete sequence without a reference genome. This step is both time and memory space consuming. To address this problem, we develop ParaHash, a system that partitions the input data in a compact format, parallelizes the computation on both the CPUs and the GPUs in a single computer, and performs hash-based De Bruijn graph construction. This way, ParaHash utilizes all available processors to assemble big genomes that cannot fit into memory. Furthermore, we analyze the characteristics of genome data to set the hash table size, design concurrent hashing algorithms to handle the inherent multiplicity, and pipeline the data transfer and the computation for further efficiency. Our experiments on real-world genome datasets show that the workload was balanced across heterogeneous processors, and that ParaHash was able to construct billion-node graphs on a single machine with an overall performance up to 20 times faster than the state-of-the-art shared-memory assemblers."
"Private, Yet Practical, Multiparty Deep Learning.","In this paper, we consider the problem of multiparty deep learning (MDL), wherein autonomous data owners jointly train accurate deep neural network models without sharing their private data. We design, implement, and evaluate ∝MDL, a new MDL paradigm built upon three primitives: asynchronous optimization, lightweight homomorphic encryption, and threshold secret sharing. Compared with prior work, ∝MDL departs in significant ways: a) besides providing explicit privacy guarantee, it retains desirable model utility, which is paramount for accuracy-critical domains; b) it provides an intuitive handle for the operator to gracefully balance model utility and training efficiency; c) moreover, it supports delicate control over communication and computational costs by offering two variants, operating under loose and tight coordination respectively, thus optimizable for given system settings (e.g., limited versus sufficient network bandwidth). Through extensive empirical evaluation using benchmark datasets and deep learning architectures, we demonstrate the efficacy of ∝MDL."
Fast and Flexible Networking for Message-Oriented Middleware.,"Distributed applications deployed in multi-datacenter environments need to deal with network connections of varying quality, including high bandwidth and low latency within a datacenter and, more recently, high bandwidth and high latency between datacentres. In principle, for a given network connection, each message should be sent over the best available network protocol, but existing middlewares do not provide this functionality. In this paper, we present KompicsMessaging, a messaging middleware that allows for fine-grained control of the network protocol used on a per-message basis. Rather than always requiring application developers to specify the appropriate protocol for each message, we also provide an online reinforcement learner that optimises the selection of the network protocol for the current network environment. In experiments, we show how connection properties, such as the varying round-trip time, influence the performance of the application and we show how throughput and latency can be improved by picking the right protocol at the right time."
TailCut: Power Reduction under Quality and Latency Constraints in Distributed Search Systems.,"Web search constitutes an important class of data-intensive online services in data centers. Optimizing search systems for energy efficiency, timely response and high search quality (i.e., how relevant the returned results are to a search query), however, is very challenging, as a search system involves a distributed architecture with hundreds of thousands of index serving nodes (ISNs) that return searching results to an aggregator through multiple interdependent retrieval stages in a partition-aggregate fashion. In this paper, we discover through experiments two important characteristics that can affect the system performance: (1) response time and energy consumption are greatly impacted by a small fraction of queries with long processing times; (2) the quality contribution of the ISN is independent of the query processing time. Based on our observation, we propose TailCut, which judiciously discards long query executions and enables ISN-aggregator coordination to minimize energy consumption subject to latency and quality constraints. Our experimental results show that TailCut can achieve up to 39% power saving, while satisfying the tail latency and quality constraint."
StoArranger: Enabling Efficient Usage of Cloud Storage Services on Mobile Devices.,"Cloud storage usages are becoming increasingly popular on mobile devices. Through an extensive motivation study, we find that cloud storage accesses from mobile apps suffer from several notable problems that undermine usage experiences. The root cause is that the way of cloud storage providers deploying their services onto mobile devices relies on app developers for the correct and appropriate implementations and lacks the ability of monitoring and servicing client-side cloud storage accesses. We propose StoArranger, a practical system framework that solves the problems by coordinating, rearranging, and transforming cloud storage communications on mobile devices. We have prototyped the proposed system using two different implementation approaches. We discuss our experiences of the implementations in the paper. The real-app evaluation experiments show that StoArranger can significantly improve mobile cloud storage access efficiency with little overheads."
Characterizing Performance and Energy-Efficiency of the RAMCloud Storage System.,"Most large popular web applications, like Facebook and Twitter, have been relying on large amounts of in-memory storage to cache data and offer a low response time. As the main memory capacity of clusters and clouds increases, it becomes possible to keep most of the data in the main memory. This motivates the introduction of in-memory storage systems. While prior work has focused on how to exploit the low-latency of in-memory access at scale, there is very little visibility into the energy-efficiency of in-memory storage systems. Even though it is known that main memory is a fundamental energy bottleneck in computing systems (i.e., DRAM consumes up to 40% of a server's power). In this paper, by the means of experimental evaluation, we have studied the performance and energy-efficiency of RAMCloud - a well-known in-memory storage system. We reveal that although RAMCloud is scalable for read-only applications, it exhibits non-proportional power consumption. We also find that the current replication scheme implemented in RAMCloud limits the performance and results in high energy consumption. Surprisingly, we show that replication can also play a negative role in crash-recovery."
Proactively Secure Cloud-Enabled Storage.,"Attacking cloud-enabled storage is becoming increasingly lucrative as more personal and enterprise data moves to the cloud. Traditional security mechanisms temporarily limit such attacks, but over a long period of time attackers will eventually find vulnerabilities; this can lead to compromising large amounts of valuable data and lead to large-scale privacy breaches. This paper addresses this problem by incorporating proactive security guarantees into cloud-enabled storage. Proactive security deals with an adversary's ability to eventually compromise all involved servers in a distributed storage or computation system. While there are several proactively secure secret sharing protocols that can be used to improve confidentiality of data stored in the cloud, their high overhead has traditionally limited them to less than ten parties and to only 100s of bytes typical for cryptographic keys. Realizing proactively secure cloud storage for larger data (e.g, MBs) requires careful design and calibration of system parameters, and faces several challenges. In this paper we design, implement and assess performance of the first system for Proactively Secure Cloud-Enabled Storage (PiSCES) of data larger than cryptographic keys. Based on our practical performance results we advocate that the high level of resilience and long-term security and confidentiality guarantees enabled by proactive security should be considered in future distributed and cloud-based storage and computing services."
BEES: Bandwidth- and Energy- Efficient Image Sharing for Real-Time Situation Awareness.,"In order to save human lives and reduce injury and property loss, Situation Awareness (SA) information is essential and important for rescue workers to perform the effective and timely disaster relief. The information is generally derived from the shared images via widely used smartphones. However, conventional smartphone-based image sharing schemes fail to efficiently meet the needs of SA applications due to two main reasons, i.e., real-time transmission requirement and application-level image redundancy, which is exacerbated by limited bandwidth and energy availability. In order to provide efficient image sharing in disasters, we propose a bandwidth-and energy-efficient image sharing system, called BEES. The salient feature behind BEES is to propose the concept of Approximate Image Sharing (AIS), which explores and exploits approximate feature extraction, redundancy detection, and image uploading to trade the slightly low quality of computation results in content-based redundancy elimination for higher bandwidth and energy efficiency. Nevertheless, the boundaries of the tradeoffs between the quality of computation results and efficiency are generally subjective and qualitative. We hence propose the energy-aware adaptive schemes in AIS to leverage the physical energy availability to objectively and quantitatively determine the tradeoffs between the quality of computation results and efficiency. Moreover, unlike existing work only for cross-batch similar images, BEES further eliminates in-batch ones via a similarity-aware submodular maximization model. We have implemented the BEES prototype which is evaluated via three real-world image datasets. Extensive experimental results demonstrate the efficacy and efficiency of BEES."
Transparent Fault-Tolerance Using Intra-Machine Full-Software-Stack Replication on Commodity Multicore Hardware.,"As the number of processors and the size of the memory of computing systems keep increasing, the likelihood of CPU core failures, memory errors, and bus failures increases and can threaten system availability. Software components can be hardened against such failures by running several replicas of a component on hardware replicas that fail independently and that are coordinated by a State-Machine Replication protocol. One common solution is to replicate the physical machine to provide redundancy, and to rewrite the software to address coordination. However, a CPU core failure, a memory error, or a bus error is unlikely to always crash an entire machine. Thus, full machine replication may sometimes be an overkill, increasing resource costs. In this paper, we introduce full software stack replication within a single commodity machine. Our approach runs replicas on fault-independent hardware partitions (e.g., NUMA nodes), wherein each partition is software-isolated from the others and has its own CPU cores, memory, and full software stack. A hardware failure in one partition can be recovered by another partition taking over its functionality. We have realized this vision by implementing FT-Linux, a Linux-based operating system that transparently replicates race-free, multithreaded POSIX applications on different hardware partitions of a single machine. Our evaluations of FT-Linux on several popular Linux applications show a worst case slowdown (due to replication) by ≈20%."
A Preventive Auto-Parallelization Approach for Elastic Stream Processing.,"Nowadays, more and more sources (connected devices, social networks, etc.) emit real-time data with fluctuating rates over time. Existing distributed stream processing engines (SPE) have to resolve a difficult problem: deliver results satisfying end-users in terms of quality and latency without over-consuming resources. This paper focuses on parallelization of operators to adapt their throughput to their input rate. We suggest an approach which prevents operator congestion in order to limit degradation of results quality. This approach relies on an automatic and dynamic adaptation of resource consumption for each continuous query. This solution takes advantage of i) a metric estimating the activity level of operators in the near future ii) the AUTOSCALE approach which evaluates the need to modify parallelism degrees at local and global scope iii) an integration into the Apache Storm solution. We show performance tests comparing our approach to the native solution of this SPE."
Dependable Cloud Resources with Guardian.,"Despite advances in making datacenters dependable, failures still happen. This is particularly onerous for long-running ""big data"" applications, where partial failures can lead to significant losses and lengthy recomputations. Big data processing frameworks like Hadoop MapReduce include fault tolerance (FT) mechanisms, but these are commonly targeted at specific system/failure models, and are often redundant between frameworks. This paper proposes the paradigm of dependable resources: big data processing frameworks are typically built on top of resource management systems (RMSs), and proposing FT support at the level of such an RMS yields generic FT mechanisms, which can be provided with low overhead by leveraging constraints on resources. We demonstrate our concepts through Guardian, a robust RMS based on YARN. Guardian allows frameworks to run their applications with individually configurable FT granularity and degree, with only minor changes to their implementation. We demonstrate the benefits of our approach by evaluating Hadoop, Tez, Spark and Pig on Guardian in Amazon-EC2, improving completion time by around 68% in the presence of failures, while maintaining around 6% overhead."
A Communication-Aware Container Re-Distribution Approach for High Performance VNFs.,"Containers have been used in many applications for isolation purposes due to the lightweight, scalable and highly portable properties. However, to apply containers in virtual network functions (VNFs) faces a big challenge because high-performance VNFs often generate frequent communication workloads among containers while the container communications are generally not efficient. Compared with hardware modification solutions, properly distributing containers among hosts is an efficient and low-cost way to reduce communication overhead. However, we observe that this approach yields a trade-off between the communication overhead and the overall throughput of the cluster. In this paper, we focus on the communication-aware container redistribution problem to optimize the communication overhead and the overall throughput jointly for VNF clusters. We propose a solution called FreeContainer which utilizes a novel two-stage algorithm to re-distribute containers among hosts. We implement FreeContainer in Baidu clusters with 6000 servers and 35 services deployed. Extensive experiments on real networks are conducted to evaluate the performance of the proposed approach. The results show that FreeContainer can increase the overall throughput up to 90% with significant reduction on communication overhead."
Minimizing Cost in IaaS Clouds Via Scheduled Instance Reservation.,"Regular diurnal patterns are often seen in the workloads of cloud-based online applications. This kind of non-stationary workloads changes the processing demands over time. To run application services with minimum costs, the number of cloud instances can be dynamically adjusted according to the workload variations. Recently, a new type of scheduled instances has emerged in the Infrastructure-as-a-Service market to facilitate such configurations. Scheduled instances can be reserved based on a recurring schedule and they offer price discounts. Meanwhile, cloud vendors require minimum scheduled durations to avoid the overhead of frequently launching and terminating cloud instances. Coupled with traditional on-demand and reserved instances, it becomes more complicated for users to find the optimal combination of these three pricing options to minimize their monetary costs. For the new scheduled instances, not only the number of instances but also their start and stop times have to be decided. In this paper, we develop a fast and effective strategy to solve this problem. Based on the hourly workload distributions, we first compute the optimal number of instances to acquire for each pricing option. Then, we design a scheduling algorithm to arrange the scheduled instances in compliance with the restriction of their scheduled durations. Using the workloads of the LOL online game and the Wikipedia Mobile service as two case studies, the efficacy of our strategy is demonstrated."
Efficient Distributed Coordination at WAN-Scale.,"Traditional coordination services for distributed applications do not scale well over wide-area networks (WAN): centralized coordination fails to scale with respect to the increasing distances in the WAN, and distributed coordination fails to scale with respect to the number of nodes involved. We argue that it is possible to achieve scalability over WAN using a hierarchical coordination architecture and a smart token migration mechanism, and lay down the foundation of a novel design for a flexible-consistent coordination framework, called WanKeeper. We implemented WanKeeper based on the ZooKeeper API and deployed it over WAN as a proof of concept. Our experimental results based on the Yahoo! Cloud Serving Benchmark (YCSB), Apache BookKeeper replicated log service, and the Shared Cloud-backed File System (SCFS) show that WanKeeper provides multiple folds improvement in write/update performance in WAN compared to ZooKeeper, while keeping the same read performance."
Specifying a Distributed Snapshot Algorithm as a Meta-Program and Model Checking it at Meta-Level.,"The paper proposes a new approach to model checking Chandy-Lamport Distributed Snapshot Algorithm (CLDSA). The essential of the approach is that CLDSA is specified as a meta-program in Maude such that the meta-program takes a specification of an underlying distributed system (UDS) and generates the specification of the UDS on which CLDSA is superimposed (UDS-CLDSA). To model check that a UDS-CLDSA enjoys a desired property, it suffices that human users specify the UDS for the proposed approach, while human users need to specify the UDS-CLDSA for the existing approach for each UDS. Since the proposed approach conducts model checking at meta-level, it produces a counterexample if a UDS-CLDSA does not enjoy the property, while the existing approach does not. Our method specifying CLDSA as a meta-program can be applied to formal specification of the class of distributed algorithms that are superimposed on UDSs."
Self-Evolving Subscriptions for Content-Based Publish/Subscribe Systems.,"Traditional pub/sub systems cannot adequately handle workloads of applications with dynamic, short-lived subscriptions such as location-based social networks, predictive stock trading, and online games. Subscribers must continuously interact with the pub/sub system to remove and insert subscriptions, thereby inefficiently consuming network and computing resources, and sacrificing consistency. In the aforementioned applications, we recognize that the changes in the subscriptions can follow a predictable pattern over some variable (e.g., time). In this paper, we present a new type of subscription, called evolving subscription, which encapsulates these patterns and allow the pub/sub system to autonomously adapt to the dynamic interests of the subscribers without incurring an expensive re-subscription overhead. We propose a general model for expressing evolving subscriptions and a framework for supporting them in a pub/sub system. To this end, we propose three different designs to support evolving subscriptions, which are evaluated and compared to the traditional resubscription approach in the context of two use cases: online games and high-frequency trading. Our evaluation shows that our solutions can reduce subscription traffic by 96.8% and improve delivery accuracy when compared to the baseline resubscription mechanism."
Scalable Routing for Topic-Based Publish/Subscribe Systems Under Fluctuations.,The loose coupling and the inherent scalability make publish/subscribe systems an ideal candidate for event-driven services for wireless networks using low power protocols such as IEEE 802.15.4. This work introduces a distributed algorithm to build and maintain a routing structure for such networks. The algorithm dynamically maintains a multicast tree for each node. While previous work focused on minimizing these trees we aim to keep the effort to maintain them in case of fluctuations of subscribers low. The multicast trees are implicitly defined by a novel structure called augmented virtual ring. The main contribution is a distributed algorithm to build and maintain this augmented virtual ring. Maintenance operations after sub-and unsubscriptions require message exchange in a limited region only. We compare the average lengths of the constructed forwarding paths with an almost ideal approach. As a result of independent interest we present a distributed algorithm using messages of size O(log n) for constructing virtual rings of graphs that are on average shorter than rings based on depth first search.
OPPay: Design and Implementation of a Payment System for Opportunistic Data Services.,"The large number of personal wireless devices in the urban areas could be used to provide various opportunistic data services, such as WiFi sharing, content-based file sharing and opportunistic networking. In order to facilitate these services, it is essential to incentivise the device owners to become service providers. However, previous research failed to deliver any practical payment systems for opportunistic data services. Inspired by smart contracts functionalities of bitcoin, this paper proposes a payment system named OPPay for opportunistic data services, which implements a micropayment communication protocol for mobile devices to perform data transactions and make payments using bitcoin. The system is designed to make incremental payments and thus resilient to interrupted communications caused by human mobility in the mobile network. By implementing and evaluating the system for three different applications, we show that the system is able to work in heterogeneous hardware and software environments and can achieve fast transactions confirmation with small fee overhead and low faulty payment value."
Optimal Resource Allocation for Multi-user Video Streaming over mmWave Networks.,"We investigate the resource allocation problem, including time slot allocation, channel allocation, and power adaptation, in a millimeter Wave (mmWave) network with multiple transmission links, multiple channels, and a PicoNet Coordinator (PNC). Each link has a video session to transmit from the transmitter to the receiver. The objective is to minimize the number of time slots to finish the video sessions of all links by jointly optimizing channel allocation and time slot allocation for links, while considering the possible interference between different links on the same channel. The optimal solution for the formulated problem is computationally prohibitive to obtain due to the exponential complexity. We developed a column generation based method to reformulate the original problem into a main problem along with a series of sub-problems, with greatly reduced complexity. We prove that the optimal solution for the reformulated problem converges to the optimal solution of the original problem, and we derived a lower bound for the performance of the reformulated problem at each iteration, which will finally converge to the global optimal solution. The proposed scheme is validated with simulations with its superior performance over existing work is observed."
A Multi-agent Parallel Approach to Analyzing Large Climate Data Sets.,"Despite various cloud technologies that have parallelized and scaled up big data analysis, they target data mostly in texts which are easy to partition and thus easy to map over a cluster system. Therefore, their parallelization do not necessarily cover scientific structured data such as NetCDF or need additional, user-provided tools to convert the original data into specific formats. To facilitate user-intuitive parallelization of such scientific data analysis, this paper presents an agent-based approach that instantiates distributed arrays over a cluster system, maintains structured scientific data in these arrays, deploys many mobile agents over the arrays to perform computational actions on data, and collects necessary results. To demonstrate the practicability of our agent-based approach, we focused on climate change research and implemented a web-interfaced climate analysis, using the MASS (multi-agent spatial simulation) library. In this paper, we show practical advantages of, performance improvements by, and challenges for our agent-based approach in structured data analysis."
Energy Proportional Servers: Where Are We in 2016?,"The huge energy consumption in data centers produces not only high electricity bill but also tremendous carbon footprints. Although today's servers and data centers of leading internet companies are more energy efficient than ever before, the fluctuations in external workload and internal resource utilization calls for energy proportional computing. Insight into server energy proportionality can help improve workload placement while also reducing energy consumption. In this paper, we investigate all 477 valid published results of SPECpower_ssj benchmark from 2007 to 2016Q3 and reorganize them by hardware availability year for more accurate analysis on production servers. Through comprehensive analysis we find that: (1) The specious stagnation of energy proportionality in recent years is mainly caused by the adoption of processors of specific microarchitecture and is not the indicative trend of energy proportionality improvement. (2) Microarchitecture evolution has more influence on energy efficiency improvement than energy proportionality. (3) Today's servers' peak energy efficiencies are shifting from 100% resource utilization to 80% or 70% utilization and server energy proportionality improves with such shifting. We then conduct extensive experiments on 4 rack servers to investigate the energy efficiency variations under different hardware configurations, including memory per core installation and processor frequency scaling. Our experiments show that hardware configuration has significant impact on server's energy efficiency. Our findings presented in this paper provide useful insights and guidance to system designers, as well as data center operators for energy proportionality aware workload placement and energy savings."
Are HTTP/2 Servers Ready Yet?,"Superseding HTTP/1.1, the dominating web protocol, HTTP/2 promises to make web applications faster and safer by introducing many new features, such as multiplexing, header compression, request priority, server push, etc. Although a few recent studies examined the adoption of HTTP/2 and evaluated its impacts, little is known about whether the popular HTTP/2 servers have correctly realized the new features and how the deployed servers use these features. To fill in the gap, in this paper, we conduct the first systematic investigation by inspecting six popular implementations of HTTP/2 servers (i.e., Nginx, Apache, H2O, Lightspeed, nghttpd and Tengine) and measuring the top 1 million Alexa web sites. In particular, we propose new methods and develop a tool named H2Scope to assess the new features in those servers. The results of the large-scale measurement on HTTP/2 web sites reveal new observations and insights. This study sheds light on the current status and the future research of HTTP/2."
Data Integrity for Collaborative Applications over Hosted Services.,"In this work we focus on integrity and consistency of data accessed and manipulated by multiple collaborating users, and stored in an (untrusted) hosted service. This is a problem, aspects of which have been studied in isolation in hitherto distinct communities. Consistency is one of the cardinal problems of distributed computing. Integrity of hosted data has been studied over the last decade, and numerous techniques for proof of data possession and/or retrievability have been explored. The latter line of work however have often assumed static data, and techniques to handle dynamic or versioned data have only very recently been proposed. Yet, even the existing solutions that handle mutable content do so under the assumption that only a single data owner (using a single client) manipulate and verify said data. This is a serious limitation in terms of the variety of applications that can benefit from such mechanisms for proof of data possession. The novelty, and primary contribution of this work is in filling this gap. Specifically, we extend the existing ideas of proof of possession of dynamic data, in order to support multiple users who may collaborate in real time or asynchronously. In contrast (and addition) to the challenge of an untrusted storage server that existing techniques for proof of data possession need to overcome, we had to, simultaneously account for data integrity violations that may be incurred due to all the usual challenges of maintaining consistency of collaborative data (even if the storage server was trusted)."
Virtual Machine Power Accounting with Shapley Value.,"The ever-increasing power consumption of datacenters has eaten up a large portion of their profit. One possible solution is to charge datacenter users for their actual power usage. However, it poses a great technical challenge as the power of VMs co-existing in a physical machine cannot be measured directly. It is thus critical to develop a fair method to disaggregate the power of a physical machine to individual VMs. We tackle the above challenge by modeling the power disaggregation problem as a cooperative game and propose non-deterministic Shapley value to discover the fair power share of VMs (in the sense of satisfying four desired axiomatic principles), while compensating the negative impact of VM power variation. We demonstrate that the results from existing power model-based solution can deviate from the ""ground truth"" by 25.22% ~46.15%. And compared with the exact Shapley value, our non-deterministic Shapley value can achieve less than 5% error for 90% of the time."
A General Purpose Testbed for Mobile Data Gathering in Wireless Sensor Networks and a Case Study.,"In recent years, mobile data gathering in wireless sensor networks has attracted much interests in the research community. However, despite extensive efforts, many of previous work in this area lies only in theory and evaluates network performance with computer simulations, which leaves a large gap from reality. In this paper, we present the design and implementation of a general purpose, flexible platform for mobile data gathering in wireless sensor networks to evaluate network performance and algorithms in a practical setting. Instead of relying on hand-crafted theoretical models, our platform integrates both mobile data collector and sensor nodes to provide realistic performance evaluations. In addition, the platform adopts a modular design in mobile data collector and sensor nodes, and equips the mobile data collector with advanced computing capability, which makes it versatile for evaluating the performance of a wide-range of applications. Finally, as a case study, weimplement a wildlife monitoring system on our platform. Our experimental results demonstrate that real implementations can evaluate many practical performance factors which would have a great impact on the sensing results and are very difficult to fully capture by theoretical models and simulations. We expect that this platform can become a very powerful general tool for more accurate network simulations and facilitate performance optimization in wireless sensor networks."
On Directional Neighbor Discovery in mmWave Networks.,"The directional neighbor discovery problem, i.e., spatial rendezvous, is a fundamental problem in millimeter wave (mmWave) networks. The challenge is how to let the transmitter and receiver beams meet in space under deafness caused by directional transmission and reception. In this paper, we present a Hunting-based Directional Neighbor Discovery (HDND) scheme, where a node continuously rotates its directional beam to scan its neighborhood for neighbors. Through a rigorous analysis, we derive the conditions for ensured neighbor discovery, as well as a bound for the worst case discovery time. We validate the analysis with extensive simulations, and demonstrate the superior performance of the proposed scheme over two benchmark schemes."
Observable-by-Design.,"We present the observable-by-design principle. We believe that the new generation of services, products, and environment management systems should be designed to adapt to changes. Therefore, they should be designed to be observable, and their design should proactively and reactively adapt to the changes observed both internally and externally. Two concrete examples illustrate the application of observable-by-design principle: (1) ship building and management, and (2) river dam water flow management. We believe that the observable-by-design principle can be applied in a large scale. In the long term, a new generation of observable- by-design infrastructures can be built that incorporates the sensing and adapting capabilities in their construction."
"An Architectural Vision for a Data-Centric IoT: Rethinking Things, Trust and Clouds.","The Internet of Things (IoT) is producing a tidal wave of data, much of it originating at the network edge, from applications with requirements unmet by the traditional back-end Cloud architecture. To address the disruption caused by the overabundance of data, this paper offers a holistic data-centric architectural vision for the data-centric IoT. It advocates that we rethink our approach to the design and definition of key elements: that we shift our focus from Things to Smart Objects; grow Trust organically; and evolve back-end Clouds toward Edge and Fog clouds, which leverage data-centric networks and enable optimal handling of upstream data flows. Along the way, we wax poetic about several blue-sky topics, assess the status of these elements in the context of related work, and identify known gaps in meeting this vision."
Edge Computing and IoT Based Research for Building Safe Smart Cities Resistant to Disasters.,"Recently, several researches concerning with smart and connected communities have been studied. Soon the 4G / 5G technology becomes popular, and cellular base stations will be located densely in the urban space. They may offer intelligent services for autonomous driving, urban environment improvement, disaster mitigation, elderly/disabled people support and so on. Such infrastructure might function as edge servers for disaster support base. In this paper, we enumerate several research issues to be developed in the ICDCS community in the next decade in order for building safe, smart cities resistant to disasters. In particular, we focus on (A) up-to-date urban crowd mobility prediction and (B) resilient disaster information gathering mechanisms based on the edge computing paradigm. We investigate recent related works and projects, and introduce our on-going research work and insight for disaster mitigation."
The Internet of Things and Multiagent Systems: Decentralized Intelligence in Distributed Computing.,"Traditionally, distributed computing concentrates on computation understood at the level of information exchange and sets aside human and organizational concerns as largely to be handled in an ad hoc manner. Increasingly, however, distributed applications involve multiple loci of autonomy. Research in multiagent systems (MAS) addresses autonomy by drawing on concepts and techniques from artificial intelligence. However, MAS research generally lacks an adequate understanding of modern distributed computing. In this Blue Sky paper, we envision decentralized multiagent systems as a way to place decentralized intelligence in distributed computing, specifically, by supporting computation at the level of social meanings. We motivate our proposals for research in the context of the Internet of Things (IoT), which has become a major thrust in distributed computing. From the IoT's representative applications, we abstract out the major challenges of relevance to decentralized intelligence. These include the heterogeneity of IoT components; asynchronous and delay-tolerant communication and decoupled enactment; and multiple stakeholders with subtle requirements for governance, incorporating resource usage, cooperation, and privacy. The IoT yields high-impact problems that require solutions that go beyond traditional ways of thinking. We conclude with highlights of some possible research directions in decentralized MAS, including programming models; interaction-oriented software engineering; and what we term enlightened governance."
Internet of Things: From Small- to Large-Scale Orchestration.,"The domain of Internet of Things (IoT) is rapidly expanding beyond research, and becoming a major industrial market with such stakeholders as major manufacturers of chips and connected entities (i.e., things), and fast-growing operators of wide-area networks. Importantly, this emerging domain is driven by applications that leverage an IoT infrastructure to provide users with innovative, high-value services. IoT infrastructures range from small scale (e.g., homes and personal health) to large scale (e.g., cities and transportation systems). In this paper, we argue that there is a continuum between orchestrating connected entities in the small and in the large. We propose a unified approach to application development, which covers this spectrum. To do so, we examine the requirements for orchestrating connected entities and address them with domainspecific design concepts. We then show how to map these design concepts into dedicated programming patterns and runtime mechanisms. Our work revolves around domain-specific concepts and notations, integrated into a tool-based design methodology and dedicated to develop IoT applications. We have applied our work across a spectrum of infrastructure sizes, ranging from an automated pilot in avionics, to an assisted living platform for the home of seniors, to a parking management system in a smart city."
EdgeOS_H: A Home Operating System for Internet of Everything.,"The proliferation of Internet of Everything (IoE) and the success of rich Cloud services have pushed the horizon of a new computing paradigm, Edge Computing, which calls for processing the data at the edge of the network. Smart home as a typical IoE application is being widely adapted into people's lives. Edge Computing has the potential to empower the smart home, but it needs more contribution from the community before it truly benefits our lives. In this paper, we present the vision of EdgeOSH, a home operating system for Internet of Everything. We further discuss functional challenges, namely programming interface, self-management, data management, security & privacy, and naming, as well as non-functional challenges, such as user experience, system cost, delay, and the lack of availability of open testbed. Within each challenge we also discuss the potential directions that are worth further investigation."
A Vision for Zero-Hop Networking (ZeN).,"It has become increasingly important for content providers (CPs) to reach consumers with low latency. Peering links that connect CPs directly to access Internet service providers (access ISPs) have been used for this purpose thus providing one-hop AS paths from CPs to users. While providing improved latency, these peering links still do not give CPs control over the entire end-to-end path to their users. This has made it difficult for CPs to completely manage user experience. Motivated by this, we propose the deployment of Zero-Hop Networks (ZeN), where a CP's entire end-to-end path to users is under its control. We believe it is important to respond to the compelling demand for ZeN and enable its provision over the shared Internet infrastructure so that all may continue to reap its benefits. In this paper we lay out the vision for ZeN, describing its goals and challenges. We propose to deploy ZeN by allowing CPs to extend their network's control over the access ISP substrate in a way that allows the CP to control the entire end-to-end path. We develop two strawman architectures based on Software-Defined Networking ideas: one based on resource reservation and the other based on network virtualization. We also discuss some elements of a research agenda that is needed to bring ZeN deployments to realization."
Structured Overlay Networks for a New Generation of Internet Services.,"The dramatic success and scaling of the Internet was made possible by the core principle of keeping it simple in the middle and smart at the edge (or the end-to-end principle). However, new applications bring new demands, and for many emerging applications, the Internet paradigm presents limitations. For applications in this new generation of Internet services, structured overlay networks offer a powerful framework for deploying specialized protocols that can provide new capabilities beyond what the Internet natively supports by leveraging global state and in-network processing. The structured overlay concept includes three principles: A resilient network architecture, a flexible overlay node software architecture that exploits global state and unlimited programmability, and flow-based processing. We demonstrate the effectiveness of structured overlay networks in supporting today's demanding applications and propose forward-looking ideas for leveraging the framework to develop protocols that push the boundaries of what is possible in terms of performance and resilience."
Ensuring Network Neutrality for Future Distributed Systems.,"Network Neutrality is essential for ensuring a level playing field for the development of new applications and services on the Internet. Laws and rules alone might not be enough to protect innovation, fair competition and consumer's freedom of choice online. The research community has the responsibility to propose solutions that reveal discriminatory traffic management mechanisms on the Internet. We present the potential risks of a non-neutral Internet, identify several open challenges for designing solutions that detect traffic differentiation, and propose a model that addresses such challenges by taking advantage of distributed systems technologies."
Uncovering the Useful Structures of Complex Networks in Socially-Rich and Dynamic Environments.,"Many group activities can be represented as a complex network where entities (vertices) are connected in pairs by lines (edges). Uncovering a useful global structure of complex networks is important for understanding system behaviors and in providing global guidance for application designs. We briefly review existing network models, discuss several tools used in the traditional graph theory, distributed computing, distributed systems, and social network communities, and point out their limitations. We discuss opportunities to uncover the structural properties of complex networks, especially in a mobile environment, and we summarize three promising approaches for uncovering useful structures: trimming, layering, and remapping. Finally, we present some challenges in algorithmic techniques, with a focus on distributed and localized solutions, to represent various structures."
Future Networking Challenges: The Case of Mobile Augmented Reality.,"Mobile augmented reality (MAR) applications are gaining popularity due to the wide adoption of mobile and especially wearable devices. Such devices often present limited hardware capabilities while MAR applications often rely on computationally intensive computer vision algorithms with extreme latency requirements. To compensate for the lack of computing power, offloading data processing to a distant machine is often desired. However, if this process introduces new constrains in the application, especially in terms of latency and bandwidth. If current network infrastructures are not ready for such traffic, we envision that future wireless networks such as 5G will rapidly be saturated by resource hungry MAR applications. Moreover, due to the high variance of wireless networks, MAR applications should not rely only on the evolution of infrastructures. In this article, we analyze MAR applications and justify their need for accessing external infrastructure. After a review of the existing network infrastructures and protocols, we define guidelines for future real-time and multimedia transport protocols, with a focus on MAR offloading."
Software Defined Cyberinfrastructure.,"Within and across thousands of science labs, researchers and students struggle to manage data produced in experiments, simulations, and analyses. Largely manual research data lifecycle management processes mean that much time is wasted, research results are often irreproducible, and data sharing and reuse remain rare. In response, we propose a new approach to data lifecycle management in which researchers are empowered to define the actions to be performed at individual storage systems when data are created or modified: actions such as analysis, transformation, copying, and publication. We term this approach software-defined cyberinfrastructure because users can implement powerful data management policies by deploying rules to local storage systems, much as software-defined networking allows users to configure networks by deploying rules to switches.We argue that this approach can enable a new class of responsive distributed storage infrastructure that will accelerate research innovation by allowing any researcher to associate data workflows with data sources, whether local or remote, for such purposes as data ingest, characterization, indexing, and sharing. We report on early experiments with this approach in the context of experimental science, in which a simple if-trigger-then-action (IFTA) notation is used to define rules."
Computing in the Continuum: Combining Pervasive Devices and Services to Support Data-Driven Applications.,"The exponential growth of digital data sources has the potential to transform all aspects of society and our lives. However, to achieve this impact, the data has to be processed promptly to extract insights that can drive decision making. Further, traditional approaches that rely on moving data to remote data centers for processing are no longer feasible. Instead, new approaches that effectively leverage distributed computational infrastructure and services are necessary. Specifically, these approaches must seamlessly combine resources and services at the edge, in the core, and along the data path as needed. This paper presents our vision for enabling an approach for computing in the continuum, i.e., realizing a fluid ecosystem where distributed resources and services are programmatically aggregated on-demand to support emerging data-driven application workflows. This vision calls for novel solutions for federating infrastructure, programming applications and services, and composing dynamic workflows, which are capable of reacting in real-time to unpredictable data sizes, availabilities, locations, and rates."
Decision-Driven Execution: A Distributed Resource Management Paradigm for the Age of IoT.,"This paper introduces a novel paradigm for resource management in distributed systems, called decision-driven execution. The paradigm is appropriate for mission-driven systems, where the goal is to enable faster, leaner, and more effective decision making. All resource consumption, in this paradigm, is tied to the needs of making decisions on alternative courses of action. A point of departure from traditional architectures lies in interfaces that allow applications to specify their underlying decision logic. This specification, in turn, allows the system to reason about most effective means to meet information needs of decisions, resulting in simultaneous optimization of decision accuracy, cost, and speed. The paper discusses the overall vision of decision-driven execution, outlining preliminary work and novel challenges."
ACTiCLOUD: Enabling the Next Generation of Cloud Applications.,"Despite their proliferation as a dominant computing paradigm, cloud computing systems lack effective mechanisms to manage their vast amounts of resources efficiently. Resources are stranded and fragmented, ultimately limiting cloud systems' applicability to large classes of critical applications that pose non-moderate resource demands. Eliminating current technological barriers of actual fluidity and scalability of cloud resources is essential to strengthen cloud computing's role as a critical cornerstone for the digital economy. ACTiCLOUD proposes a novel cloud architecture that breaks the existing scale-up and share-nothing barriers and enables the holistic management of physical resources both at the local cloud site and at distributed levels. Specifically, it makes advancements in the cloud resource management stacks by extending state-of-the-art hypervisor technology beyond the physical server boundary and localized cloud management system to provide a holistic resource management within a rack, within a site, and across distributed cloud sites. On top of this, ACTiCLOUD will adapt and optimize system libraries and runtimes (e.g., JVM) as well as ACTiCLOUD-native applications, which are extremely demanding, and critical classes of applications that currently face severe difficulties in matching their resource requirements to state-of-the-art cloud offerings."
JointCloud: A Cross-Cloud Cooperation Architecture for Integrated Internet Service Customization.,"Cloud computing has completely changed the economics of IT industry. Recently, the new form of ""shared global economy"" requires cloud services to be collaboratively provisioned by different cloud providers in a Geo-distributed manner, which brings severe challenges in service performance and cost. To address this problem, in this paper we propose JointCloud, a cross-cloud cooperation architecture for integrated Internet service customization. JointCloud borrows the idea from airline alliances and aims at empowering the cooperation among multiple clouds to provide efficient cross-cloud services. JointCloud focuses not only on the vertical integration of cloud resources but also on the horizontal cooperation among different cloud vendors. This paper describes the concept and architecture of JointCloud, as well as the initial design of the key components, namely, communication, storage, and computation."
Supporting Data Analytics Applications Which Utilize Cognitive Services.,"A wide variety of services are available over the Web which can dramatically improve the functionality of applications. These services include information retrieval (including data lookups from a variety of sources and Web searches), natural language understanding, visual recognition, and data storage. A key problem is how to provide support for applications which use these services. This paper presents a rich software development kit (SDK) which accesses these services and provides a variety of features applications need to use these services, optimize performance, and compare them. A key aspect of our SDK is its support for natural language understanding services. We also present a personalized knowledge base built on top of our rich SDK that uses publically available data sources as well as private information. The knowledge base supports data analysis and reasoning over data."
Trillion Operations Key-Value Storage Engine: Revisiting the Mission Critical Analytics Storage Software Stack.,"Data is the new natural resource of this century. As data volumes grow and applications aimed at monetizing the data continue to evolve, data processing platforms are expected to meet new scale, performance, reliability and data retention requirements. At the same time, storage hardware continues to improve in performance and price-performance. In this paper, we present TOKVS - Trillion Operation Key-Value Store, a NoSQL storage engine that redefines the storage software stack to meet the requirements of next-generation applications on next-generation hardware."
"How Computer Science Risks to Lose Its Innocence, and Should Attempt to Take Responsibility.","Computer science is playing a driving role in transforming today's society through information technology. In this transformation we observe power shifts increasingly strengthening centralised organisations, which are negatively perceived by many people. We outline technical questions that computer science should pay attention to in order to enable individuals in preserving their interest and to take meaningful decisions based on reliable information."
A Cognitive Policy Framework for Next-Generation Distributed Federated Systems: Concepts and Research Directions.,"Next-generation collaborative activities and missions will be carried out by autonomous groups of devices with a large variety of cognitive capabilities. These devices will have to operate in environments characterized by uncertainty, insecurity (both physical and cyber), and instability. In such environments, communications may be fragmented. Proper policy-based management of such autonomous device groups is thus critical. However current policy management systems have many limitations, including lack of flexibility. In this paper, we articulate novel architectural approaches addressing the requirements for the effective management of autonomous groups of devices and discuss the notion of generative policies - a novel paradigm that enhances the flexibility of policy-based approaches to management. In this paper, we also survey types of policy that are essential for managing device groups. Even though many such policy types exist in conventional settings, their use in our context poses novel challenges that we articulate in the paper. We also introduce a research roadmap discussing several research directions towards the development of a cognitive and flexible policy-based approach to the management of autonomous groups of devices for collaborative missions. Finally, as our proposed policy paradigm is data-intensive, we discuss the problem of supplying the data required for policy decisions in environments characterized by mobility, uncertainly, and fragmented communications."
Machine to Machine Trust in Smart Cities.,"In the coming decades, we will live in a world surrounded by tens of billions of devices that will interoperate and collaborate in an effort to deliver personalized and autonomic services. This paradigm of smart objects and smart things interconnected and ubiquitously surrounding us is called the Internet of Things (IoT). Cities may be the first to benefit from the IoT, but reliance on these machines to make decisions has profound implications for trust, and makes mechanisms for expressing and reasoning about trust essential. This paper introduces the project funded by the Georgia Tech Research Institute to look at several dimensions of Machine to Machine Trust in the context of Smart Cities."
Lateral Thinking for Trustworthy Apps.,"The growing computerization of critical infrastructure as well as the pervasiveness of computing in everyday life has led to increased interest in secure application development. We observe a flurry of new security technologies like ARM TrustZone and Intel SGX, but a lack of a corresponding architectural vision. We are convinced that point solutions are not sufficient to address the overall challenge of secure system design. In this paper, we outline our take on a trusted component ecosystem of small individual building blocks with strong isolation. In our view, applications should no longer be designed as massive stacks of vertically layered frameworks, but instead as horizontal aggregates of mutually isolated components that collaborate across machine boundaries to provide a service. Lateral thinking is needed to make secure systems going forward."
Rumor Initiator Detection in Infected Signed Networks.,"In many cases, the information spread in an online network may not always be truthful or correct; such information corresponds to rumors. In recent years, signed networks have become increasingly popular because of their ability to represent diverse relationships such as friends, enemies, trust, and distrust. Signed networks are ideal for information flow in a network with varying beliefs (trust or distrust) about facts. In this paper, we will study the problem of influence analysis and diffusion models in signed networks and investigate the problem of rumor initiator detection, given the state of the network at a given moment in time. Conventional information diffusion models for unsigned networks cannot be applied to signed networks directly, and we show that the rumor initiator detection problem is NP-hard. We propose a new information diffusion model, referred to as asyMmetric Flipping Cascade (MFC), to model the propagation of information in signed networks. Based on MFC, a novel framework, Rumor Initiator Detector (RID), is introduced to determine the potential number and the identity of the rumor initiators from the state of the network at a given time. Extensive experiments conducted on real-world signed networks demonstrate that MFC works very well in modeling information diffusion in signed networks and RID can significantly outperform other comparison methods in identifying rumor initiators."
Addressing Smartphone-Based Multi-factor Authentication via Hardware-Rooted Technologies.,"Multi-factor authentication is a well-recognized access control method that enhances the security of users' sensitive data and identities. A successful authentication attempt requires a user to correctly present two or more authentication factors such as knowledge factors, possession factors and inherence factors. For smartphone-based multi-factor authentication, a promising way to authenticate a user is to verify his possession of a legitimate smartphone, which calls for secure and usable device authentication schemes. In this article, we propose to authenticate a device through tracking the hardware fingerprint of its built-in sensor. We first review the existing hardware-rooted identification methods and discuss the merits of applying a hardware fingerprint as a smartphone's unique identity. Then, we analyze the security issues underlying these methods and identify two security requirements for the identification methods to be used in an authentication scheme: Fingerprint Leakage Resilience and Fingerprint Forgery Resilience. Finally, we look into a specific hardware fingerprint originally used for digital cameras. We analyze the feasibility of applying this fingerprint to differentiate off-the-shelf smartphones and list several challenging practical issues underlying this method."
Enabling Wide Area Data Analytics with Collaborative Distributed Processing Pipelines (CDPPs).,"Life without the Internet is no longer possible nor thinkable. Consider the effects of a prolonged Internet outage: In the least impactful way, most of our kids and peers just would no longer be able to interact with their peers. They might severely miss out on the quality of their leisure time activities which increasingly relies on social networks, online games, YouTube, and other online entertainment offers. This may be a nuisance but still is tolerable. More seriously and economically relevant, manufacturing and trade would no longer work as all interactions inside and among companies rely on a working Internet. Indeed, just-in-time ordering mechanisms and Internet of Things-enhanced production chains within the Industry 4.0 framework would no longer be operational as old-style communication means such as phone and faxes have completely been replaced. Indeed, neither of these alternative mechanisms-faxes, phone, and also messaging-would be available either as they also rely on Internet technology. Even worse, the control of critical infrastructures would also be affected severely as they increasingly rely on the Internet for gathering input data and propagating control information. Moreover, all big data analytic applications, including financial transactions, would fail as they can no longer gather and process their input data. Even worse, the fact that there is “no communication without energy” nowadays also means that the reciprocal statement applies that there is no “energy without communication”."
"The Millibottleneck Theory of Performance Bugs, and Its Experimental Verification.","The performance of n-tier web-facing applications often suffer from response time long-tail problem. With relatively low resource utilization (less than 50%) and the majority of requests returning within a few milliseconds, a non-negligible num-ber of normally short requests may take seconds to return. We propose the millibottleneck theory of performance bugs (that lead to long-tail problems). Several case studies have confirmed the millibottlenecks (that last a few tens to hundreds of milliseconds) as causal agents of long requests. A concrete example (garbage collection) illustrates the experimental verification of millibottlenecks. An open source fine-grain monitoring toolkit is being devel-oped to facilitate the experimental research on millibottlenecks."
Exacution: Enhancing Scientific Data Management for Exascale.,"As we continue toward exascale, scientific data volume is continuing to scale and becoming more burdensome to manage. In this paper, we lay out opportunities to enhance state of the art data management techniques. We emphasize well-principled data compression, and using it to achieve progressive refinement. This can both accelerate I/O and afford the user increased flexibility when she interacts with the data. The formulation naturally maps onto enabling partitioning of the progressively improving-quality representations of a data quantity into different media-type destinations, to keep the highest priority information as close as possible to the computation, and take advantage of deepening memory/storage hierarchies in ways not previously possible. Careful monitoring is requisite to our vision, not only to verify that compression has not eliminated salient features in the data, but also to better understand the performance of massively parallel scientific applications. Increased mathematical rigor would be ideal,to help bring compression on a better-understood theoretical footing, closer to the relevant scientific theory, more aware of constraints imposed by the science, and more tightly error-controlled. Throughout, we highlight pathfinding research we have begun exploring related these topics, and comment toward future work that will be needed."
Hardware Acceleration Landscape for Distributed Real-Time Analytics: Virtues and Limitations.,"We are witnessing a technological revolution with a broad impact ranging from daily life (e.g., personalized medicine and education) to industry (e.g., data-driven healthcare, commerce, agriculture, and mining). At the core of this transformation lies ""data"". This transformation is facilitated by embedded devices, collectively known as Internet of Things (IoT), which produce real-time feeds of sensor data which are collected and processed to produce a dynamic physical model used for optimized real-time decision making. At the infrastructure level, there is a need to develop a scalable architecture for processing massive volumes of present and historical data at an unprecedented velocity to support the IoT paradigm. To cope with such extreme scale, we argue for the need to revisit the hardware and software co-design landscape in light of two key technological advancements. First is the virtualization of computation and storage over highly distributed data centers spanning across continents. Second is the emergence of a variety of specialized hardware accelerators that complement traditional general-purpose processors. Further efforts are required to unify these two trends in order to harness the power of big data. In this paper, we present a formulation and characterization of the hardware acceleration landscape geared towards real-time analytics in the cloud. Our goal is to assist both researchers and practitioners navigating the newly revived field of software and hardware co-design for building next generation distributed systems. We further present a case study to explore software and hardware interplay for designing distributed real-time stream processing."
Coordinating Distributed Speaking Objects.,"In this paper we sketch a vision of future environments densely populated by smart sensors and actuators - possibly embedded in everyday objects - that, rather than simply producing streams of data, are capable of understanding and reporting, via factual assertions and arguments, about what is happening (for sensors) and about what they can make possibly happen (for actuators). These ""speaking objects"" form the nodes of a dense distributed computing infrastructure that can be exploited to monitor and control activities in our everyday environment. However, the nature of speaking objects will dramatically change the approaches to implementing and coordinating the activities of distributed processes. In fact, distributed coordination is likely to become associated with the capability of argumenting about situations and about the current ""state of the affairs"", with the aim of triggering and directing proper distributed ""conversations"" to collectively reach a future desirable state. Accordingly, we discuss how such a novel vision can build upon some readily available technologies, and the research challenges that it poses. Two case studies are used as exemplary scenarios."
Model-Driven Domain-Specific Middleware.,"Middleware was introduced to facilitate the development of sophisticated applications based on a uniform methodology and industry standards. However, early research and practice suggested that no one-size-fits-all approach was suitable for all application domains and scenarios. This gave rise to industry initiatives to standardize domain-specific middleware services and profiles, as well as research efforts on configurable, reflective, and adaptive middleware. The industry's approach led to easy deployment, although with a level of flexibility limited by the extent of existing profiles. The approach of the research community, on the other hand, enabled high flexibility, allowing any middleware configuration to be defined. Nevertheless, creating sound configurations using this approach is a challenging task, limiting the target audience to expert engineers. As a consequence, both initiatives do not scale with the current proliferation of specialized application domains. In this paper, we target this problem with an approach that leverages model-driven engineering for the construction of domain-specific middleware platforms. A set of high-level, yet expressive, building blocks is defined in the form of a metamodel, which is used to create models that specify the desired middleware configuration. We argue that this approach enables the rapid development of middleware platforms to match the proliferation of application domains, at the same time as it does not require per-application middleware construction or even highly skilled middleware engineers. We present the current state of our research and discuss research directions to fully realize the approach."
On the Design of a Blockchain Platform for Clinical Trial and Precision Medicine.,"This paper proposes a blockchain platform architecture for clinical trial and precision medicine and discusses various design aspects and provides some insights in the technology requirements and challenges. We identify 4 new system architecture components that are required to be built on top of traditional blockchain and discuss their technology challenges in our blockchain platform: (a) a new blockchain based general distributed and parallel computing paradigm component to devise and study parallel computing methodology for big data analytics, (b) blockchain application data management component for data integrity, big data integration, and integrating disparity of medical related data, (c) verifiable anonymous identity management component for identity privacy for both person and Internet of Things (IoT) devices and secure data access to make possible of the patient centric medicine, and (d) trust data sharing management component to enable a trust medical data ecosystem for collaborative research."
Towards Dataflow-Based Graph Accelerator.,"Existing graph processing frameworks greatly improve the performance of memory subsystem, but they are still subject to the underlying modern processor, resulting in the potential inefficiencies for graph processing in the sense of low instruction level parallelism and high branch misprediction. These inefficiencies, in accordance with our comprehensive micro-architectural study, mainly arise out of a wealth of dependencies, serial semantic of instruction streams, and complex conditional instructions in graph processing. In this paper, we propose that a fundamental shift of approach is necessary to break through the inefficiencies of the underlying processor via the dataflow paradigm. It is verified that the idea of applying dataflow approach into graph processing is extremely appealing for the following two reasons. First, as the execution and retirement of instructions only depend on the availability of input data in dataflow model, a high degree of parallelism can be therefore provided to relax the heavy dependency and serial semantic. Second, dataflow is guaranteed to make it possible to reduce the costs of branch misprediction by simultaneously executing all branches of a conditional instruction. Consequently, we make the preliminary attempt to develop the dataflow insight into a specialized graph accelerator. We believe that our work would open a wide range of opportunities to improve the performance of computation and memory access for large-scale graph processing."
Towards a RISC Framework for Efficient Contextualisation in the IoT.,"The Internet of Things (IoT) is a new internet evolution that involves connecting billions of internet-connected devices that we refer to as IoT things. These devices can communicate directly and intelligently over the Internet, and generate a massive amount of data that needs to be consumed by a variety of IoT applications. This paper focuses on the automatic contextualisation of IoT data, which also involves distilling information and knowledge from the IoT aiming to simplify answering the following fundamental questions that often arises in IoT applications: Which data collected by IoT are relevant to myself and the IoT Things I care for? Related work around context management and contextualisation ranges from database techniques that involve query re-writing, to semantic web and rule-based context management approaches, to machine learning and data science-based solutions in mobile and ambient computing. All such existing approaches have two main aspects in common: They are highly incompatible and horribly inefficient from a scalability and performance perspective. In this paper, we discuss a new RISC Contextualisation Framework (RCF) we have developed, implemented key aspects of, and assess its scalability. RCF provides fundamental contextualisation concepts that can be mapped to all existing contextualisation approaches for IoT data (and in this sense, it provides a common denominator that unifies the contextualisation space). RCF can be easily implemented as a cloud-based service, and provides better scalability and performance that any of the existing content management and contextualisation approaches in the IoT space."
The Future of the Semantic Web: Prototypes on a Global Distributed Filesystem.,"An important part of the Semantic Web vision is the idea that data is shared seamlessly and that world wide distributed, accessible, and interlinked knowledge bases can be created. However, the current incarnation of the Semantic Web falls short of this vision: while some necessary infrastructure (e.g., Linked Data) has been put in place, the current use of Linked Data in the Semantic Web is still happening in data silos, and sharing and reusing of knowledge is cumbersome and not straightforward. Recently the idea of prototypical objects was proposed to remedy this situation. This concept, known as Prototypes originates from early Frame systems and is also adopted in programming languages such as Javascript. In this vision paper we describe how a distributed file system forms a natural habitat for prototype knowledge representation, advancing the Semantic Web. In particular, we describe how we envision the deployment of Linked Data and Prototype Knowledge bases atop of the InterPlanetary File System (IPFS), which has several useful features matching the needs for knowledge representation based on prototypes."
On Broad Big Data.,"A broad data system explicitly represents a large number of concepts and properties together with its corresponding data proper. We observe the characteristics of several broad data systems and elicit three challenges we will need to research when scaling these broad data systems to become big data systems, too."
SRLB: The Power of Choices in Load Balancing with Segment Routing.,"Network load-balancers generally either do not take application state into account, or do so at the cost of a centralized monitoring system. This paper introduces a load-balancer running exclusively within the IP forwarding plane, i.e. in an application protocol agnostic fashion - yet which still provides application-awareness and makes real-time, decentralized decisions. To that end, IPv6 Segment Routing is used to direct data packets from a new flow through a chain of candidate servers, until one decides to accept the connection, based on its local state. This way, applications themselves naturally decide on how to share incoming connections, while incurring minimal network overhead, and no out-of-band signaling. Tests on different workloads - including realistic workloads such as replaying actual Wikipedia access traffic towards a set of replica Wikipedia instances - show significant performance benefits, in terms of shorter response times, when compared to a traditional random load-balancer."
Improving Efficiency of Link Clustering on Multi-core Machines.,"Link clustering groups different edges in a graph according to their similarities. Link clustering can reveal the overlapping and hierarchical organizations in a wide spectrum of networks. This work studies how to improve efficiency of link clustering along three dimensions, algorithm, modeling, and parallelization, on multi-core machines. We evaluate the efficiency improved due to each of the three dimensions using word association graphs extracted from a twitter dataset."
S3: Joint Scheduling and Source Selection for Background Traffic in Erasure-Coded Storage.,"Erasure-coded storage systems have gained considerable adoption recently since they can provide the same level of reliability with significantly lower storage overhead compared to replicated systems. However, background traffic of such systems - e.g. repair, rebalance, backup and recovery traffic - often has large volume and consumes significant network resources. Independently scheduling such tasks and selecting their sources can easily create interference among data flows, causing severe deadline violation. We show that the well-known heuristic scheduling algorithms fail to consider important constraints, thus resulting in unsatisfactory performance. In this paper, we claim that an optimal scheduling algorithm that aims to maximizethe number of background tasks completed before deadlines must simultaneously consider deadline-aware scheduling, network topology, chunk placement, and time-varying resource availability. To solve this problem, we propose a novel algorithm, called Linear Programming for Selected Tasks (LPST) to maximize the number of successful tasks and improve overallutilization of the datacenter network. It jointly schedules tasks and selects their sources based on a notion of Remaining Time Flexibility, which measures the slackness of the starting time of a task. We evaluated the efficacy of our algorithm using extensive simulations. Our results show that, under certain scenarios, LPST can perform 7x~70x better than the heuristics which blindly treat the infrastructure as a collection of homogeneous resources, and 46.6%~65.9% better than the algorithms that take into accountthe network topology."
On the Feasibility of Inter-Domain Routing via a Small Broker Set.,"The current inter-domain routing protocol, namely, the Border Gateway Protocol (BGP), cannot provide end-to-end (E2E) quality-of-service (QoS) guarantees. The main reason is that an autonomous system (AS) can only receive guarantees from its first hop ASes via service level agreements (SLAs). But beyond the first hop, QoS along the path from source to destination AS is not within the source AS's control regime. In this paper, we investigate the feasibility of providing high QoS-guaranteed E2E transit services by utilizing a (small) set of ASes/IXPs to serve as ""brokers"" to provide supervision, control and resource negotiation. Finding an optimal set of ASes as brokers can be formulated as a Maximum Coverage with B-dominating path Guarantee (MCBG) problem, which we prove to be NP-hard. To address this problem, we design a (1-e
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">-1</sup>
/4)-approximation algorithm and also an efficient heuristic algorithm when considering additional constraints (e.g., path length). Based on the current Internet topology, we discover a ""3540-alliance"" subset (accounting only 6.8%) of 52,079 ASes/IXPs, which can provide high QoS guarantees for 99.29% E2E connections."
Subscription Covering for Relevance-Based Filtering in Content-Based Publish/Subscribe Systems.,"Large-scale applications require a scalable data dissemination service with advanced filtering capabilities. We propose the use of a content-based publish/subscribe system with support for top-k filtering in the context of such applications. We focus on the problem of top-k subscription filtering, where a publication is delivered only to the k highest scoring subscribers. The naive approach to perform filtering early at the publisher edge works only if complete knowledge of the subscriptions is available, which is not compatible with the well-established covering optimization in scalable content-based publish/subscribe systems. We propose an efficient rank-cover technique to reconcile top-k subscription filtering with covering. We extend the covering model to support top-k and describe a novel algorithm for forwarding subscriptions to publishers while maintaining correctness. Finally, we compare our solutions to a baseline covering system. In a typical setting, our optimized solution is scalable and provides over 81% of the covering benefit."
Workflow Optimization in PAW.,"Many industrial applications, from domains such as telecommunication, web and sales, require to perform complex analytics across several data processing systems. The performance of such analytics is usually expressed in workflows, and it is a task that is both labor-intensive and time-consuming. At the same time, with increasing amounts of data to be analysed, the optimization of analytics workflows becomes crucial for satisfying business objectives. This paper focuses on workflow optimization with respect to time efficiency, over multiple execution engines, such as a traditional DBMS, a MapReduce engine, and a scripting engine. This configuration is emerging as a common paradigm used to combine analysis of unstructured and structured data. We propose a novel optimization technique as part of our system called PAW (Platform for Analytics Workflows). This technique creates alternative workflow structures and their execution plans based on equivalent combinations and orders of operators. The technique employs an exhaustive and a heuristic algorithm to search efficiently the space of equivalent workflow structures and select the one with the optimal execution plan. We present a thorough experimental study and we showcase the efficiency of the proposed optimization technique in a fully fledged multi-engine system, applied on three real-world applications and their data, as well as on a synthetic benchmark."
A First Look at Information Entropy-Based Data Pricing.,"Distribution of intangible information goods is experiencing tremendous growth in recent years, which has facilitated a blossoming of information goods economics. As big data develops, there are more and more information goods markets for data trading. In the current of data pricing policies in data trading, there are many metrics to measure the value of data goods, such as the data generation date, data volume, and data integrity, etc. However, it is very challenging to identify the amount of data information and its distribution, and the corresponding data pricing has rarely been discussed. In this paper, we propose a new data pricing metric, i.e., the data information entropy, which helps to make a reasonable price in the data trading. We first demonstrate a data information measurement method based on information entropy, and then propose a pricing function based on the result of data information measurement. To comprehensively understand the new data pricing metric and facilitate its application in data trading, we verify the rationality of the data information measurement method and give three concrete pricing functions. It is the first time to look at the information entropy-based data pricing, which can inspire the research concerning the pricing mechanism of data goods, further promoting the development of data products business."
Retrospective Lightweight Distributed Snapshots Using Loosely Synchronized Clocks.,"In order to take a consistent snapshot of a distributed system, it is necessary to collate and align local logs from each node to construct a pairwise concurrent cut. By leveraging NTP synchronized clocks, and augmenting them with logical clock causality information, Retroscope provides a lightweight solution for taking unplanned retrospective snapshots of past distributed system states. Instead of storing a multiversion copy of the entire system data, this is achieved efficiently by maintaining a configurable-size sliding window-log at each node to capture recent operations. In addition to retrospective snapshots, Retroscope also provides incremental and rolling snapshots that utilize an existing snapshot to reduce the cost of constructing a new snapshot in proximity. This capability is useful for performing stepwise debugging and root-cause analysis, and supporting data integrity monitoring and checkpoint-recovery. We implement Retroscope for the Voldemort distributed datastore and evaluate its performance under varying workloads."
Power-Aware Population Protocols.,"In this paper, we propose a formal energy model which allows an analytical study of energy consumption, for the first time in the context of population protocols (PP). In PP, anonymous and bounded memory agents move unpredictably and communicate in pairs. In order to illustrate the power and the usefulness of the proposed energy model, we develop a new power-aware protocol (EB-TTFM) for the task of data collection. The analytical results show that, in terms of energy consumption, EB-TTFM outperforms a known data collection protocol under certain conditions. Finally, we present a lower bound concerning energy consumption of any possible data collection protocol in PP, which also justifies the efficiency of EB-TTFM."
MultiPub: Latency and Cost-Aware Global-Scale Cloud Publish/Subscribe.,"Topic-based pub/sub is a widely used communication mechanism in distributed systems for targeted information dissemination between loosely coupled entities. To scale dynamically depending on the current communication demands, pub/services can be conveniently deployed in the cloud. To provide fast dissemination, the service can be distributed across multiple cloud regions. The architectural design and run-time deployment of such a middleware is tricky, though, as it can have a significant effect on communication latency and cloud-based cost. In this paper, we propose MultiPub, a flexible pub/sub middleware for latency-constrained, world-wide distributed applications that dynamically reconfigures the communication layer to ensure a predefined maximum latency for publication dissemination while minimizing cloud-based costs. This is achieved by routing publications either through a single or across multiple cloud regions. We demonstrate the effectiveness of MultiPub by presenting a set of experiments that report on the achieved communication latency and cost savings compared to traditional approaches, as well as a performance evaluation."
Reachability in Binary Multithreaded Programs Is Polynomial.,"Automatic finding of bugs in multithreaded programs is an important but inherently difficult task, even in the finite-state interleaving-semantics case. The complexity of this task has only been partially explored so far. We measure quantities such as the diameter, which is the longest finite distance realizable in the transition graph of the program, the local diameter, which is the maximum distance from any program state to any thread-local state, and the computational complexity of bugfinding. For the subclass of so-called binary multithreaded programs, we prove new bounds: all these quantities are majorized by a polynomial and, in certain cases, by a linear, logarithmic, or even constant function. Our bounds present a preparation step towards the corresponding polynomial-bound claims for general programs. These claims contrast sharply with the common belief that the main obstacle to analyzing concurrent programs is the exponential state explosion in the number of threads."
An Event-Level Abstraction for Achieving Efficiency and Fairness in Network Update.,"Changes of network state are a common source of instability in networks. An update event typically involves multiple flows that compete for network resources at the cost of rescheduling and migrating some existing flows. Previous network updating schemes tackle such flows independently, rather than as the entity of an update event. They only optimize the flow-level metrics for the flows involved in an update event. In this paper, we present an event-level abstraction of network update which groups flows of an update event and schedules them together to minimize the event completion time (ECT). We then study the scheduling problem of multiple update events for achieving high scheduling efficiency and preserving fairness. The designed least migration traffic first (LMTF) method schedules all update events in the FIFO order, but avoids head-of-line blocking by randomly fine-tuning the queue order of some events. It can considerably reduce the update cost, the average, and tail ECTs of all update events. In addition, we design a general parallel-LMTF (P-LMTF) method to guarantee fairness and further improve scheduling efficiency among update events. It improves the LMTF method by opportunistically updating multiple events simultaneously. The comprehensive evaluation results indicate that the average ECT of our approach is up to 10× faster than the flow-level scheduling method for network update events, and its tail ECT is up to 6x faster. Our P-LMTF method incurs 75% reduction in the average ECT compared with FIFO when the network utilization exceeds 70%, and it achieves a 42% reduction in tail ECT."
DCM: Dynamic Concurrency Management for Scaling n-Tier Applications in Cloud.,"Scaling web applications such as e-commerce in cloud by adding or removing servers in the system is an important practice to handle workload variations, with the goal of achieving both high quality of service (QoS) and high resource efficiency. Through extensive scaling experiments of an n-tier application benchmark (RUBBoS), we have observed that scaling only hardware resources without appropriate adaptation of soft resource allocations (e.g., thread or connection pool size) of each server would cause significant performance degradation of the overall system by either under- or over-utilizing the bottleneck resource in the system. We develop a dynamic concurrency management (DCM) framework which integrates soft resource allocations into the system scaling management. DCM introduces a model which determines a near-optimal concurrency setting to each tier of the system based on a combination of operational queuing laws and online analysis of fine-grained measurement data. We implement DCM as a two-level actuator which scales both hardware and soft resources in an n-tier system on the fly without interrupting the runtime system performance. Our experimental results demonstrate that DCM can achieve significantly more stable performance and higher resource efficiency compared to the state-of-the-art hardware-only scaling solutions (e.g., Amazon EC2-AutoScale) under realistic bursty workload traces."
"More Peak, Less Differentiation: Towards A Pricing-aware Online Control Framework for Inter-Datacenter Transfers.","The emerging deployment of geographically distributed data centers (DCs) incurs a significant amount of data transfers over the Internet. Such transfers are typically charged by Internet Service Providers (ISPs) with the widely adopted q-th percentile charging model. In such charging model, the time slots with top 100-q percent of data transmission do not affect the total transmission cost, and can be viewed as free. This brings the opportunity to optimize the scheduling of inter-DC transfers to minimize the entire transmission cost. However, very little work has been done to exploit those free time slots for scheduling inter-DC transfers. The crux is that existing work either lacks a mechanism to accumulate traffic to free time slots, or inevitably relies on prior knowledge of traffic arrival patterns. In this paper, we attempt to exploit those free time slots by leveraging diverse time-sensitivities among inter-DC transfers, so as to reduce or even minimize the transmission cost. Specifically, we advocate that a simple principle should be followed: more traffic peaks should be scheduled in free time slots, while less traffic differentiation should be maintained among the remaining time slots. To this end, we take advantage of the Lyapunov optimization techniques to design a pricing-aware control framework. This framework efficiently makes online decisions for inter-DC transfers without requiring a prior knowledge of traffic arrivals. To verify our proposed framework, we conduct small-scale testbed implementation. The results show that our framework can realistically reduce the transmission cost by up to 19.38%."
Robust Multi-tenant Server Consolidation in the Cloud for Data Analytics Workloads.,"Server consolidation is the hosting of multiple tenants on a server machine. Given a sequence of data analytics tenant loads defined by the amount of resources that the tenants require and a service-level agreement (SLA) between the customer and the cloud service provider, significant cost savings can be achieved by consolidating multiple tenants. Since server machines can fail causing their tenants to become unavailable, service providers can place replicas of each tenant on multiple servers and reserve capacity to ensure that tenant failover will not result in overload on any remaining server. We present the CUBEFIT algorithm for server consolidation that reduces costs by utilizing fewer servers than existing approaches for data analytics workloads. Unlike existing consolidation algorithms, CUBEFIT can tolerate multiple server failures while ensuring that no server becomes overloaded. Through theoretical analysis and experimental evaluation, we show that CUBEFIT is superior to existing algorithms and produces near-optimal tenant allocation when the number of tenants is large. Through evaluation and deployment on a cluster of 73 machines as well as through simulation studies, we experimentally demonstrate the efficacy of CUBEFIT."
Flow-Aware Adaptive Pacing to Mitigate TCP Incast in Data Center Networks.,"In data center networks, many network-intensive applications leverage large fan-in and many-to-one communication to achieve high performance. However, the special traffic patterns, such as micro-burst and high concurrency, easily cause TCP Incast problem and seriously degrade the application performance. To address the TCP Incast problem, we first reveal theoretically and empirically that alleviating packet burstiness is much more effective in reducing the Incast probability than controlling the congestion window. Inspired by the findings and insights from our experimental observations, we further propose a general supporting scheme Adaptive Pacing (AP), which dynamically adjusts burstiness according to the flow concurrency without any change on switch. Another feature of AP is its broad applicability. We integrate AP transparently into different TCP protocols (i.e., DCTCP, L2DCT and D2TCP). Through a series of large-scale NS2 simulations, we show that AP significantly reduces the Incast probability across different TCP protocols and the network goodput can be increased consistently by on average 7x under severe congestion."
Real-Time Power Cycling in Video on Demand Data Centres Using Online Bayesian Prediction.,"Energy usage in data centres continues to be a major and growing concern as an increasing number of everyday services depend on these facilities. Research in this area has examined topics including power smoothing using batteries and deep learning to control cooling systems, in addition to optimisation techniques for the software running inside data centres. We present a novel real-time power-cycling architecture, supported by a media distribution approach and online prediction model, to automatically determine when servers are needed based on demand. We demonstrate with experimental evaluation that this approach can save up to 31% of server energy in a cluster. Our evaluation is conducted on typical rack mount servers in a data centre testbed and uses a recent real-world workload trace from the BBC iPlayer, an extremely popular video on demand service in the UK."
A Distributed Access Control System for Cloud Federations.,"Cloud federations are a new collaboration paradigm where organizations share data across their private cloud infrastructures. However, the adoption of cloud federations is hindered by federated organizations' concerns on potential risks of data leakage and data misuse. For cloud federations to be viable, federated organizations' privacy concerns should be alleviated by providing mechanisms that allow organizations to control which users from other federated organizations can access which data. We propose a novel identity and access management system for cloud federations. The system allows federated organizations to enforce attribute-based access control policies on their data in a privacy-preserving fashion. Users are granted access to federated data when their identity attributes match the policies, but without revealing their attributes to the federated organization owning data. The system also guarantees the integrity of the policy evaluation process by using block chain technology and Intel SGX trusted hardware. It uses block chain to ensure that users identity attributes and access control policies cannot be modified by a malicious user, while Intel SGX protects the integrity and confidentiality of the policy enforcement process. We present the access control protocol, the system architecture and discuss future extensions."
Voyager: Complete Container State Migration.,"Due to the small memory footprint and fast startup times offerred by container virtualization, made ever more popular by the Docker platform, containers are seeing rapid adoption as a foundational capability to build PaaS and SaaS clouds. For such container clouds, which are fundamentally different from VM clouds, various cloud management services need to be revisited. In this paper, we present our Voyager - just-in-time live container migration service, designed in accordance with the Open Container Initiative (OCI) principles. Voyager is a novel filesystem-agnostic and vendor-agnostic migration service that provides consistent full-system migration. Voyager combines CRIU-based memory migration together with the data federation capabilities of union mounts to minimize migration downtime. With a union view of data between the source and target hosts, Voyager containers can resume operation instantly on the target host, while performing disk state transfer lazily in the background."
Keddah: Capturing Hadoop Network Behaviour.,"As a distributed system, Hadoop heavily relies on the network to complete data processing jobs. While Hadoop traffic is perceived to be critical for job execution performance, the actual behaviour of Hadoop network traffic is still poorly understood. This lack of understanding greatly complicates research relying on Hadoop workloads. In this paper, we explore Hadoop traffic through experimentation. We analyse the generated traffic of multiple types of MapReduce jobs, with varying input sizes, and cluster configuration parameters. As a result, we present Keddah, a toolchain for capturing, modelling and reproducing Hadoop traffic, for use with network simulators. Keddah can be used to create empirical Hadoop traffic models, enabling reproducible Hadoop research in more realistic scenarios."
A Scalable and Distributed Approach for NFV Service Chain Cost Minimization.,"Network function virtualization (NFV) represents the latest technology advancement in network service provisioning. Traditional hardware middleboxes are replaced by software programs running on industry standard servers and virtual machines, for service agility, flexibility, and cost reduction. NFV users are provisioned with service chains composed of virtual network functions (VNFs). A fundamental problem in NFV service chain provisioning is to satisfy user demands with minimum system-wide cost. We jointly consider two types of cost in this work: nodal resource cost and link delay cost, and formulate the service chain provisioning problem using nonlinear optimization. Through the method of auxiliary variables, we transform the optimization problem into its separable form, and then apply the alternating direction method of multipliers (ADMM) to design scalable and fully distributed solutions. Through simulation studies, we verify the convergence and efficacy of our distributed algorithm design."
Elastic Paxos: A Dynamic Atomic Multicast Protocol.,"Replication is a common technique used to design reliable distributed systems by masking defective components. To cope with the requirements of modern Internet applications, replication protocols must allow for throughput scalability and dynamic reconfiguration, that is, on-demand replacement or provisioning of system resources. This paper describes Elastic Paxos, a new dynamic atomic multicast protocol that fulfills these requirements. Elastic Paxos allows to dynamically add and remove resources to an online partially replicated state machine. We implemented Elastic Paxos and evaluated its performance in OpenStack, a cloud environment. We demonstrate its practicality to dynamically scale up and down a partially replicated data store with itand to reconfigure a distributed system."
Boosting The Benefits of Hybrid SDN.,"The upgrade of a legacy network to a full software-defined networking (SDN) deployment is usually an incremental process, during which SDN switches and legacy switches coexist in the hybrid network. However, with inappropriate deployment of SDN switches and design of hybrid control, the advantages of SDN control could not exert, and it even results in performance degradation or inconsistency (e.g., loops, black-holes). Therefore, the hybrid SDN requires considerable coordination of the centralized control and distributed routing. In this paper, we propose a solution to handle the heterogeneity caused by distinct forwarding characteristics of SDN and legacy switches, therefore boosting the benefits of hybrid SDN. We plan SDN placement to enhance the SDN controllability over the hybrid network, and conduct traffic engineering considering both the forwarding characteristics of SDN and legacy switches. The experiments with various topologies show that the SDN placement planning and hybrid forwarding yield better network performance especially in the early 70% SDN deployment."
Adopting SDN Switch Buffer: Benefits Analysis and Mechanism Design.,"One critical issue in SDN is to reduce the communication overhead between the switches and the controller. Such overhead is mainly caused by handling miss-match packets, because for each miss-match packet, a switch will send a request to the controller asking for forwarding rule. Existing approaches to address this problem generally need to deploy intermediate proxy or authority switches to hold rule copies, so as to reduce the number of requests sent to the controller. In this paper, we argue that using the intrinsic buffer in a SDN switch can also greatly reduce the communication overhead without using additional devices. If a switch buffers each miss-match packet, only a few header fields instead of the entire packet are required to be sent to the controller. Experiment results show that this can reduce 78.7% control traffic and 37% controller overhead at the cost of increasing only 5.6% switch overhead on average. If the proposed flow-granularity buffer mechanism is adopted, only one request message needs to be sent to the controller for a new flow with many arrival packets. Thus the control traffic and controller overhead can be further reduced by 64% and 35.7% respectively on average without increasing the switch overhead."
IoT SENTINEL: Automated Device-Type Identification for Security Enforcement in IoT.,"With the rapid growth of the Internet-of-Things (IoT), concerns about the security of IoT devices have become prominent. Several vendors are producing IP-connected devices for home and small office networks that often suffer from flawed security designs and implementations. They also tend to lack mechanisms for firmware updates or patches that can help eliminate security vulnerabilities. Securing networks where the presence of such vulnerable devices is given, requires a brownfield approach: applying necessary protection measures within the network so that potentially vulnerable devices can coexist without endangering the security of other devices in the same network. In this paper, we present IoT Sentinel, a system capable of automatically identifying the types of devices being connected to an IoT network and enabling enforcement of rules for constraining the communications of vulnerable devices so as to minimize damage resulting from their compromise. We show that IoT Sentinel is effective in identifying device types and has minimal performance overhead."
Efficient Z-Order Encoding Based Multi-Modal Data Compression in WSNs.,"Wireless sensor networks have significant limitationsin available bandwidth and energy. The limited bandwidthin sensor networks can cause higher message delivery latencyin applications such as monitoring poisonous gas leak. In suchapplications, there are multi-modal sensors whose values such astemperature, gas concentration, location and CO2 level need tobe transmitted together for faster detection and timely assessmentof gas leak. In this paper, we propose novel Z-order based datacompression schemes (Z-compression) to reduce energy and savebandwidth without increasing the message delivery latency. Insteadof using the popular Huffman tree style based encoding, Zcompressionuses Z-order encoding to map the multidimensionalsensing data into one-dimensional binary stream transmittedusing a single packet. Our experimental evaluations using realworlddata sets show that Z-compression has a much bettercompression ratio, energy saving, streaming rate than knownschemes like LEC (and adaptive LEC), FELACS and TinyPackfor multi-modal sensor data."
PTrack: Enhancing the Applicability of Pedestrian Tracking with Wearables.,"The ability to accurately track pedestrians is valuable for variant application designs. Although pedestrian tracking has been investigated excessively and owned a well-suited sensing platform, the proposed solutions are far from being mature yet. Pedestrian tracking contains step counting and stride estimation two components. Step counting already has commercial products, but the performance is still unreliable and less trustworthy in practice. Stride estimation even stays in the research stage without ready solutions released on the market. Such a non-negligible gap between long-term research investigation and technique's actual usage exists due to a series of crucial applicability issues unsolved, including design vulnerability to interfering activities, extracting purely body's movement from additive sensor signals, and parameter training without user's intervention. In this paper, we deeply analyze human's gait cycles and obtain inspiring observations to address these issues. We incorporate our techniques into existing pedestrian tracking designs and implement a prototype, PTrack, on LG smartwatch. We find PTrack effectively enhances the system applicability and achieves promising performance under very practical settings."
Source Location Privacy-Aware Data Aggregation Scheduling for Wireless Sensor Networks.,"Source location privacy (SLP) is an important property for the class of asset monitoring problems in wireless sensor networks (WSNs). SLP aims to prevent an attacker from finding a valuable asset when a WSN node is broadcasting information due to the detection of the asset. Most SLP techniques focus at the routing level, with typically high message overhead. The objective of this paper is to investigate the novel problem of developing a TDMA MAC schedule that can provide SLP. We make a number of important contributions: (i) we develop a novel formalisation of a class of eavesdropping attackers and provide novel formalisations of SLP-aware data aggregation schedules (DAS), (ii) we present a decision procedure to verify whether a DAS schedule is SLP-aware, that returns a counterexample if the schedule is not, similar to model checking, and (iii) we develop a 3-stage distributed algorithm that transforms an initial DAS algorithm into a corresponding SLP-aware schedule against a specific class of eavesdroppers. Our simulation results show that the resulting SLP-aware DAS protocol reduces the capture ratio by 50% at the expense of negligable message overhead."
Velocity Optimization of Pure Electric Vehicles with Traffic Dynamics Consideration.,"As Electric Vehicles (EVs) become increasingly popular, their battery-related problems (e.g., short driving range and heavy battery weight) must be resolved as soon as possible. Velocity optimization of EVs to minimize energy consumption in driving is an effective alternative to handle these problems. However, previous velocity optimization methods assume that vehicles will pass through traffic lights immediately at green traffic signals. Actually, a vehicle may still experience a delay to pass a green traffic light due to a vehicle waiting queue in front of the traffic light. In this paper, for the first time, we propose a velocity optimization system which enables EVs to immediately pass green traffic lights without delay. We collected real driving data on a 4.0 km long road section of US-25 highway to conduct extensive trace-driven simulation studies. The experimental results from Matlab and Simulation for Urban MObility (SUMO) traffic simulator show that our velocity optimization system reduces energy consumption by up to 17.5% compared with real driving patterns without increasing trip time."
PIANO: Proximity-Based User Authentication on Voice-Powered Internet-of-Things Devices.,"Voice is envisioned to be a popular way for humans to interact with Internet-of-Things (IoT) devices. We propose a proximity-based user authentication method (called PIANO) for access control on such voice-powered IoT devices. PIANO leverages the built-in speaker, microphone, and Bluetooth that voice-powered IoT devices often already have. Specifically, we assume that a user carries a personal voice-powered device (e.g., smartphone, smartwatch, or smartglass), which serves as the user's identity. When another voice-powered IoT device of the user requires authentication, PIANO estimates the distance between the two devices by playing and detecting certain acoustic signals; PIANO grants access if the estimated distance is no larger than a user-selected threshold. We implemented a proof-of-concept prototype of PIANO. Through theoretical and empirical evaluations, we find that PIANO is secure, reliable, personalizable, and efficient."
Category Information Collection in RFID Systems.,"In RFID-enabled applications, when a tag is put into use and associated with a specific object, the category-related information (e.g., the brands of clothes) about this object might be preloaded into the tag's memory as required. Since such information reflects the category attributes, all tags in the same category carry the identical category information. To collect this information, we do not need to repeatedly interrogate each tag; one tag's response in a category is sufficient. In this paper, we investigate the new problem of category information collection in a multi-category RFID system, which is referred to as information sampling. We propose an efficient two-phase sampling protocol (TPS). By quickly zooming into a category and isolating a tag from this category, TPS is able to sample a category by broadcasting only 7.5-bit polling vector (very efficient when compared to the 96-bit tag ID). We theoretically analyze the protocol performance and discuss the optimal parameter settings that minimize the overall execution time. Extensive simulations show that TPS outperforms the benchmark, greatly improving the sampling performance."
Scalable Role-Based Data Disclosure Control for the Internet of Things.,"The Internet of Things (IoT) is the latest Internet evolution that interconnects billions of devices, such as cameras, sensors, RFIDs, smart phones, wearable devices, ODBII dongles, etc. Federations of such IoT devices (or things) provides the information needed to solve many important problems that have been too difficult to harness before. Despite these great benefits, privacy in IoT remains a great concern, in particular when the number of things increases. This presses the need for the development of highly scalable and computationally efficient mechanisms to prevent unauthorised access and disclosure of sensitive information generated by things. In this paper, we address this need by proposing a lightweight, yet highly scalable, data obfuscation technique. For this purpose, a digital watermarking technique is used to control perturbation of sensitive data that enables legitimate users to de-obfuscate perturbed data. To enhance the scalability of our solution, we also introduce a contextualisation service that achieve real-time aggregation and filtering of IoT data for large number of designated users. We, then, assess the effectiveness of the proposed technique by considering a health-care scenario that involves data streamed from various wearable and stationary sensors capturing health data, such as heart-rate and blood pressure. An analysis of the experimental results that illustrate the unconstrained scalability of our technique concludes the paper."
Multi-representation Based Data Processing Architecture for IoT Applications.,"Internet of Things (IoT) applications like smart cars, smart cities and wearables are becoming widespread and are the future of the Internet. One of the major challenges for IoT applications is efficiently processing, storing and analyzing the continuous stream of incoming data from a large number of connected sensors. We propose a multi-representation based data processing architecture for IoT applications. The data is stored in multiple representations, like rows, columns, graphs which provides support for diverse application demands. A unifying update mechanism based on deterministic scheduling is used to update the data representations, which completely removes the need for data transfer pipelines like ETL (Extract, Transform and Load). The combination of multiple representations, and the deterministic update mechanism, provides the ability to support real-time analytics and caters to IoT applications by minimizing the latency of operations like computing pre-defined aggregates."
Long Term Sensing via Battery Health Adaptation.,"Energy Neutral Operation (ENO) has created the ability to continuously operate wireless sensor networks in areas such as environmental monitoring, hazard detection and industrial IoT applications. Current ENO approaches utilise techniques such as sample rate control, adaptive duty cycling and data reduction methods to balance energy generation, storage and consumption. However, the state of the art approaches makes a strong and unrealistic assumption that battery capacity is fixed throughout the deployment time of an application. This results in scenarios where ENO systems over allocate sensing tasks, therefore as battery capacity degrades it causes the system to no longer be energy neutral and then fail unexpectedly. In this paper, we formulate the problem to maximise the quality-of-service in terms of duty cycle and the battery capacity to extend the deployment lifetime of a sensing application. In addition, we develop a lightweight algorithm to solve the formulated problem. Moreover, we evaluate the proposed method using real sensor energy consumption data captured from micro-climate sensors deployed in Queen Elizabeth Olympic Park, London. Results show that a 307% extension of deployment lifetime can be achieved when compared to a traditional ENO solution without a reduction in the duty cycle of the sensor."
Detecting Time Synchronization Attacks in Cyber-Physical Systems with Machine Learning Techniques.,"Recently, researchers found a new type of attacks, called time synchronization attack (TS attack), in cyber-physical systems. Instead of modifying the measurements from the system, this attack only changes the time stamps of the measurements. Studies show that these attacks are realistic and practical. However, existing detection techniques, e.g. bad data detection (BDD) and machine learning methods, may not be able to catch these attacks. In this paper, we develop a ""first difference aware"" machine learning (FDML) classifier to detect this attack. The key concept behind our classifier is to use the feature of ""first difference"", borrowed from economics and statistics. Simulations on IEEE 14-bus system with real data from NYISO have shown that our FDML classifier can effectively detect both TS attacks and other cyber attacks."
Speed-Based Location Tracking in Usage-Based Automotive Insurance.,"Usage-based Insurance (UBI) is regarded as a promising way to offer more accurate insurance premium by profiling driving behaviors. Compared with traditional insurance which considers drivers' history of accidents, traffic violations and etc, UBI focuses on driving data and can give a more reasonable insurance premium based on the current driving behaviors. Insurers use sensors in smartphone or vehicle to collect driving data (e.g. mileage, speed, hark braking) and compute a risk score based on these data to recalculate insurance premium. Many insurance programs, which are advertised as being privacy-preserving, do not directly use the GPS-based tracking, but it is not enough to protect driver's location privacy. In real world, many environment factors such as real-time traffic and traffic regulations can influence driving speed. These factors provide the side-channel information about the driving route, which can be exploited to infer the vehicle's trace. Based on the observation, we propose a novel speed based trajectory inference algorithm which can track drivers only with the speed data and original location. We implement the attack on a public dataset in New Jersey. The evaluation results show that the attacker can recover the route with a high successful rate."
On Efficient Offloading Control in Cloud Radio Access Network with Mobile Edge Computing.,"Cloud radio access network (C-RAN) and mobile edge computing (MEC) have emerged as promising candidates for the next generation access network techniques. Unfortunately, although MEC tries to utilize the highly distributed computing resources in close proximity to user equipments equipments (UE), C-RAN suggests to centralize the baseband processing units (BBU) deployed in radio access networks. To better understand and address such a conflict, this paper closely investigates the MEC task offloading control in C-RAN environments. In particular, we focus on perspective of matching problem. Our model smartly captures the unique features in both MEC and C-RAN with respect to communication and computation efficiency constraints. We divide the cross-layer optimization into the following three stages: (1) matching between remote radio heads (RRH) and UEs, (2) matching between BBUs and UEs, and (3) matching between mobile clones (MC) and UEs. By applying the Gale-Shapley Matching Theory in the duplex matching framework, we propose a multi-stage heuristic to minimize the refusal rate for user's task offloading requests. Trace-based simulation confirms that our solution can successfully achieve near-optimal performance in such a hybrid deployment."
Location Privacy in Mobile Edge Clouds.,"In this paper, we consider user location privacy in mobile edge clouds (MECs). MECs are small clouds deployed at the network edge to offer cloud services close to mobile users, and many solutions have been proposed to maximize service locality by migrating services to follow their users. Co-location of a user and his service, however, implies that a cyber eavesdropper observing service migrations between MECs can localize the user up to one MEC coverage area, which can be fairly small (e.g., a femtocell). We consider using chaff services to defend against such an eavesdropper, with focus on strategies to control the chaffs. Assuming the eavesdropper performs maximum likelihood (ML) detection, we consider both heuristic strategies that mimic the user's mobility and optimized strategies designed to minimize the detection or tracking accuracy. We show that a single chaff controlled by the optimal strategy can drive the eavesdropper's tracking accuracy to zero when the user's mobility is sufficiently random. The efficacy of our solutions is verified through extensive simulations."
Approximation Designs for Cooperative Relay Deployment in Wireless Networks.,"In this paper, we aim to maximize users' satisfaction by deploying limited number of relays in a target region to form a wireless relay network, and define the Deployment of Cooperative Relay (DoCR) problem, which is proved to be NP-complete. We first propose an O(δ log n) approximation algorithm that utilizes the algorithms for budget weighted Steiner tree problem with novel position weighting assignment. We further propose a heuristic method to solve the DoCR problem releasing potential location constraint. Our extensive experiments indicate that the algorithms we propose can significantly improve the total satisfaction of the network. Furthermore, we establish a testbed using USRP to showcase our designs in real scenarios. To the best of our knowledge, we are the first to propose approximation algorithm for relay placement problem to maximize user satisfaction, which has both theoretical and practical significance in the related area."
Dispersing Social Content in Mobile Crowd through Opportunistic Contacts.,"Crowdsourced content sharing has become a fast-growing activity in today's online social networks, where contents of interest are created by diverse source users and conveyed over the network as friends view and reshare. The rapid and boundless propagation in a mobile crowd however often creates severe bottlenecks on the server side and incurs significant energy and monetary costs on the mobile side, particularly with the still expensive 3G/4G cellular connections. This paper presents SoCrowd, a novel framework for large-scale content sharing in a mobile crowd by exploiting contacts, i.e., users happen to move close with such short range low power communications as WiFi and bluetooth being enabled. We formulate the scheduling problem for social content propagation in a mobile crowd with contacts, and present optimal dynamic programming solution, which further motivates a series of practical heuristics. The effectiveness of SoCrowd has been demonstrated by extensive simulations driven by realworld traces and datasets."
A Lightweight Recommendation Framework for Mobile User's Link Selection in Dense Network.,"With the proliferation of mobile devices and the development of communication technology, mobile devices have permeated every aspect of our daily lives. However, in dense network where large crowd of mobile devices try to access to the network simultaneously, the severe interference between mobile devices may incur a remarkable deterioration of the wireless communication quality. How to improve individual's experience in such scenario is a critical yet open problem. Inspired by the mobile device users' usage pattern as well as the characteristic of most wireless communication systems, we propose a framework offering uplink/downlink selection recommendation to different mobile device users to enhance their utility in this paper. The design of the framework starts with formulating the problem as a link selection game. Analysis shows that the game can be categorized as a generalized ordinal potential game whose Nash Equilibrium is guaranteed. We then devise a distributed link selection algorithm to generate a Nash Equilibrium of the game. To accommodate to the characteristic of dense network and the capacity limitation of mobile device, the design of the algorithm shows a light-weight property and does not require each mobile device user to know others' current selection. The probability of incomplete information gathering is also considered. Extensive experiments are conducted to demonstrate the effectiveness and superiority of the proposed framework. Experimental results show that the global average utility increase rate reaches above 20%, and about 70% mobile device users can benefit from using our framework."
Making Smartphone Smart on Demand for Longer Battery Life.,"A major concern for today's smartphones is their much faster battery drain than traditional feature phones, despite their greater battery capacities. The difference is mainly contributed by those more powerful but also much more power-consuming smartphone components, such as the multi-core application processor. While the application processor must be active when any smart apps are being used, it is also unnecessarily waken up, even during idle periods, to perform operations related to basic phone functions (i.e., incoming calls and text messages). In this paper, we investigate how to increase the battery life of smartphones by minimizing the use of the application processor during idle periods. We find that the application processor is often waken up by a process running on it, called the Radio Interface Layer Daemon (RILD), which interfaces the user and apps to the GSM/LTE cellular network. In particular, we demonstrate that a great amount of energy could be saved if RILD is stopped, such that the application processor can sleep more often. Based on this key finding, we design a Smart On Demand (SOD) configuration that reduces smartphone idle energy consumption by running RILD operations on a secondary low-power microcontroller. As a result, RILD operations can be handled at much lower energy costs and the application processor is waken up only when one needs to use any smart apps, in an on-demand manner. We have built a hardware prototype of SOD. Our results show that SOD can reduce the energy consumption by up to 42%."
FADEWICH: Fast Deauthentication Over the Wireless Channel.,"Both authentication and deauthentication are instrumental for preventing unauthorized access to computers and other resources. While there are obvious motivating factors for using strong authentication mechanisms, convincing users to deauthenticate is not straight-forward, since deauthentication is not considered mandatory. A user who leaves a logged-in workstation unattended (especially for a short time) is typically not inconvenienced in any way; in fact, the other way around - no annoying reauthentication is needed upon return. However, an unattended workstation is trivially susceptible to the well-known ""lunchtime attack"" by any nearby adversary who simply takes over the departed user's log-in session. At the same time, since deauthentication does not intrinsically require user secrets, it can, in principle, be made unobtrusive. To this end, this paper designs the first automatic user deauthentication system - FADEWICH - that does not rely on biometric-or behavior-based techniques (e.g., keystroke dynamics) and does not require users to carry any devices. It uses physical properties of wireless signals and the effect of human bodies on their propagation. To assess FADEWICH's feasibility and performance, extensive experiments were conducted with its prototype. Results show that it suffices to have nine inexpensive wireless sensors deployed in a shared office setting to correctly deauthenticate all users within six seconds (90% within four seconds) after they leave their workstation's vicinity. We considered two realistic scenarios where the adversary attempts to subvert FADEWICH and showed that lunchtime attacks fail."
Cognitive Wireless Charger: Sensing-Based Real-Time Frequency Control For Near-Field Wireless Charging.,"A recent increase in mobile and IoT devices has led to the advancement of wireless charging. The state-of-the-art wireless charging systems operate at a particular frequency, controlled by the explicit networking from the power-receiving device (which relays the battery status information, useful for the frequency selection), but such control is not designed to cope with the variations in the power receiving device's placements and alignments (which are more significant in near-field and pseudo-tightly coupled charging applications, as more charging pads are being deployed in the public domains and serving heterogeneous clients). In this work, we analyze the impact of the power transfer performance caused by the power receiver's load, distance, and coil alignment/overlap and introduce cognitive wireless charger (CWC), which adaptively controls the operating frequency in real-time using implicit feedback from sensing for optimal operations. In addition to the theoretical and LTSpice-based simulation analysis, we build a prototype compatible to the Qi standard and analyze the performance of CWC with it. Through our analyses, we establish that frequency control achieves performance gains in inductive-coupling charging applications and is sensitive to the variations in the placement and alignment between the power-transmitting and the power-receiving coils. Our prototype, when CWC is turned off, has comparable performance to the commercial-grade Qi wireless chargers and, with CWC enabled, demonstrates significant improvement over modern wireless chargers."
Density and Mobility-Driven Evaluation of Broadcast Algorithms for MANETs.,"Broadcast is a fundamental operation in Mobile Ad-Hoc Networks (MANETs). A large variety of broadcast algorithms have been proposed. They differ in the way message forwarding between nodes is controlled, and in the level of information about the topology that this control requires. Deployment scenarios for MANETs vary widely, in particular in terms of nodes density and mobility. The choice of an algorithm depends on its expected coverage and energy cost, which are both impacted by the deployment context. In this work, we are interested in the comprehensive comparison of the costs and effectiveness of broadcast algorithms for MANETs depending on target environmental conditions. We describe the results of an experimental study of five algorithms, representative of the main design alternatives. Our study reveals that the best algorithm for a given situation, such as a high density and a stable network, is not necessarily the most appropriate for a different situation such as a sparse and mobile network. We identify the algorithms characteristics that are correlated with these differences and discuss the pros and cons of each design."
Energy-Aware CPU Frequency Scaling for Mobile Video Streaming.,"The energy consumed by video streaming includes the energy consumed for data transmission and CPU processing, which are both affected by the CPU frequency. High CPU frequency can reduce the data transmission time but it consumes more CPU energy. Low CPU frequency reduces the CPU energy but increases the data transmission time and then increases the energy consumption. In this paper, we aim to reduce the total energy of mobile video streaming by adaptively adjusting the CPU frequency. Based on real measurement results, we model the effects of CPU frequency on TCP throughput and system power. Based on these models, we propose an Energy-aware CPU Frequency Scaling (EFS) algorithm which selects the CPU frequency that can achieve a balance between saving the data transmission energy and CPU energy. Since the downloading schedule of existing video streaming apps is not optimized in terms of energy, we also propose a method to determine when and how much data to download. Through trace-driven simulations and real measurement, we demonstrate that the EFS algorithm can reduce 30% of energy for the Youtube app, and the combination of our download method and EFS algorithm can save 50% of energy than the default Youtube app."
Crazy Crowd Sourcing to Mitigate Resource Scarcity.,"Resource scarcity prohibits developing country population in many ways from ubiquitous services. One such service is providing information about the best route for an ambulance in a crisis situation due to lack of proper road network information and GPS data. We have worked on a routing method in Dhaka, Bangladesh and utilized the power of crowd sourcing in times of resource scarcity. We share our challenges and opportunities that opened up followed by the challenges in this paper."
Detecting Rogue AP with the Crowd Wisdom.,"WiFi networks are vulnerable to rogue AP attacks in which an attacker sets up an imposter AP to lure mobile users to connect. The attacker can eavesdrop on the communication, severely threatening users' privacy. Existing rogue AP detection solutions are confined to some specific attack scenarios (e.g., by relaying the traffic to a target AP) or require additional hardware. In this paper, we propose a crowdsensing based approach, named CRAD, to detect rogue APs in camouflage without specialized hardware requirement. CRAD exploits the spatial correlation of RSS to identify a potential imposter, which should be at a different location from the legitimate one. The RSS measurements collected from the crowd facilitate a robust profile and minimize the inaccuracy effect of a single RSS value. As a result, CRAD can filter out abnormal samples sensed in the realtime by dynamically matching the profile. We evaluate our approach with both a public dataset and a real prototype. The results show that CRAD can yield 90% detection accuracy and precision with proper crowd presence, even when the rogue AP is launched close to the legitimate one (e.g., within 1m)."
Towards Multilingual Automated Classification Systems.,"In this paper we propose and evaluate three approaches for automated classification of texts in over 60 languages without the need for a manually annotated dataset in those languages. All approaches are based on the randomized Explicit Semantic Analysis method using multilingual Wikipedia articles as their knowledge repository. We evaluate the proposed approaches by classifying a Twitter dataset in English and Portuguese into relevant and irrelevant items with respect to landslide as a natural disaster, where the highest achieved F1-score is 0.93. These approaches can be used in various applications where multilingual classification is needed, including multilingual disaster reporting using Social Media to improve coverage and increase confidence. As illustration, we present a demonstration that combines data from physical sensors and social networks to detect landslide events reported in English and Portuguese."
The Joint Effects of Tweet Content Similarity and Tweet Interactions for Topic Derivation.,"Interactions among tweets, i.e., mentions, retweets, replies, are important factors contributing to the quality of topic derivation on Twitter. If applied correctly, the incorporation of tweet interactions can significantly improve the quality of topic derivation in comparison with approaches that are mainly based on the content similarity analysis. However, how interactions can be measured and integrated with content similarity for topic derivation remains a challenge. In previous work, the strength of tweet-to-tweet relationship has been computed by simply adding measures for content similarity, mentions, and reply-retweets. This simple linear addition does not accurately reflect the various impacts these factors have on tweet relationships. In order to address this issue, we propose a joint probability model that can effectively integrate the effects of the content similarity, mentions, and reply-retweets to measure the tweet relationship for the purpose of topic derivation. The proposed method is based on matrix factorization techniques, which enables a flexible implementation on a distributed system in an incremental manner. Experimental results show that the proposed model results in a significant improvement in the quality of topic derivation over existing methods."
Timed-Release of Self-Emerging Data Using Distributed Hash Tables.,"Releasing private data to the future is a challenging problem. Making private data accessible at a future point in time requires mechanisms to keep data secure and undiscovered so that protected data is not available prior to the legitimate release time and the data appears automatically at the expected release time. In this paper, we develop new mechanisms to support self-emerging data storage that securely hide keys of encrypted data in a Distributed Hash Table (DHT) network that makes the encryption keys automatically appear at the predetermined release time so that the protected encrypted private data can be decrypted at the release time. We show that a straight-forward approach of privately storing keys in a DHT is prone to a number of attacks that could either make the hidden data appear before the prescribed release time (release-ahead attack) or destroy the hidden data altogether (drop attack). We develop a suite of self-emerging key routing mechanisms for securely storing and routing encryption keys in the DHT. We show that the proposed scheme is resilient to both release-ahead attack and drop attack as well as to attacks that arise due to traditional churn issues in DHT networks. Our experimental evaluation demonstrates the performance of the proposed schemes in terms of attack resilience and churn resilience."
Caching for Pattern Matching Queries in Time Evolving Graphs: Challenges and Approaches.,"Pattern matching is an important class of problems related to graphs. It is a fundamental problem for many applications and has been extensively studied in literature. With the advent of huge graphs, the challenges in this domain have increased manifold. Consequently a lot of recent research has led to new architectures and approaches for optimized solutions to the pattern matching problem. A vast majority of these graphs hardly remain static and constantly evolve over time (like social networks, web graphs, etc). Recently, caching has been studied in the context of static graphs to optimize the throughput of query processing systems. In this paper, we list the challenges in caching in the context of Time Evolving Graphs (TEGs). Amongst others, one major challenge is consistency which entails to making sure the cache is consistent with the streaming changes. We propose an approach to successfully implement caching that addresses those issues and based on the initial results, we see significant gains in the overall performance of system."
GraphA: Adaptive Partitioning for Natural Graphs.,"Large-scale graph computation is central to applications ranging from language processing to social networks. However, natural graphs tend to have skewed power-law distributions where a small subset of the vertices have a large number of neighbors. Existing graph-parallel systems suffer from load imbalance, high communication cost, or suboptimal and complex processing. In this paper we present GraphA, an Adaptive approach to efficient partitioning and computation of large-scale natural graphs. GraphA provides an adaptive and uniform graph partitioning algorithm, which partitions the datasets in a load-balanced manner by using an incremental number of hash functions. We have implemented GraphA both on Spark and on GraphLab. Extensive evaluation shows that GraphA remarkably outperforms state-of-the-art graph-parallel systems (GraphX and PowerLyra) in ingress time, execution time and storage overhead, for both real-worldand synthetic graphs."
Parallel Algorithm for Core Maintenance in Dynamic Graphs.,"This paper initiates the studies of parallel algorithm for core maintenance in dynamic graphs. The core number is a fundamental index reflecting the cohesiveness of a graph, which is widely used in large-scale graph analytics. We investigate the parallelism in the core update process when multiple edges and vertices are inserted. Specifically, we discover a structure called superior edge set, the insertion of edges in which can be processed in parallel. Based on the structure of superior edge set, an efficient parallel algorithm is then devised. To the best of our knowledge, the proposed algorithm is the first parallel one for the fundamental core maintenance problem. Finally, extensive experiments are conducted on different types of real-world and synthetic datasets, and the results illustrate the efficiency, stability and scalability of the proposed algorithm. The algorithm shows a significant speedup in the processing time compared with previous results that sequentially handle edge and vertex insertions."
DHCRF: A Distributed Conditional Random Field Algorithm on a Heterogeneous CPU-GPU Cluster for Big Data.,"As one of the most recognized models in machine learning, the conditional random fields (CRF) has been widely used in many applications. As the parameter estimation of CRF is highly time-consuming, how to improve the performance of CRF has received significant attention, in particular in the big data environment. To deal with large-scale data, CPU-based or GPU-based parallelization solutions have been proposed to improve performance. However, the problem is an ongoing one. In this paper, we focus on the big data environment and propose a distributed CRF on a heterogeneous CPU-GPU cluster called DHCRF. Our approach differs from previous work. Specifically, it leverages a three-stage heterogeneous Map and Reduce operation to improve the performance, making full use of CPU-GPU collaborative computing capabilities in a big data environment. Furthermore, by combining elastic data partition and intermediate results multiplexing method, the distributed CRF is optimized. Elastic data partition is performed to keep the load balanced, and the intermediate results multiplexing method is adopted to reduce data communication. Experimental results show that the DHCRF outperforms the baseline CRF algorithm and the CPU-based parallel CRF algorithm with notable performance improvement while maintaining competitive correctness at the same time."
Towards New Abstractions for Implementing Quorum-Based Systems.,"This paper introduces Gorums, a novel RPC framework for building fault tolerant distributed systems. Gorums offers a flexible and simple quorum call abstraction, used to communicate with a set of processes, and to collect and process their responses. Gorums provides separate abstractions for (a) selecting processes for a quorum call and (b) processing replies. These abstractions simplify the main control flow of protocol implementations, especially for quorum-based systems, where only a subset of the replies to a quorum call need to be processed. To show that Gorums can be used in practical systems, we implemented EPaxos' latency-efficient quorum system, and ran experiments using a key-value storage. Our results show that Gorums' abstractions can provide additional performance benefits to EPaxos."
Selective Traffic Offloading on the Fly: A Machine Learning Approach.,"It has been well recognized that network transmission constitutes a large portion of smartphone energy consumption, mainly because of the tail energy caused by cellular network interface. Traffic offloading has been proposed to reduce energy by letting a smartphone offload network traffic to its neighbors in vicinity via low-power direct connections (e.g., WiFi Direct or Bluetooth). Our experiments conducted in a realistic environment reveal that energy efficiency cannot be improved or even deteriorates without a carefully designed offloading strategy. In this paper, we propose a selective traffic offloading scheme implemented as a smartphone middleware in a software-defined fashion, which consists of a packet classifier and a traffic scheduler. Using a light-weight machine learning approach exploiting unique smartphone context information, the packet classifier identifies packets generated on the fly as offloadable or not with substantially improved efficiency and feasibility on resource limited smartphones compared to traditional approaches. Both testbed and simulation based experiments are conducted and the results show that our proposal always attains the superior performance on a number of comparison metrics."
A Fast Heuristic Attribute Reduction Algorithm Using Spark.,"Energy data, which consists of energy consumption statistics and other related data in green data centers, grows dramatically. The energy data has great value, but many attributes within it are redundant and unnecessary. Thus attribute reduction for the energy data has been conceived as a critical step. However, many existing attribute reduction algorithms are often computationally time-consuming. To address these issues, we extend the methodology of rough sets to construct data center energy consumption knowledge representation system. By taking good advantage of in-memory computing, an attribute reduction algorithm for energy data using Spark is proposed. In this algorithm, we use a heuristic formula for measuring the significance of attribute to reduce search space, and an efficient algorithm for simplifying energy consumption decision table, which further improve the computation efficiency. The experimental results show the speed of our algorithm gains up to 0.28X performance improvement over the traditional attribute reduction algorithm using Spark."
Profiling Users by Modeling Web Transactions.,"Users of electronic devices, e.g., laptop, smartphone, etc. have characteristic behaviors while surfing the Web. Profiling this behavior can help identify the person using a given device. In this paper, we introduce a technique to profile users based on their web transactions. We compute several features extracted from a sequence of web transactions and use them with one-class classification techniques to profile a user. We assess the efficacy and speed of our method at differentiating 25 synthetic users on a benchmark dataset (from a major security vendor) representing 6 months of web traffic monitoring from a small enterprise network."
JeCache: Just-Enough Data Caching with Just-in-Time Prefetching for Big Data Applications.,"Big data clusters introduce an intermediate cache layer between the computing frameworks and the underlying distributed file systems, to enable upper-level applications or end users to efficiently access big datasets in cache and effectively share them among different computing frameworks. As caches are shared by multiple applications or end users, directly applying existing on-demand caching strategies will result in intense conflicts, when big datasets are cached as a whole. Meanwhile, big data applications usually involve massive numbers of file scans, cached-in data blocks may have little chance of being accessed before they are cached out to make way for other on-demand data blocks. Thus, it is unwise to cache data blocks long before they are actually accessed. In this paper, we propose a novel just-enough big data caching scheme for just-in-time block prefetching to improve the cache effectiveness of big data clusters. With just-in-time block prefetching, a block is cached in just before the task begins to process the block, rather than being cached in along with other blocks of the same dataset being processed. We monitor block accesses to measure the average processing time of data blocks, and then estimate the minimal number of blocks that should be kept in cache for a big dataset, so that the speed of data processing matches with that of data prefetching, and each upper-level task can obtain its input blocks from cache just in time. Our experimental results show that the proposed cache method can restrain over-requirement of cache resources in big data applications, and provides the same performance improvement as when all data blocks are cached."
Proximity Awareness Approach to Enhance Propagation Delay on the Bitcoin Peer-to-Peer Network.,"In the Bitcoin system, a peer-to-peer electronic currency system, the delay overhead in transaction verification prevents the Bitcoin from gaining increasing popularity nowadays as it makes the system vulnerable to double spend attacks. This paper introduces a proximity-aware extension to the current Bitcoin protocol, named Bitcoin Clustering Based Ping Time protocol (BCBPT). The ultimate purpose of the proposed protocol, that is based on how the clusters are formulated and the nodes define their membership, is to improve the transaction propagation delay in the Bitcoin network. In BCBPT, the proximity of connectivity in the Bitcoin network is increased by grouping Bitcoin nodes based on ping latencies between nodes. We show, through simulations, that the proximity base ping latency defines better clustering structures that optimize the performance of the transaction propagation delay. The reduction of the communication link cost measured by the information propagation time between nodes is mainly considered as a key reason for this improvement. Bitcoin Clustering Based Ping Time protocol is more effective at reducing the transaction propagation delay compared to the existing clustering protocol (LBC) that we proposed in our previous work."
Catch Me If You Can: Detecting Compromised Users Through Partial Observation on Networks.,"People are suffering from a range of risks in the ubiquitous networks of current world, such as rumours spreading in social networks, computer viruses propagating throughout the Internet and unexpected failures happened in Smart grids. We usually monitor only a few users of detecting various risks due to the resource constraints and privacy protection. This leads to a critical problem to detect compromised users who are out of surveillance. In this paper, we propose a risk assessment method to address this problem. The aim is to assess the security status of unmonitored users according to the limited information collected from monitored users in networks. There are two innovative techniques developed: First, we identify the source of risk propagation by inversely disseminating risks from the influenced (by rumours) or infected (by viruses) monitored users. We show a new finding that the ones who synchronously receive the risk copies from all monitored users are most likely to be the sources. Second, we propose a microscopic mathematical model to present the risk propagation from the exposed sources. This model forms a discriminant to classify the compromised users from others. For evaluations, we collect three real networks on which we launch simulated risk propagation and then sample the status of monitored users. The experiment results show that our method is effective and the result of risk assessment well matches the real status of the unmonitored users."
Location Privacy Breach: Apps Are Watching You in Background.,"Smartphone users can conveniently install a set of apps that provide Location Based Service (LBS) from markets. These LBS-based apps facilitate users in many application scenarios, but they raise concerns on the breach of privacy related to location access. Smartphone users can hardly perceive location access, especially when it happens in background. In comparison to location access in foreground, location access in background could result in more serious privacy breach because it can continuously know a user's locations. In this paper, we study the problem of location access in background, and especially perform the first measurement of this background action on the Google app market. Our investigation demonstrates that many popular apps conduct location access in background within short intervals. This enables these apps to collect a user's location trace, from which the important personal information, Points of Interest (PoIs), can be recognized. We further extract a user's movement pattern from the PoIs, and utilize it to measure the extent of privacy breach. The measurement results also show that using the combination of movement pattern related metrics and the other PoI related metrics can help detect the privacy breach in an earlier manner than using either one of them alone."
Android Malware Detection Using Complex-Flows.,"This paper proposes a new technique to detect mobile malware based on information flow analysis. Our approach examines the structure of information flows to identify patterns of behavior present in them and which flows are related, those that share partial computation paths. We call such flows Complex-Flows, as their structure, patterns, and relations accurately capture the complex behavior exhibited by both recent malware and benign applications. N-gram analysis is used to identify unique and common behavioral patterns present in Complex-Flows. The N-gram analysis is performed on sequences of API calls that occur along Complex-Flows' control flow paths. We show the precision of our technique by applying it to different data sets totaling 7,798 apps. These data sets consist of both recent and older generation benign and malicious apps to demonstrate the effectiveness of our approach across different generations of apps."
Privacy Implications of DNSSEC Look-Aside Validation.,"To complement DNSSEC operations, DNSSEC Look-aside Validation (DLV) is designed for alternative off-path validation. While DNS privacy attracts a lot of attention, the privacy implications of DLV are not fully investigated and understood. In this paper, we take a first in-depth look into DLV, highlighting its lax specifications and privacy implications. By performing extensive experiments over datasets of domain names under comprehensive experimental settings, our findings firmly confirm the privacy leakages caused by DLV. We discover that a large number of domains that should not be sent to DLV servers are being leaked. We explore the root causes, including the lax specifications of DLV. We also propose two approaches to fix the privacy leakages. Our approaches require trivial modifications to the existing DNS standards and we demonstrate their cost in terms of latency and communication."
FlipNet: Modeling Covert and Persistent Attacks on Networked Resources.,"Persistent and zero-day attacks have increased considerably in the recent past in terms of scale and impact. Security experts can no longer rely only on known defenses and thereby protect their resources permanently. It is increasingly common now to observe attackers being able to repeatedly break systems exploiting new vulnerabilities and defenders hardening systems with new measures. To model this phenomenon of the repeated takeover of the computing resources by system administrators and malicious attackers, a novel game framework, FlipIt, has been proposed by (Van Dijk et al. 2013) for a system consisting of a single resource. In this paper, we extend this and develop FlipNet, which is a repeated game framework for a networked system of multiple resources. This game involves two players-a defender and an attacker. Each player's objective is to maximize its gain (i.e., its control over the nodes in the network with stealthy moves), while minimizing the cost for making those moves. This leads to a novel and natural game formulation, with a very complex strategy space, that depends on the network structure. We show that finding the best response strategy for both the defender and attacker is NP-hard. In a key result in this study, we show that the attacker's gain for an instance of the game has a type of diminishing marginal return property, which leads to a near-optimal algorithm for maximizingthe attacker's gain. We examine the impact of network structure on the strategy space using simulations."
Understanding the Market-Level and Network-Level Behaviors of the Android Malware Ecosystem.,"The prevalence of malware in Android marketplaces is a growing and significant problem. Most existing studies focus on detecting Android malware or designing new security extensions to defend against specific types of attacks. In this paper, we perform an empirical study on analyzing the market-level and network-level behaviors of the Android malware ecosystem. We focus on studying whether there are interesting characteristics of those market accounts that distribute malware and specific networks that are mainly utilized by Android malware authors. We further investigate community patterns among Android malware from the perspective of their market account infrastructure and remote server infrastructure. Spurred by these analysis, we design a novel community inference algorithm to find more malicious apps by exploiting their community relationships. By using a small seed set (50) of known malicious apps, we can effectively find another extra 20 times of malicious apps, while maintaining considerable accuracy higher than 94%."
EnGarde: Mutually-Trusted Inspection of SGX Enclaves.,"Intel's SGX architecture allows cloud clients to create enclaves, whose contents are cryptographically protected by the hardware even from the cloud provider. While this feature protects the confidentiality and integrity of the client's enclave content, it also means that enclave content is completely opaque to the cloud provider. Thus, the cloud provider is unable to enforce policy compliance on enclaves. In this paper, we introduce EnGarde, a system that allows cloud providers to ensure SLA compliance on enclave content. In EnGarde, cloud providers and clients mutually agree upon a set of policies that the client's enclave content must satisfy. EnGarde executes when the client provisions the enclave, ensuring that only policy-compliant content is loaded into the enclave. EnGarde is able to achieve its goals without compromising the security guarantees offered by the SGX, and imposes no runtime overhead on the execution of enclave code. We have demonstrated the utility of EnGarde by using it to enforce a variety of security policies on enclave content."
Truthful Online Auction for Cloud Instance Subletting.,"Despite that IaaS users are busy scaling up/out their cloud instances to meet the ever-increasing demands, the dynamics of their demands, as well as the coarse-grained billing options offered by leading cloud providers, have led to substantial instance underutilization in both temporal and spatial domains. This paper theoretically examines an instance subletting service, where underutilized instances are leased to others within user-specified periods. Serving as a secondary market that complements the existing instance market of IaaS providers,we specifically identify the theoretical challenges in instance subletting services, and design an online auction mechanism to make allocation and pricing decisions for the instances to be sublet. Our mechanism guarantees truthfulness and individual rationality with the best possible competitive ratio. Extensive trace-driven simulations show that our proposed mechanism achieves significant performance gains in both cost and social welfare."
On the Power of Weaker Pairwise Interaction: Fault-Tolerant Simulation of Population Protocols.,"In this paper we investigate the computational power of population protocols under some unreliable or weaker interaction models. More precisely, we focus on two features related to the power of interactions: omission failures and one-way communications. We start our investigation by providing a complete classification of all the possible models arising from the aforementioned weaknesses, and establishing the computational hierarchy of these models. We then address for each model the fundamental question of what additional power is necessary and sufficient to completely overcome the model's weakness and make it able to simulate faultless two-way protocols. We answer this question by presenting simulators that work under certain assumptions and by proving that simulation is impossible without such assumptions."
Distributed Fault Tolerant Linear System Solvers Based on Erasure Coding.,"We present efficient coding schemes and distributed implementations of erasure coded linear system solvers. Erasure coded computations belong to the class of algorithmic fault tolerance schemes. They are based on augmenting an input dataset, executing the algorithm on the augmented dataset, and in the event of a fault, recovering the solution from the corresponding augmented solution. This process can be viewed as the computational analog of erasure coded storage schemes. The proposed technique has a number of important benefits: (i) as the hardware platform scales in size and number of faults, our scheme yields increasing improvement in resource utilization, compared to traditional schemes; (ii) the proposed scheme is easy to code - the core algorithms remain the same; and (iii) the general scheme is flexible - accommodating a range of computation and communication tradeoffs. We present new coding schemes for augmenting the input matrix that satisfy the recovery equations of erasure coding with high probability in the event of random failures. These coding schemes also minimize fill (non-zero elements introduced by the coding block), while being amenable to efficient partitioning across processing nodes. We demonstrate experimentally that our scheme adds minimal overhead for fault tolerance, yields excellent parallel efficiency and scalability, and is robust to different fault arrival models."
Preserving Incumbent Users' Privacy in Exclusion-Zone-Based Spectrum Access Systems.,"Dynamic spectrum access (DSA) technique has emerged as a fundamental approach to mitigate the spectrum scarcity problem. As a key form of DSA, the government is proposing to release more federal spectrum for sharing with commercial wireless users. However, the flourish of federal-commercial sharing hinges upon how the federal privacy is managed. In current DSA proposals, the sensitive exclusion zone (E-Zone) information of federal incumbent users (IUs) needs to be shared with a spectrum access system (SAS) to realize spectrum allocation. However, SAS is not necessarily trust-worthy for holding the sensitive IU E-Zone data, especially considering that FCC allows some industry third parties (e.g., Google) to operate SAS for better efficiency and scalability. Therefore, the current proposals dissatisfy the IUs' privacy requirement. To address the privacy issue, this paper presents an IU-privacy-preserving SAS (IP-SAS) design, which realizes the spectrum allocation process through secure computation over ciphertext based on homomorphic encryption so that none of the IU EZone information is exposed to SAS. This paper also proposes mechanisms to prevent malicious parties from compromising IP-SAS. We prove the privacy-preserving properties of IP-SAS and demonstrate the scalability and practicality of IP-SAS using experiments based on real-world data. Evaluation results show that IP-SAS can respond an SU's spectrum request in 1.25 seconds with communication overhead of 17.8 KB."
LITMUS: Towards Multilingual Reporting of Landslides.,"LITMUS is a real-time online and openly accessible service that collects high quality information on landslide events from social media. This service uses disaster related keywords, such as ""landslide"" and ""mudslide"", to analyze messages posted by English speaking users. However, comprehensive coverage of disasters must include multilingual support as there are events that are reported in languages other than English. We discuss and evaluate possible implementations of such support using ""native"" and ""translated"" approaches. ""Native"" approach involves a complete reimplementation of the existing infrastructure in another language whereas in the ""translated"" approach the existing infrastructure can be used without modification. As an illustration, we present a demo that extends LITMUS to implement a ""native"" approach for multilingual reporting of landslide events."
Pythia: A System for Online Topic Discovery of Social Media Posts.,"Social media constitute nowadays one of the most common communication mediums. Millions of users exploit them daily to share information with their community in the network via messages, referred as posts. The massive volume of information shared is extremely diverse and covers a vast spectrum of topics and interests. Automatically identifying the topics of the posts is of particular interest as this can assist in a variety of applications, such as event detection, trends discovery, expert finding etc. However, designing an automated system that requires no human agent participation to identify the topics covered in posts published in Online Social Networks (OSNs) presents manifold challenges. First, posts are unstructured and commonly short, limited to just a few characters. This prevents existing classification schemes to be directly applied in such cases, due to sparseness of the text. Second, new information emerges constantly, hence building a learning corpus from past posts may fail to capture the ever evolving information emerging in OSNs. To overcome the aforementioned limitations we have designed Pythia, an automated system for short text classification that exploits the Wikipedia structure and articles to identify the topics of the posts. The topic discovery is performed in two phases. In the first step, the system exploits Wikipedia categories and articles of the corresponding categories to build the training corpus for the suppervised learning. In the second step, the text of a given post is augmented using a text enrichment mechanism that extends the post with relevant Wikipedia articles. After the initial steps are performed, we deploy k-NN classifier to determine the topic(s) covered in the original post."
Data-Driven Serendipity Navigation in Urban Places.,"With the proliferation of mobile computing and the ability to collect detailed data for the urban environment a number of systems that aim at providing Points of Interest (POIs) and tour recommendations have appeared. The overwhelming majority of these systems aims at providing an optimal recommendation, where optimality refers to objectives of minimizing the distance to be covered or maximizing the quality of the POIs recommended. A major problem is that by focusing on the optimization of these objectives, there remains little room to the user for serendipity. Urban and social scientists have identified serendipity, i.e., the ability to come across unexpected places, as a feature that makes a city livable. In this work, we introduce a prototype of an experimental platform for evaluating venue recommendation algorithms by providing informative tour recommendations based on the suggested venues. Our prototype system integrates the notion of serendipity in urban navigation at both the venue as well as the route recommendation level without compromising the quality and diversity of the recommended POIs. In addition, our system allows the user to upload their own algorithms and explore their performance as compared to many well-known algorithms."
Toward an Integrated Approach to Localizing Failures in Community Water Networks (DEMO).,"We present a cyber-physical-human (CPHS) distributed computing framework, AquaSCALE, for gathering, analyzing and localizing anomalous operations of increasingly failure-prone community water services. Today, detection of water pipe leaks takes hours to days. AquaSCALE leverages dynamic data from multiple information sources including IoT (Internet of Things) sensing data, geophysical data, human input and simulation/modeling engines to create a sensor-simulation-data integration platform that can locate multiple simultaneous pipe failures at fine level of granularity with high level of accuracy and detection time reduced by orders of magnitude (from hours/days to minutes)."
PrivateGraph: A Cloud-Centric System for Spectral Analysis of Large Encrypted Graphs.,"Graph datasets have invaluable use in business applications and scientific research. Because of the growing size and dynamically changing nature of graphs, graph data owners may want to use public cloud infrastructures to store, process, and perform graph analytics. However, when outsourcing data and computation, data owners are at burden to develop methods to preserve data privacy and data ownership from curious cloud providers. This demonstration exhibits a prototype system for privacy-preserving spectral analysis framework for large graphs in public clouds (PrivateGraph) that allows data owners to collect graph data from data contributors, and store and conduct secure graph spectral analysis in the cloud with preserved privacy and ownership. This demo system lets its audience interactively learn the major cloud-client interaction protocols: the privacy-preserving data submission, the secure Lanczos and Nyström approximate eigen-decomposition algorithms that work over encrypted data, and the outcome of an important application of spectral analysis - spectral clustering. In the process of demonstration the audience will understand the intrinsic relationship amongst costs, result quality, privacy, and scalability of the framework."
IoT Sentinel Demo: Automated Device-Type Identification for Security Enforcement in IoT.,"The emergence of numerous new manufacturers producing devices for the Internet-of-Things (IoT) has given rise to new security concerns. Many IoT devices exhibit security flaws making them vulnerable for attacks and manufacturers have difficulties in providing appropriate security patches to their products in a timely and user-friendly manner. In this paper, we present our implementation of IoT Sentinel, which is a system aimed at protecting the user's network from vulnerable IoT devices. IoT Sentinel automatically identifies vulnerable devices when they are first introduced to the network and enforces appropriate traffic filtering rules to protect other devices from the threats originating from the vulnerable devices."
Rogue Access Point Detector Using Characteristics of Channel Overlapping in 802.11n.,"In this work, we introduce a powerful hardware-based rogue access point (PrAP), which can relay traffic between a legitimate AP and a wireless station back and forth, and act as a man-in-the-middle attacker. Our PrAP is built of two dedicated wireless routers interconnected physically, and can relay traffic rapidly between a station and a legitimate AP. Through extensive experiments, we demonstrate that the state-of-the-art time-based rogue AP (rAP) detectors cannot detect our PrAP, although effective against software-based rAP. To defend against PrAPs, we propose PrAP-Hunter based on intentional channel interference. PrAP-Hunter is highly accurate, even under heavy traffic scenarios. Using a high-performance (desktop) and low-performance (mobile) experimental setups of our PrAP-Hunter in various deployment scenarios, we demonstrate close to 100% of detection rate, compared to 60% detection rate by the state-of-the-art. We show that PrAP-Hunter is fast (takes 5-10 sec), does not require any prior knowledge, and can be deployed in the wild by real world experiments at 10 coffee shops."
ReverseCloak: A Reversible Multi-level Location Privacy Protection System.,"With the fast popularization of mobile devices and wireless networks, along with advances in sensing and positioning technology, we are witnessing a huge proliferation of Location-based Services (LBSs). Location anonymization refers to the process of perturbing the exact location of LBS users as a cloaking region such that a user's location becomes indistinguishable from the location of a set of other users. However, existing location anonymization techniques focus primarily on single level unidirectional anonymization, which fails to control the access to the cloaking data to let data requesters with different privileges get information with varying degrees of anonymity. In this demonstration, we present a toolkit for ReverseCloak, a location perturbation system to protect location privacy over road networks in a multi-level reversible manner, consisting of an `Anonymizer' GUI to adjust the anonymization settings and visualize the multilevel cloaking regions over road network for location data owners and a `De-anonymizer' GUI to de-anonymize the cloaking region and display the reduced region over road network for location data requesters. With the toolkit, we demonstrate the practicality and effectiveness of the ReverseCloak approach."
"Hopsworks: Improving User Experience and Development on Hadoop with Scalable, Strongly Consistent Metadata.","Hadoop is a popular system for storing, managing,and processing large volumes of data, but it has bare-bonesinternal support for metadata, as metadata is a bottleneck andless means more scalability. The result is a scalable platform withrudimentary access control that is neither user- nor developer-friendly. Also, metadata services that are built on Hadoop, suchas SQL-on-Hadoop, access control, data provenance, and datagovernance are necessarily implemented as eventually consistentservices, resulting in increased development effort and morebrittle software. In this paper, we present a new project-based multi-tenancymodel for Hadoop, built on a new distribution of Hadoopthat provides a distributed database backend for the HadoopDistributed Filesystem's (HDFS) metadata layer. We extendHadoop's metadata model to introduce projects, datasets, andproject-users as new core concepts that enable a user-friendly, UI-driven Hadoop experience. As our metadata service is backed bya transactional database, developers can easily extend metadataby adding new tables and ensure the strong consistency ofextended metadata using both transactions and foreign keys."
Isolation in Docker through Layer Encryption.,"Containers are constantly gaining ground in the virtualization landscape as a lightweight and efficient alternative to hypervisor-based Virtual Machines, with Docker being the most successful representative. Docker relies on union-capable file systems, where any action performed to a base image is captured as a new file system layer. This strategy allows developers to easily pack applications into Docker image layers and distribute them via public registries. However, this image creation and distribution strategy does not protect sensitive data from malicious privileged users (e.g., registry administrator, cloud provider), since encryption is not natively supported. We propose and demonstrate a mechanism for secure Docker image manipulation throughout its life cycle: The creation, storage and usage of a Docker image is backed by a data-at-rest mechanism, which maintains sensitive data encrypted on disk and encrypts/decrypts them on-the-fly in order to preserve their confidentiality at all times, while the distribution and migration of images is enhanced with a mechanism that encrypts only specific layers of the file system that need to remain confidential and ensures that only legitimate key holders can decrypt them and reconstruct the original image. Through a rich interaction with our system the audience will experience first-hand how sensitive image data can be safely distributed and remain encrypted at the storage device throughout the container's lifetime, bearing only a marginal performance overhead."
Dela - Sharing Large Datasets between Hadoop Clusters.,"Big data has, in recent years, revolutionised an ever-growing number of fields, from machine learning to climate science to genomics. The current state-of-the-art for storing large datasets is either object stores or distributed filesystems, with Hadoop being the dominant open-source platform for managing `Big Data'. Existing large-scale storage platforms, however, lack support for the efficient sharing of large datasets over the Internet. Those systems that are widely used for the dissemination of large files, like BitTorrent, need to be adapted to handle challenges such as network links with both high latency and high bandwidth, and scalable storage backends that are optimised for streaming and not random access. In this paper, we introduce Dela, a peer-to-peer data-sharing service integrated into the Hops Hadoop platform that provides an end-to-end solution for dataset sharing. Dela is designed for large-scale storage backends and data transfers that are both non-intrusive to existing TCP network traffic and provide higher network throughput than TCP on high latency, high bandwidth network links, such as transatlantic network links. Dela provides a pluggable storage layer, implementing two alternative ways for clients to access shared data: stream processing of data as it arrives with Kafka, and traditional offline access to data using the Hadoop Distributed Filesystem. Dela is the first step for the Hadoop platform towards creating an open dataset ecosystem that supports user-friendly publishing, searching, and downloading of large datasets."
In Vivo Evaluation of the Secure Opportunistic Schemes Middleware Using a Delay Tolerant Social Network.,"Over the past decade, online social networks (OSNs) such as Twitter and Facebook have thrived and experienced rapid growth to over 1 billion users. A major evolution would be to leverage the characteristics of OSNs to evaluate the effectiveness of the many routing schemes developed by the research community in real-world scenarios. In this demonstration, we showcase the Secure Opportunistic Schemes (SOS) middleware which allows different routing schemes to be easily implemented relieving the burden of security and connection establishment. The feasibility of creating a delay tolerant social network is demonstrated by using SOS to enable AlleyOop Social, a secure delay tolerant networking research platform that serves as a real-life mobile social networking application for iOS devices. AlleyOop Social allows users to interact, publish messages, and discover others that share common interests in an intermittent network using Bluetooth, peer-to-peer WiFi, and infrastructure WiFi."
Scaling and Load Testing Location-Based Publish and Subscribe.,"The rise of the Internet of things (IoT) poses massive scalability issues for location-based services. More particularly, location-aware publish and subscribe services are struggling to scale out the computation of matches between publications and subscriptions that continuously update their location. In this demonstration paper, we propose a novel distributed and horizontally scalable architecture for location-aware publish and subscribe. Our middleware architecture relies on a multi-step routing mechanism based on consistent hashing and range partitioning. To demonstrate its scalability, we present a traffic data generator, which, in contrast to existing generators, can be used to perform real-time load tests. Finally, we show that our architecture can be deployed on a small 10-node cluster and can process up to 80,000 location updates per second producing 25,000 matches per seconds."
A Distributed Event-Centric Collaborative Workflows Development System for IoT Application.,"The rapid development of Internet of Things (IoT) attracts growing attention from both industry and academia. IoT seamlessly connects the physical world and cyberspace via various sensors. It is more worth for us to pay attention to the mechanism of the events to work collaboratively rather than those standalone sensors. In this paper, we present a Distributed Event-centric Collaborative Workflows development system for IoT application, called DECW. It supports loosely coupled event-based interaction between processes, which enables real-time response to events from the physical world. Unlike traditional centralized control flow mode, the interaction between processes in DECW is constrained by the event interface. Users could dynamically adjust the interface between processes without modifying the internal logic of the process. In addition, DECW system provides a full lifecycle for the development and operation of the IoT application, including graphical creation of processes, dynamic definition of the process interaction interfaces, logical validation, distributed packaging and deployment, parallel execution, and real-time monitoring and managing the running status of the IoT application."
Incentive Mechanism for Data-Centric Message Delivery in Delay Tolerant Networks.,"A key issue in delay tolerant networks (DTN) is to find the right node to store and relay messages. We consider messages annotated with the unique keywords describing the message subject, and nodes also adds keywords to describe their mission interests, priority and their transient social relationship (TSR). To offset resource costs, an incentive mechanism is developed over transient social relationships which enrich enroute message content and motivate better semantically related nodes to carry and forward messages. The incentive mechanism ensures avoidance of congestion due to uncooperative or selfish behavior of nodes."
Performance of Cognitive Wireless Charger for Near-Field Wireless Charging.,"Wireless charging provides a convenient way to charge various mobile and IoT devices. Prior work in state-of-the-art wireless charging systems operates at a frequency controlled by explicit networking from the power-receiving devices and is designed for the environment when the participating devices are perfectly aligned with each other. The need for the finer control due to the devices' misalignment is increasing in near-field and pseudo-tightly coupled charging applications, as more charging pads, are being deployed in the public domains and serving heterogeneous clients. Because inductive-coupled charging applications are sensitive to the placement and alignment variations between the power-transmitting and the power-receiving coils, we design and build Cognitive Wireless Charger (CWC). CWC adaptively controls the operating frequency in real time using implicit feedback for optimal power transfer operations. This demo is to supplement our paper about CWC [1]. In this demo, we showcase the impact on power transfer performance caused by the variations in the placement and alignment between the charging coils of power transmitter and power receiver and demonstrate the performance improvement provided by CWC."
Toward Vehicle Sensing: An Integrated Application with Sparse Video Vameras and Intelligent Taxicabs.,"Due to the sparse distribution of road video surveillance cameras, precise trajectory tracking for vehicles remains a challenging task. To the best of our knowledge, none of the previous research considered using on-road taxicabs as mobile video surveillance cameras and road traffic flow patterns, therefore not suitable for recovering trajectories of vehicles. With this insight, we model the travel time-cost of a road segment during various time periods precisely with LNDs (Logarithmic Normal Distributions), then use LSNDs (Log Skew Normal Distributions) to approximate the time-cost of an urban trip during various time periods. We propose an approach to calculate possible location and time distribution of the vehicle, select the taxicab to verify the distribution by uploading and checking video clips of this taxicab, finally refine the restoring trajectory in a recursive manner. We evaluate our solution on real-world taxicab and road surveillance system datasets. Experimental results demonstrate that our approach outperforms alternative solutions in terms of accuracy ratio of vehicle tracking."
Segmentation of Time Series Based on Kinetic Characteristics for Storage Consumption Prediction.,"The Internet services generate huge amount of data, which require large space for storage. Determining device purchase plan turns out to be very important for the service providers. Under-purchasing might lead to data loss, while over-purchasing would result in waste. In this paper, we propose a linear regression based approach to predict the storage demand according to the time series of the storage consumption. We partitioned the storage con-sumption time series into several linear segments, and perform prediction on the last segment using linear regression. Since the position of turning points between adjacent segments and the total number of the segments are both unknown, how to achieve the online segmentation becomes a big challenge. Aiming to solve this problem, we carried out the Kalman-Anova segmentation method. Experiment results show that our method has good accuracy in precision, recall and F-measure values. Moreover, the method is able to segment nonlinear time series as well, suggesting a potential wider application. The proposed method has been deployed in Baidu Inc. and saves about 45 thousand dollars in one of its device purchase program."
A Multi-stage Hierarchical Window Model with Application to Real-Time Graph Analysis.,"The dynamic nature of real-world networks, such as social networks and communication networks, has increased the focus towards real-time dynamic graph analysis. Observations made based on real-time analysis of dynamic graphs reflect the latest properties of the graph and have the most value in real-time analysis. Computing graph properties of large-scale, fast-evolving graphs in real-time is challenging due, not only to the high computational and memory cost, but also to the understanding of the result with respect to the data from which it was derived. This paper proposes a multi-stage hierarchical window model that can aid in rigorous understanding of complicated real-time results and we apply it to generate graphs based on real-time updates along with periodic computations on graph snapshots for processing dynamic graphs. Moreover, the paper discusses the utilization of parallel window computation. The paper evaluates the hierarchical model through analyzing graphs formed by cooccurring hashtags in a Twitter data-stream."
Dynamic Pricing at Electric Vehicle Charging Stations for Queueing Delay Reduction.,"The research of electric vehicles (EVs) has gained more and more attention in recent years in both industry and academia, and new registrations of EVs increase rapidly, while the long delay at the convenient but crowded charging stations may discourage many drivers from switching to EVs. To address the problems, we propose a novel dynamic pricing policy that allows charging stations to adjust their service fees in real time based on the load at the stations. In our work, the selection of drivers is modeled by a new dissatisfaction function with multiple variables, which can be easily validated and improved by real applications, and our solution is evaluated from the real-world e-charge dataset. To the best of our knowledge, this is the first work that considers dynamic service fees among various charging stations for load balancing and reduction of queueing delay. This makes our work more realistic and beneficial."
Pairwise Ranking Aggregation by Non-interactive Crowdsourcing with Budget Constraints.,"Crowdsourced ranking algorithms ask the crowd to compare the objects and infer the full ranking based on the crowdsourced pairwise comparison results. In this paper, we consider the setting in which the task requester is equipped with a limited budget that can afford only a small number of pairwise comparisons. To make the problem more complicated, the crowd may return noisy comparison answers. We propose an approach to obtain a good-quality full ranking from a small number of pairwise preferences in two steps, namely task assignment and result inference. In the task assignment step, we generate pairwise comparison tasks that produce a full ranking with high probability. In the result inference step, based on the transitive property of pairwise comparisons and truth discovery, we design an efficient heuristic algorithm to find the best full ranking from the potentially conflictive pairwise preferences. The experiment results demonstrate the effectiveness and efficiency of our approach."
Buffer-Based Reinforcement Learning for Adaptive Streaming.,"Adaptive streaming improves user-perceived quality by altering the streaming bitrate depending on network conditions, trading reduced video bitrates for reduced stall times. Existing adaptation approaches, e.g., rate-based, buffer-based, either rely heavily on accurate bandwidth prediction or can be overly-conservative about video bitrates. In this work, we propose a reinforcement learning approach to choose the segment quality during playback. This approach uses only the buffer state information and optimizes for a measure of user-perceived streaming quality. Simulation results show that our proposed approach achieves better QoE than rate-, buffer-based approaches, as well as other reinforcement learning approaches."
The Case for Using Content-Centric Networking for Distributing High-Energy Physics Software.,"Named Data Networking (NDN) is one of the promising future internet architectures, which focuses on the data rather than its location (IP/host-based system). NDN has several characteristics which facilitate addressing and routing the data: fail-over, in-network caching and load balancing. This makes it useful in areas such as managing scientific data. The CMS experiment on the Large Hadron Collider (LHC) has a data access problem amenable to content-centric networking. CERN Virtual Machine File System (CVMFS) is used by High Energy Physics (HEP) community for worldwide software distribution. CVMFS maintain its data by using content-addressable storage, which makes it suitable for NDN. n this paper, we investigate the possibilities of using a content-centric networking architecture such as NDN on distributing CMS software."
LAVEA: Latency-Aware Video Analytics on Edge Computing Platform.,"We present LAVEA, a system built for edge computing, which offloads computation tasks between clients and edge nodes, collaborates nearby edge nodes, to provide low-latency video analytics at places closer to the users. We have utilized an edge-first design to minimize the response time, and compared various task placement schemes tailed for inter-edge collaboration. Our results reveal that the client-edge configuration has task speedup against local or client-cloud configurations."
Complete Tolerance Relation Based Filling Algorithm Using Spark.,"With the advent of cloud computing, renewable energy is integrated into data center power supply systems increasingly. The power statistics collection may not be available due to the instability of renewable energy, which results in incomplete data. The incomplete energy data will significantly disturb the management of data centers. We further propose a filling algorithm based on complete tolerance class. The algorithm expands the traditional tolerance relation, and fills the missing values of the energy data, which ensures the data integrity. By taking good advantage of in-Memory Computing, We further parallelize and optimize our algorithm using Spark. The experiment results demonstrate that our algorithm outperforms other general filling algorithms in terms of filling accuracy. The proposed algorithm also shows good performance as the missing rate rises up."
Towards Secure Public Directory for Privacy-Preserving Data Sharing.,"In the age of big data, information sharing paradigm has been fundamentally changed: Data collection and consumption are increasingly decentralized (partly due to the advent of personal computing devices), and sharing personal data over the Internet becomes a prominent paradigm for new applications. One of these applications is Health Information Exchange (or HIE) where a patient's electronic medical record (EMR) was produced at one hospital, and is consumed by a physician in another hospital."
Anonymous Routing to Maximize Delivery Rates in DTNs.,"In this paper, we seek to address anonymous communications in delay tolerant networks (DTNs). While many different approaches for the internet and ad hoc networks, to the best of our knowledge, only variants of onion-based routing have been tailored for DTNs. Since each type of anonymous routing protocol has its advantages and drawbacks, there is no single anonymous routing protocol for DTNs that can adapt to the different levels of security requirements. In this paper, we first design a set of anonymous routing protocols for DTNs, called anonymous epidemic and zone-based anonymous routing, based on the original anonymous routing protocols for ad hoc networks. Then, we propose a framework of anonymous routing (FAR) for DTNs, which subsumes all the aforementioned protocols. By tuning its parameters, the proposed FAR is able to outperform onion-based, anonymous Epidemic, and zone-based routing."
Evaluating Connection Resilience for the Overlay Network Kademlia.,"Kademlia is a decentralized overlay network, up to now mainly used for highly scalable file sharing applications. Due to its distributed nature, it is free from single points of failure. Communication can happen over redundant network paths, which makes information distribution with Kademlia resilient against failing nodes and attacks. In this paper, we simulate Kademlia networks with varying parameters and analyze the number of node-disjoint paths. With our results, we show the influence of these parameters on the network connectivity and, therefore, the resilience against failing nodes and communication channels."
Shortfall-Based Optimal Security Provisioning for Internet of Things.,"We present a formal method for computing the best security provisioning for Internet of Things (IoT) scenarios characterized by a high degree of mobility. The security infrastructure is intended as a security resource allocation plan, computed as the solution of an optimization problem that minimizes the risk of having IoT devices not monitored by any resource. We employ the shortfall as a risk measure, a concept mostly used in the economics, and adapt it to our scenario. We show how to compute and evaluate an allocation plan, and how such security solutions address the continuous topology changes that affect an IoT environment."
Group Differential Privacy-Preserving Disclosure of Multi-level Association Graphs.,"Traditional privacy-preserving data disclosure solutions have focused on protecting the privacy of individual's information with the assumption that all aggregate (statistical) information about individuals is safe for disclosure. Such schemes fail to support group privacy where aggregate information about a group of individuals may also be sensitive and users of the published data may have different levels of access privileges entitled to them. We propose the notion of 
<i xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">ε</i>
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">g</sub>
-Group Differential Privacy that protects sensitive information of groups of individuals at various defined privacy levels, enabling data users to obtain the level of access entitled to them. We present a preliminary evaluation of the proposed notion of group privacy through experiments on real association graph data that demonstrate the guarantees on group privacy on the disclosed data."
Tracking Information Flow in Cyber-Physical Systems.,"Cyber-Physical Systems are distributed, heterogeneous, decentralized and loosely coupled networks in which individual systems measure physical processes, exchange information, and influence processes. Sensors measure these physical processes, while aggregators process them and actuators perform resulting actions. Decisions are often based on sensor data collected by other systems. Furthermore, the aggregators also interchange information and use them to derive own decisions. Decisions must be comprehensible. However, this is only the case if all data dependencies are known. Due to the size of these networks, their loose coupling and their dynamic behavior, decisions made by a system are not always easy to understand. If an error occurs in the system, the error source must be identified. It must be known on which data a decision was based. However, since the decision can be based on information from other nodes, the search for the error source is not a trivial task. Keep in mind, that dependent nodes can have dependencies themselves as well. We present the Information Flow Monitor (IFM) that collects information about semantic data dependencies in dynamic networks. The collected dependency information is provided at a central network location. Subsequently, semantic dependencies between information can be visualized."
Privacy-Preserving Matchmaking in Geosocial Networks with Untrusted Servers.,"As a major branch of LBSs, geosocial networking services become popular. An important functionality of geosocial networking services is allowing people to find potential friends who have similar profile within close proximity and initiate communication with each other. However, in order to realize this functionality, most existing services require mobile users to reveal their profiles and location information to an untrusted service provider, which may expose LBSs to vulnerabilities for abuse and endanger mobile users' privacy. To address this problem, we propose to encrypt users' profile with a new searchable encryption scheme. Combining this searchable encryption scheme with other cryptographic techniques we construct a privacy- preserving matchmaking system. Compared with a previous one that aims to solve the same problem, ours is more secure, supports more flexible functionalities and moves computationally heavy key updates to resourceful service providers."
You've Been Tricked! A User Study of the Effectiveness of Typosquatting Techniques.,"The deceitful practice of Typosquatting involves deliberately registering Internet domain names containing typographical errors that primarily target popular domain names, in an effort to redirect users to unintended destinations or steal traffic for monetary gain. Typosquatting has existed for well over two decades and continues to be a credible threat to this day. While much of the prior work has examined various typosquatting techniques and how they change over time, none have considered how effective they are in deceiving users. In this paper, we attempt to fill in this gap by conducting a user study that exposes subjects to several uniform resource locators (URLs) in an attempt to determine the effectiveness of several typosquatting techniques that are prevalent in the wild. We also attempt to determine if the security education and awareness of cybercrimes such as typosquatting will affect the behavior of Internet users."
Real-Time Detection of Illegal File Transfers in the Cloud.,"There has been a prolific rise in the popularityof cloud storage in recent years. While cloud storage offersmany advantages such as flexibility and convenience, users arenow unable to tell or control the actual locations of their data. This limitation may affect users' confidence and trust in thestorage provider, or even be unsuitable for storing data withstrict location requirements. To address this issue, we proposean illegal file transfer detection framework that constantlymonitors the real-time file transfers in the cloud and is capableof detecting potential illegal transfers which moves sensitivedata outside the (""legal"") boundaries specified by the fileowner. The main idea is to classifying multiple users' location preferences when making the data storage arrangement inthe cloud nodes. We model the legal file transfers amongnodes as a weighted graph and then maximize the probabilityof storing data items of similar privacy preferences in thesame region. Then we leverage the socket monitoring functionsprovided by LAST-HDFS (a recent location-aware Hadoop filestorage system) to monitor the real-time communication amongcloud nodes. Based on our legal file transfer graph and thedetected communication, we propose an approach to calculatethe probability of the detected transfer to be illegal."
Eyes of the Swarm: Streamers' Detection in BT.,"Many BitTorrent (BT) clients are using these BT networks as a video-on-demand service, taking advantage of the popularity and the large collection of media available. However, transforming the swarms into an on-demand media service can cause serious damage to the overall network performance. In this paper, we propose a methodology, using the concepts of Entropy, and present a Spy BitTorrent client that is able to identify peers streaming in a swarm. Large scale monitoring, for real swarms, were performed to detect the presence of streamers."
Load Prediction for Energy-Aware Scheduling for Cloud Computing Platforms.,"We address online scheduling for servers of Cloud service providers. Each server is composed of several variable-speed processors whose power function is convex. The servers may be busy, idle or switched off. The objective of our scheduling is to minimize the energy consumed by a Cloud computing platform. To achieve this goal, we try to anticipate computing demands by predicting a workload, then we modify the set of available servers to fit this prediction and finally we schedule our jobs on the available servers. To schedule jobs we have developed the POD (Predict Optimize Dispatch) algorithm. We evaluate its performance for real-life traces in the presence of different types of prediction. The analysis shows that our scheduling reduces energy consumption considerably."
Learn-as-You-Go with Megh: Efficient Live Migration of Virtual Machines.,"We propose a reinforcement learning algorithm, Megh, for live migration of virtual machines that simultaneously reduces the cost of energy consumption and enhances the performance. Megh learns the uncertain dynamics of workloads as-it-goes. Megh uses a dimensionality reduction scheme to projectthe combinatorially explosive state-action space to a polynomial dimensional space. These schemes enable Megh to be scalable and to work in real-time. We experimentally validate that Megh is more cost-effective and time-efficient than the MadVM and MMT algorithms."
Machine-Learning Based Performance Estimation for Distributed Parallel Applications in Virtualized Heterogeneous Clusters.,"In a virtualized heterogeneous cluster, for a distributed parallel application which runs in multiple virtual machines (VMs) concurrently, there are a huge number of possible ways to place its VMs. This paper investigates a performance estimation technique for distributed parallel applications in virtualized heterogeneous clusters. We first analyze the effects of different VM configurations on the performance of various distributed parallel applications. We then present a machine-learning based performance model for a distributed parallel application. Using a heterogeneous cluster with two different types of nodes, we show that our machine-learning based models can estimate the runtimes of distributed parallel applications with modest error rates."
Incremental Elasticity for NoSQL Data Stores.,"Elasticity actions in NoSQL data stores move large amounts of data over the network to take advantage of new resources. Here we propose incremental elasticity, a new mechanism for scheduling data transfers to a joining server, leading to smoother elasticity actions with a reduced performance impact."
A Framework for Efficient Energy Scheduling of Spark Workloads.,"Nowadays distributed processing frameworks like Apache Spark have been successfully used for the execution of big data applications. Despite their wide adoption little work has been done in terms of controlling the applications' energy consumption. Datacenters contribute over 2 % of the total US electric usage therefore minimizing the energy utilization of Spark application can be extremely helpful. Solving this energy consumption problem requires the scheduling of Spark applications in an energy-efficient way. However, the problem is challenging as we also have to consider application performance requirements. In this work, we provide the overview of a novel framework that orchestrates the execution order of Spark applications, exploiting DVFS to tune the computing nodes CPU frequencies in order to minimize the energy consumption and satisfy application's performance requirements. Our early experimental results illustrate the working and benefits of our framework."
Towards a Complete Virtual Data Center Embedding Algorithm Using Hybrid Strategy.,"A Virtual Data Center (VDC) is a set of virtual machines (VMs) connected by a Virtual Network (VN) topology. Today's cloud data centers support dynamic requests for VDCs, by using software defined embedding strategies thatallow them to mesh multiple VDCs onto their Physical Data Center(PDC) network and machines. In this paper, we present a solution to this VDC embedding problem that achieves a higher acceptance rate by minimizing fragmentation, compared to existing strategies, while at the same time minimally disrupting the existing VDCs."
Federating Consistency for Partition-Prone Networks.,"Groups of strongly consistent devices can efficiently order events under ideal (data center) conditions, but become less effective in dynamic and heterogeneous environments. Weakly consistent devices efficiently tolerate both faults and dynamic conditions but are slow to converge on a single ordering of system events. We propose ""federated consistency"", which combines the strengths of both approaches into a single protocol. Federated groups use a strongly consistent inner core of devices to maintain a totally ordered, fault-tolerant sequence of events. A cloud of weakly-consistent devices disseminates orderings and enables progress despite varying connectivity and partitions. Though the constituent sub-protocols take different (nearly opposite) approaches to resolving conflicts; we show that expanding distributed version vectors with a forte component allows them to inter-operate effectively."
Mitigating Nesting-Agnostic Hypervisor Policies in Derivative Clouds.,"The fixed granularity of virtual machines offered by IaaS providers has prompted the evolution of derivative clouds where resources are repackaged into smaller containers and leased out typically in PaaS mode. In such a setup, containers are provisioned within virtual machines. Such a nested setup results in two control centers for the resources used by those containers-the guest OS and the Hypervisor. The latter's control actions are agnostic of the application executing within a VM. This lack of visibility may result in hypervisor control that has a non-uniform effect on the VM's nested containers which is undesirable. In this work, we propose policy based control of the effect of the hypervisor's control actions amongst the containers nested in the affected VM."
A Novel Architecture for Efficient Fog to Cloud Data Management in Smart Cities.,"Traditional smart city resources management rely on cloud based solutions to provide a centralized and rich set of open data. The advantages of cloud based frameworks are their ubiquity, (almost) unlimited resources capacity, cost efficiency, as well as elasticity. However, accessing data from the cloud implies large network traffic, high data latencies, and higher security risks. Alternatively, fog computing emerges as a promising technology to absorb these inconveniences. The use of devices at the edge provides closer computing facilities, reduces network traffic and latencies, and improves security. We have defined a new framework for data management in the context of smart city through a global fog to cloud management architecture; in this paper we present the data acquisition block. As a first experiment we estimate the network traffic during data collection, and compare it with a traditional real system. We also show the effectiveness of some basic data aggregation techniques in the model, such as redundant data elimination and data compression."
Networklet: Concept and Deployment.,"In today's datacenters, resource requests from tenants are increasingly transforming into hybrid requests that may simultaneously demand IaaS, Paas, and SaaS resources. This paper tackles the challenge of modeling and deploying hybrid tenant requests in datacenters, for which we coin ""networklet"" to represent a set of VMs that collaboratively provide a PaaS or SaaS service. Through extracting networklets from tenant requests and thus sharing them between tenants, we can achieve a win-win situation for datacenter providers and tenants."
Optimistic Causal Consistency for Geo-Replicated Key-Value Stores.,"In this paper we present a new approach to implementing causal consistency in geo-replicated data stores, which we call Optimistic Causal Consistency (OCC). The optimism in our approach lies in that updates from a remote data center are immediately made visible in the local data center, without checking if their causal dependencies have been received. Servers perform the dependency check needed to enforce causal consistency only upon serving a client operation, rather than on the receipt of a replicated data item as in existing systems. OCC explores a novel trade-off in the landscape of causal consistency protocols. The potentially blocking behavior of OCC makes it vulnerable to network partitions. Because network partitions are rare in practice, however, OCC chooses to trade availability to maximize data freshness and reduce the communication overhead. We further propose a recovery mechanism that allows an OCC system to fall back on a pessimistic protocol to continue operating even during network partitions. POCC is an implementation of OCC based on physical clocks. We show that OCC improves data freshness, while offering comparable or better performance than its pessimistic counterpart."
Automated Performance Evaluation for Multi-tier Cloud Service Systems Subject to Mixed Workloads.,"In multi-tier cloud service systems, performance evaluation relies on numerous experiments in order to collect key metrics such as resources usage. The approach may result in highly time-consuming in practice. In this paper, we propose an automated framework for performance tracking, data management and analysis to minimize human intervention in multi-tier cloud service systems. The framework support fine-grained analysis of the mixed workloads through the Discrete-time Markov-modulated Poisson process (DMMPP). A general multi-tier application is theoretically formulated as a queueing network to evaluate the performance. The effectiveness of the model has been validated through extensive experiments conducted in the RUBiS benchmark system."
Decentralised Runtime Monitoring for Access Control Systems in Cloud Federations.,"Cloud federation is an emergent cloud-computing paradigm where partner organisations share data and services hosted on their own cloud platforms. In this context, it is crucial to enforce access control policies that satisfy data protection and privacy requirements of partner organisations. However, due to the distributed nature of cloud federations, the access control system alone does not guarantee that its deployed components cannot be circumvented while processing access requests. In order to promote accountability and reliability of a distributed access control system, we present a decentralised runtime monitoring architecture based on blockchain technology."
DuoFS: An Attempt at Energy-Saving and Retaining Reliability of Storage Systems.,"As issues of the Energy Wall and the Reliability Wall become unavoidable, it is a demanding and challenging task to reduce energy consumption in large-scale storage systems in modern data centres while retaining acceptable systems reliability. Most energy conservation techniques inevitably have adverse impacts on the parallel disk systems. To address the reliability issues of energy-efficient parallel storage systems, we propose a reliable energy-efficient storage system called DuoFS, which aims at improving both energy efficiency and reliability of parallel storage systems by seamlessly integrating HDDs and SSDs. With the help of the middleware layer, DuoFS can distribute popular data to SSD-based nodes and put HDD-based nodes into the low-power mode under light workload conditions without modification of the parallel systems."
A Proposal of an Efficient Traffic Matrix Estimation Under Packet Drops.,"Traffic matrix (TM) estimation has been extensively studied for decades. Although conventional estimation techniques assume that traffic volumes are unchanged between origins and destinations, packets are often discarded on a path due to traffic burstiness, silent failures, etc. This paper proposes a novel TM estimation method that works correctly even under packet drops. The method is established on a Boolean fault localization technique; the technique requires fewer counters though it only determines whether each link is healthy. This paper extends the Boolean technique so as to deal with traffic volumes with error bounds just by a small number of counters. Along with submodular optimization for the minimum counter placement, we evaluate our method with real network datasets."
Straggler Mitigation for Distributed Behavioral Simulation.,"Running large-scale behavioral simulations requires high computational power, which can be acquired by distributing computation workload to multiple computing nodes (i.e., workers) that run in parallel. The implementations of such systems commonly follow the Bulk Synchronous Parallel (BSP) model. However, implementations using BSP usually suffer from the straggler problem, where the delay of any worker slows down the entire simulation. The problem usually occurs due to communication delays or imbalanced workload among workers. To mitigate the straggler problem, we propose a novel parallel computational model, called Priority Synchronous Parallel (PSP) model. PSP exploits data dependencies of parallel processes to determine high priority data to be computed and synchronized while computing the remaining data. PSP is implemented and evaluated using traffic simulations for three large cities. The proposed technique shows significant performance improvements over the BSP model."
Supporting Resource Control for Actor Systems in Akka.,"Although there are models and prototype implementations for controlling resource use in Actor systems, they are difficult to implement for production implementations of Actors such as Akka. This is because the messaging and scheduling infrastructures of runtime systems are increasingly complex and significantly different from one system to another. This paper presents our efforts in implementing resource control support for Actor systems implemented using the Akka library. Particularly, given the lack of support in Akka for direct scheduling of actors, we compare two different ways of approximating actor-level control support. The first implementation expects messages to actors to provide estimates of resources likely to be consumed for processing them; these estimates are then relied upon to make scheduling decisions. In the second implementation, resource use of scheduled actors is tracked, and compared against allocations to decide when they should be scheduled next. We present experimental results on the performance cost of these resource control mechanisms, as well as their impact on resource utilization."
A Distributed Operating System Network Stack and Device Driver for Multicores.,"With the advances in network speeds a single processor cannot cope anymore with the growing number of data streams from a single network card. Multicore processors come at a rescue but traditional SMP OSes, which integrate the software network stack, scale only to a certain extent,limiting an application's ability to serve more connections while increasing the number of cores. On the other hand, kernel bypass solutions seem to scale better, but limit resource flexibility and control. We propose attacking these problems with a distributed OS design, using multiple network stacks (one per kernel) and relying on multi-queue hardware and hardware flow steering. This creates a single-socket abstraction among kernels while minimizing inter-core communication. We introduce our design, consisting of a distributed network stack, a distributed device driver, and a load-balancing algorithm. We compare our prototype, NetPopcorn, with Linux, Affinity Accept, FastSocket. NetPopcorn accepts between 5 to 8 times more connections and reduces the tail latency compared to these competitors. We also compare NetPopcorn with mTCP and observe that for high core counts, mTCP accepts only 18% more connections yet with higher tail latency than NetPopcorn."
Cache Potentiality of MONs: A Prime.,"Node buffer size has a big influence on performance of Mobile Opportunistic Networks (MONs). This is mainly because each node should temporarily cache packets to deal with the intermittently connected links. In this paper, we study fundamental bounds on node buffer size below which the network system can not achieve the expected performance. Given the condition that each link has the same probability p to be active, and q to be inactive during each time slot, there exits a critical value p
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">c</sub>
 from a percolation perspective. If p > p
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">c</sub>
, the network is in the supercritical case, there is an achievable upper bound on the buffer size of nodes, independent of the inactive probability q. When p <; p
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">c</sub>
, the network is in the subcritical case, and there exists a closed-form solution for buffer occupation, which is independent of the size of the network."
Oak: User-Targeted Web Performance.,"Web performance has long proved to be one of the most sought after and difficult to achieve components for the web. Since the inception of the modern web infrastructure, the situation has been growing in complexity, adding remote hosts and objects, providing everything from computation infrastructure, content distribution capability, and targeted advertising. While many of these components provide improvements for some users, the complexity of the Internet often leaves other users suffering from poor performance. We propose Oak, a system which addresses client performance on the individual level, hence addressing challenges which may be unique to the user. Oak measures a user's performance for objects loading on a page, and determines which components are under-performing. Oak further provides an automated mechanism by which sites are able to replace resources with those provided by a better performing alternative service for a particular user. In this work, we demonstrate the prevalence of under-performing services on the web, finding that over 60% of the Alexa Top 500 have at least one under-preforming server. We further evaluate Oak on experimental and popular existing webpages, and demonstrate its effectiveness in making decisions in existing environments and with a distributed user base."
A Self-Organizing Distributed and In-Band SDN Control Plane.,"Adopting distributed control planes is critical towards ensuring high availability and fault-tolerance of dependable Software-Defined Networks (SDNs). However, designing and bootstrapping a distributed SDN control plane is a challenging task, especially if to be done in-band, without a dedicated control network, and without relying on legacy networking protocols. One of the most appealing and powerful notions of fault-tolerance is self-organization and this paper discusses the possibility of self-organizing algorithms for in-band control planes."
Serverless Programming (Function as a Service).,"In this tutorial, we will present serverless computing, survey existing serverless platforms from industry, academia, and open source projects, identify key characteristics and use cases, and describe technical challenges and open problems. Our tutorial will involve a hands-on experience of using the serverless technologies available from different cloud providers (e.g. IBM, Amazon, Google and Microsoft). We expect our users to have basic knowledge of programming and basic knowledge of cloud computing."
Sensor Cloud: A Cloud of Sensor Networks.,"Traditional model of computing with wireless sensors imposes restrictions on how efficiently wireless sensors can be used due to resource constraints. Newer models for interacting with wireless sensors such as Internet of Things and Sensor Cloud aim to overcome these restrictions. In this tutorial, I will discuss sensor cloud architectures, which enable different wireless sensor networks, spread in a huge geographical area to connect together and be used by multiple users at the same time on demand basis. I will further discuss how virtual sensors assist in creating a multiuser environment on top of resource constrained physical wireless sensors and can help in supporting multiple applications on-demand basis. I will then present some security issues and provide overview of the solutions to the problems from the literature. In particular, I will discuss energy efficient privacy and data integrity preserving data aggregation algorithm, risk assessment in sensor cloud as well as attribute-based access control for sensor cloud applications. The topics covered will be: 1. Cloud of Sensors - Sensor Cloud Architectures 2. Virtualization in Sensor Cloud 3. Scheduling and QoS in Sensor Cloud 4. Data compression and Secure Aggregation in Sensor Cloud 5. Security, Privacy and Risk Issues in Sensor Cloud Rationale: Sensor cloud is a paradigm of cloud computing within sensor networks for on-demand sensing applications. This tutorial was given in ACM Middleware, 2016."
