DHL: Enabling Flexible Software Network Functions with FPGA Acceleration.,"Network function virtualization (NFV) aims to run software network functions (NFs) in commodity servers. As CPU is general-purpose hardware, one has to use many CPU cores to handle complex packet processing at line rate. Owing to its performance and programmability, FPGA has emerged as a promising platform for NFV. However, the programmable logic blocks on an FPGA board are limited and expensive. Implementing the entire NFs on FPGA is thus resource-demanding. Further, FPGA needs to be reprogrammed when the NF logic changes which can take hours to synthesize the code. It is thus inflexible to use FPGA to implement the entire NFV service chain. We present dynamic hardware library (DHL), a novel CPU-FPGA co-design framework for NFV with both high performance and flexibility. DHL employs FPGA as accelerators only for complex packet processing. It abstracts accelerator modules in FPGA as a hardware function library, and provides a set of transparent APIs for developers. DHL supports running multiple concurrent software NFs with distinct accelerator functions on the same FPGA and provides data isolation among them. We implement a prototype of DHL with Intel DPDK. Experimental results demonstrate that DHL greatly reduces the programming efforts to access FPGA, brings significantly higher throughput and lower latency over CPU-only implementation, and minimizes the CPU resources."
Scheduling Congestion-Free Updates of Multiple Flows with Chronicle in Timed SDNs.,"The advent of more accurate synchronization in Software-Defined Networks (SDNs) in general and the notion of timed updates in particular, enables operators to fully exploit the potential of the more fine-grained and adaptive traffic engineering, by avoiding disruptions and inconsistencies during the update. However, little is known today about how to schedule the update of multiple flows in such timed SDNs: As flows compete for limited resources, implementing a congestion-free update remains algorithmically challenging, even in timed SDNs. This paper initiates the study of the fundamental problem of how to reroute the update of multiple network flows in a synchronized SDN in a congestion-free manner. We show that that the problem is NP-hard already for flows of unit size and network links with unit delay. Our main contribution is a first solution for this problem: Chronicle. Our approach is based on a time-extended network construction and resource dependency graph, which is implemented by Openflow 1.5 using the scheduled bundles feature. Evaluation results show that Chronicle can reduce the makespan by 63% and reduce the number of changed rules by 50% compared to state-of-the-art."
Fair Coflow Scheduling without Prior Knowledge.,"Coflow scheduling improves the networking performance at the application level in datacenters. Ideally, a coflow scheduler should provide tenants with isolation guarantees to achieve predictable networking performance. Existing works in this regard (e.g., DRF [1] and HUG [2]) are limited to the clairvoyant scheduling, in that the complete knowledge of coflow sizes is assumed to be available before the communication starts. However, this assumption does not hold for many applications with pipelined computation, in which clairvoyant coflow schedulers become inapplicable. To bridge this gap, we develop a new non-clairvoyant coflow scheduler, called Non-Clairvoyant DRF (NC-DRF), which provides isolation guarantees between contending coflows without prior knowledge of coflow size. We show that NC-DRF achieves provable isolation guarantees in the long run. Cluster deployment and trace-driven simulations show that with NC-DRF, coflows are only delayed by 68% on average as compared with the clairvoyant, isolation-optimal DRF [1]. NC-DRF also outperforms existing alternatives (e.g., per-link fairness) by 1.7Ã— in terms of the average coflow completion time."
Support ECN in Multi-Queue Datacenter Networks via Per-Port Marking with Selective Blindness.,"ECN is a powerful tool that can achieve low latency and high throughput simultaneously. Support ECN for multiqueue scenarios is an industry trend in datacenter networks. However, ECN schemes developed for per-port marking cannot be applied directly to the multi-queue scenarios. It hurts at least one metric among latency, throughput, and the scheduling policy. State-of-the-art multi-queue ECN marking schemes each has its own limitations. In this paper, we present per-Port Marking with Selective Blindness (PMSB). The intuition is that: if a flow is found to be a victim of per-port marking, we can either revoke the marking or cancel the flow back-off even if its packets qualify the per-port threshold (i.e., selective blindness). By breaking the fixed causal relationship between ECN marking and flow backoff, flows from un-congested queues can be protected. We evaluate PMSB with large-scale NS-3 simulations. Our results demonstrate that PMSB can preserve a given scheduling policy. Compared with the current practice, PMSB can reduce the average/99% completion time for small flows by 64.49%/72.89% respectively while delivering a slightly better performance for large flows."
Designing Fast and Friendly TCP to Fit High Speed Data Center Networks.,"The dramatic expansion of link capacity in current data center network causes remarkable challenges to the design of new transport layer protocol, that is, how to converge as fast as possible to help data flow effectively utilize the high bandwidth. Meanwhile, the new protocol should be friendly to the traditional TCP because the non-cooperating applications with old TCP versions are widely existing. Therefore, it is important to achieve a trade-off between the aggressiveness and TCP-friendliness in protocol design. In this paper, we first empirically study why the existing typical data center TCP variants naturally fail to guarantee both fast convergence and TCP friendliness. Then, we design FFC, a transport protocol that makes independent decisions and self-adjustment through retrieving the two-dimensional congestion notification from the RTT and ECN. The results of simulation experiments and real implementations show that the fast convergence of FFC leads to the lower flow completion time compared with DX and DCTCP. Meanwhile, when coexisting with the traditional TCP, FFC also presents a moderate competitiveness, while introducing trivial deployment overhead only at the end hosts."
Fault Localization in Large-Scale Network Policy Deployment.,"The recent advances in network management automation and Software-Defined Networking (SDN) facilitate network policy management tasks. At the same time, these new technologies create a new mode of failure in the management cycle itself. Network policies are presented in an abstract model at a centralized controller and deployed as low-level rules across network devices. Thus, any software and hardware element in that cycle can be a potential cause of underlying network problems. In this paper, we present and solve a network policy fault localization problem that arises in operating policy management frameworks for a production network. We formulate our problem via risk modeling and propose a greedy algorithm that quickly localizes faulty policy objects in the network policy. We then design and develop SCOUT-a fully-automated system that produces faulty policy objects and further pinpoints physical-level failures which made the objects faulty. Evaluation results using a real testbed and extensive simulations demonstrate that SCOUT detects faulty objects with small false positives and false negatives."
Ignem: Upward Migration of Cold Data in Big Data File Systems.,"This paper investigates whether migrating cold data can yield significant speedup for big data jobs that run on modern big data file systems. Our work is motivated by two observations. First, improving the input stage of a job can provide significant speedup because many jobs spend a large part of their execution reading inputs. The second observation is that the inputs for many jobs are cold. Common techniques that aim to keep hot data in memory do not benefit these jobs. We analyze the Google production cluster trace data and find that the key ingredients for effectively migrating cold data do exist in such production environments. Encouraged by our findings, we design and implement Ignem, a framework for migrating cold data in big data file systems. We evaluate Ignem in a series of experiments and show that it provides significant speedup for both small and large jobs. Specifically, Hive queries are accelerated by up to 34%; the mean job duration in a trace-driven workload is reduced by 12% and the task duration by nearly 40%; other standalone jobs such as sort and wordcount also improve similarly by up to 30%."
On the Fly Load Balancing to Address Hot Topics in Topic-Based Pub/Sub Systems.,"Distributed topic-based publish/subscribe systems like Apache Kafka provide a scalable and decentralized approach to achieve data dissemination. However, despite their wide adoption they can suffer from performance degradation due to the uneven load distribution between the nodes that receive and forward the messages (i.e., brokers). This problem occurs due to the lack of effective load balancing mechanisms that consider the impact of (i) the amount of topics that are handled by a specific broker and (ii) changes in the input rate during the course of the system execution. Furthermore, while there have been some previous works that examine the problem, most of them focus on content-based pub/sub systems or require a centralized coordinator for determining the appropriate assignments. In this work we propose a novel decentralized load balancing technique for topic-based publish/subscribe systems. More specifically, we exploit the fact that brokers in systems like Kafka can communicate using inner topics to exchange their load-related information and propose a novel decentralized algorithm that executes on each individual broker to determine the topics' partitions that should be migrated in order to avoid overloaded conditions. Our detailed experimental evaluation on our local cluster, using different applications that process various data forms from different topics, illustrate the benefits of our approach and show that we can efficiently balance the load between the brokers without the need of a centralized coordination mechanism."
Parallelism-Aware Locally Repairable Code for Distributed Storage Systems.,"Distributed storage systems store a substantial amount of data in a large number of servers built with commodity hardware. In order to protect data against server failures, erasure coding has been deployed in many distributed storage systems because of its low storage overhead. In particular, since disk I/O is, in many cases, a bottleneck in the distributed storage system, locally repairable codes, have been proposed that incur low volumes of disk I/O when reconstructing missing data after server failures. However, since original data can only be read from specific servers, existing designs of locally repairable codes suffer from limited data parallelism. Besides, if the performance of servers is heterogeneous, slow servers may become the bottleneck when accessing data in parallel. In this paper, we propose Galloper codes, a novel family of locally repairable codes, that achieve low disk I/O during reconstruction and meanwhile extend data parallelism from specific servers to all servers. Moreover, the amount of original data in each server can be arbitrarily determined based on the performance of corresponding servers. We have implemented a prototype of Galloper codes on Apache Hadoop, and our experimental results have shown that Galloper codes can reduce the completion time of MapReduce jobs by up to 42.9%, with a comparable performance as existing locally repairable codes, in terms of disk I/O overhead, as well as encoding and reconstruction overhead."
Stay Fresh: Speculative Synchronization for Fast Distributed Machine Learning.,"Large machine learning models are typically trained in parallel and distributed environments. The model parameters are iteratively refined by multiple worker nodes in parallel, each processing a subset of the training data. In practice, the training is usually conducted in an asynchronous parallel manner, where workers can proceed to the next iteration before receiving the latest model parameters. While this maximizes the rate of updates, the price paid is compromised training quality as the computation is usually performed using stale model parameters. To address this problem, we propose a new scheme, termed speculative synchronization. Our scheme allows workers to speculate about the recent parameter updates from others on the fly, and if necessary, the workers abort the ongoing computation, pull fresher parameters, and start over to improve the quality of training. We design an effective heuristic algorithm to judiciously determine when to restart training iterations with fresher parameters by quantifying the gain and loss. We implement our scheme in MXNet-a popular machine learning framework-and demonstrate its effectiveness through cluster deployment atop Amazon EC2. Experimental results show that speculative synchronization achieves up to 3Ã— speedup over the asynchronous parallel scheme in many machine learning applications, with little additional communication overhead."
D2-Tree: A Distributed Double-Layer Namespace Tree Partition Scheme for Metadata Management in Large-Scale Storage Systems.,"The behavior of metadata server (MDS) cluster is critically important to the overall performance of today's petabyte-scale or even exabyte-scale distributed file system. How to maintain a high level of both system locality and load balancing is a significant challenge to MDS clusters. However, traditional metadata management schemes, including hash-based mapping and subtree partitioning, have severe bias on either system locality or load balancing. In this paper, we propose D
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
-Tree, a distributed double-layer namespace tree partition scheme, for metadata management in large-scale storage systems. The innovative idea is to design a greedy strategy to split the namespace tree into global layer and local layer subtrees, of which global layer is replicated to maintain load balancing and the lower-half subtrees are allocated separately to MDS's by a mirror division method to preserve locality. Both theoretical analysis based on empirical cumulative distribution and extensive experiments are provided to validate the efficiency of D
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
-Tree. Experiments using actual trace data on Amazon EC2 also exhibit the superior performance of D
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
-Tree compared with much previous literature."
Multi-Client Transactions in Distributed Publish/Subscribe Systems.,"Transactional operation processing among clients is increasingly required of publish/subscribe (pub/sub) systems in enterprise settings. For instance, in workflow management, dispatching or consolidating process instances require publications and (un-) subscriptions by different clients to be executed according to ACID semantics. As pub/sub systems are usually optimized for performance and scalability, such properties are often neglected, which results in unexpected system behavior. In this paper, we provide a model for supporting multiclient transactions in pub/sub. We formalize ACID properties for pub/sub, and define a consistency model and isolation level required in the aforementioned scenarios. We present three approaches for two transaction types: S-TX, where a coordinator has full static knowledge about all operations in a transaction, and D-TX/D-TXNI, where operations by other clients are dynamic and unknown to the coordinator. We describe algorithms realizing these approaches and experimentally evaluate them by comparing to a baseline mechanism, which simulates these guarantees partially with manual waits between operations. Our results show that the uncertainty introduced by the dynamic behavior renders D-TX/D-TXNI costly, and suitable only for small configurations or rare occasions. S-TX, in contrast, offers enriched semantics for many applications in a scalable manner without disrupting regular event routing."
Optimal Service Function Tree Embedding for NFV Enabled Multicast.,"In network traffic engineering, multicast is designed to deliver the same content from a single source to a group of destinations. Recently, NFV enabled multicast has been developed by deploying virtual network functions (VNFs) over the target network. To fulfill the multicast task with a service function chain (SFC) requirement, a service function tree (SFT) embedded in the shared multicast tree has to be built. Given the huge space of SFT embedding solutions, however, it is extremely hard to find the optimal one such that the total traffic delivery cost is minimized. In this paper, we tackle the optimal SFT embedding problem in NFV enabled multicast task. Specifically, we formally define the problem and formulate it with an integer linear programming (ILP), which turns out to be NP-hard. Then, a two-stage algorithm is proposed to deal with the problem with an approximation ratio of 1+p, where p is the best approximation ratio of Steiner tree and can be as small as 1.39. With extensive experimental evaluations, we demonstrate that by applying our SFT embedding solution, the cost saving of multicast traffic delivery can be up to 22.41%, compared with the random SFT embedding strategy."
NetRS: Cutting Response Latency in Distributed Key-Value Stores with In-Network Replica Selection.,"In distributed key-value stores, performance fluctuations generally occur across servers, especially when the servers are deployed in a cloud environment. Hence, the replica selected for a request will directly affect the response latency. In the context of key-value stores, even the state-of-the-art algorithm of replica selection still has considerable room for improving the response latency. In this paper, we present the fundamental factors that prevent replica selection algorithms from being effective. We address these factors by proposing NetRS, a framework that enables in-network replica selection for key-value stores. NetRS exploits emerging network devices, including programmable switches and network accelerators, to select replicas for requests. NetRS supports diverse algorithms of replica selection and is suited to the network topology of modern data centers. Compared with the conventional scheme of clients selecting replicas for requests, NetRS could effectively cut the response latency according to our extensive evaluations. Specifically, NetRS reduces the average latency by up to 48.4%, and the 99th latency by up to 68.7%."
OpuS: Fair and Efficient Cache Sharing for In-Memory Data Analytics.,"We study the fair cache allocation problem in shared cloud environments, where many users and applications contend for the main memory to cache shared datasets or files. Unlike other resources such as CPUs and networks, in-memory caches can be non-exclusively shared across many users, e.g., a cached columnar dataset queried by many Spark SQL jobs. This results in a unique challenge of the ""free-riding"" problem, where a user lies about its caching preferences to trick other users to cache files for it, using their allocated cache space. We show that existing cache allocation policies either suffer from such manipulations or result in poor efficiency. To address this problem, we propose a new cache allocation algorithm, termed OpuS, or Opportunistic Sharing for high efficiency. We show that OpuS provides performance isolation between users and is strategy-proof against ""free-riding"" manipulations. We have implemented OpuS as a pluggable cache manager in Alluxio, a popular memory-centric filesystem. Cluster deployment and trace-driven simulations demonstrate that OpuS allocates each user a fair share of caches while achieving near-optimal efficiency in cache utilization."
vNetTracer: Efficient and Programmable Packet Tracing in Virtualized Networks.,"As the scale of cloud systems continues to grow, virtualized networks that provide connectivity between services within and across data centers, are becoming increasingly important to the performance and reliability of the cloud. Despite many advantages, including fast deployment, ease of management, and programmability, virtualized networks require additional layers of abstraction and complicate monitoring and diagnosis of performance issues compared to traditional networks on physical hardware. Virtualized networks usually connect components in multiple protection domains, such as a guest OS, the hypervisor, network bridges, and separate virtualized network functions. There is no efficient means to trace packet transmission across the boundaries. Furthermore, it is challenging to reason about the performance of dynamic virtualized networks. Therefore, fine-grained, user customizable, and reconfigurable network tracing becomes a great need. To address these challenges, we built vNetTracer, an efficient and programmable packet profiler for virtualized networks. vNetTracer relies on the extended Berkeley Packet Filter (eBPF) to dynamically insert user-defined trace programs into a live virtualized network without any changes to the applications or restarts of the monitored network. Through three case studies, we demonstrate the effectiveness of vNetTracer in diagnosing various virtualized networking problems."
"""Semi-Asynchronous"": A New Scheduler for Robot Based Computing Systems.","The study of mobile entities, called robots, that have to accomplish global tasks on the basis of local information has attracted many researchers. A well-known scenario is that in which robots operate in Look-Compute-Move (LCM) computational cycles. In each cycle, a robot takes a snapshot of the environment (Look phase), then executes a distributed algorithm on the basis of the obtained snapshot (Compute phase), and finally moves toward a desired destination, if any (Move phase). LCM cycles might be subject to different temporal constraints dictated by the considered schedule. The classic models for the activation and synchronization of mobile robots are the fully-synchronous, semi-synchronous, and asynchronous models. The three models have been shown to constitute a hierarchy, that is fully-synchronous robots can accomplish more tasks than semi-synchronous robots that in turn can accomplish more tasks than asynchronous robots. The computational power of robots based on the different models has been extensively investigated, revealing a big gap between asynchronous robots and the other models. For many problems it is still not known whether the synchronization is crucial for designing resolution algorithms or not. In order to better understand the asynchronous case, here we propose further models referred to as semi-asynchronous, showing that for robots moving on graphs, semi-synchronous robots can accomplish more tasks than semi-asynchronous robots that in turn can accomplish more tasks than asynchronous robots. Whether the same strict hierarchy also holds for robots moving on the Euclidean plane remains open, however our investigation reveals interesting consequences that may help in better characterizing the computational power of robots with respect to the different synchronization models."
Shrewd Selection Speeds Surfing: Use Smart EXP3!,"In this paper, we explore the use of multi-armed bandit online learning techniques to solve distributed resource selection problems. As an example, we focus on the problem of network selection. Mobile devices often have several wireless networks at their disposal. While choosing the right network is vital for good performance, a decentralized solution remains a challenge. The impressive theoretical properties of multi-armed bandit algorithms, like EXP3, suggest that it should work well for this type of problem. Yet, its real-word performance lags far behind. The main reasons are the hidden cost of switching networks and its slow rate of convergence. We propose Smart EXP3, a novel bandit-style algorithm that (a) retains the good theoretical properties of EXP3, (b) bounds the number of switches, and (c) yields significantly better performance in practice. We evaluate Smart EXP3 using simulations, controlled experiments, and in-the-wild experiments. Results show that it stabilizes at the optimal state, achieves fairness among devices and gracefully deals with transient behaviors. In real world experiments, it can achieve 18% faster download over alternate strategies. We conclude that multi-armed bandit algorithms can play an important role in distributed resource selection problems, when practical concerns, such as switching costs and convergence time, are addressed."
A Scalable Linearizable Multi-Index Table.,"Concurrent data structures typically index data using a single primary key and provide fast atomic access to data associated with a given key value. However, it is often required to atomically access information via multiple primary and secondary keys, and even through additional properties that do not naturally represent keys for the given data. We present lock-free and lock-based algorithms of a table with multiple indexing, supporting linearizable inserts, deletes, and retrieve operations. We have implemented Java versions of our algorithms and evaluated their performance on a multi-core machine. The results show that the proposed table implementations are scalable and more efficient than any existing available alternative for in-memory realizations of a multi-index table."
Tight Bounds for Maximal Identifiability of Failure Nodes in Boolean Network Tomography.,"We study maximal identifiability, a measure recently introduced in Boolean Network Tomography to characterize networks' capability to localize failure nodes in end-to-end path measurements. Under standard assumptions on topologies and on monitors placement, we prove tight upper and lower bounds on the maximal identifiability of failure nodes for specific classes of network topologies, such as trees, bounded-degree graphs, d-dimensional grids, in both directed and undirected cases. Among other results we prove that directed d-dimensional grids with support n have maximal identifiability d using nd monitors; and in the undirected case we show that 2d monitors suffice to get identifiability of d-1. We then study identifiability under embeddings: we establish relations between maximal identifiability, embeddability and dimension when network topologies are modelled as DAGs. Through our analysis we also refine and generalize results on limits of maximal identifiability recently obtained in [12] and [1]. Our results suggest the design of networks over N nodes with maximal identifiability Î©(âˆšlog N) using 2âˆšlog N monitors and heuristics to place monitors and edges in a network to boost maximal identifiability."
PEA: Parallel Evolutionary Algorithm by Separating Convergence and Diversity for Large-Scale Multi-Objective Optimization.,"Running evolutionary algorithms in parallel is an intuitive way to speed up the process of solving large-scale multi-objective optimization problems, which have hundreds or thousands of decision variables. However, the framework of the existing multi-objective evolutionary algorithms seriously limits their parallelization. During each iteration, the environmental selection operators present in the existing framework need to collect and compare all the candidate solutions to balance the convergence and diversity, thus dividing the whole evolutionary process into a series of dependent sub-processes and resulting in frequent data transmission. To address this issue, we propose a novel parallel framework that separates the environmental selection operator from the entire evolutionary process, evidently removing the dependencies among sub-processes and reducing the data transmission. On the basis of the parallel framework, a new parallel evolutionary algorithm, namely PEA, is designed. In PEA, the convergence is achieved by a series of independent sub-populations, and the diversity is merely emphasized at the converged solutions from each subpopulation, which is helpful for avoiding that the environmental selection operator limits the parallelization of the algorithm. Moreover, a new environmental selection strategy is proposed to improve the diversity without considering the convergence. To assess the performance of the proposed PEA, we compare it with five representative multi-objective evolutionary algorithms in terms of both the convergence and diversity. The performance of the parallel framework is also analyzed by comparing with two existing parallel models. The experimental results demonstrate the superiority of the proposed parallel algorithms in terms of the convergence, diversity, and speedup."
Renaissance: A Self-Stabilizing Distributed SDN Control Plane.,"By introducing programmability, automated verification, and innovative debugging tools, Software-Defined Networks (SDNs) are poised to meet the increasingly stringent dependability requirements of today's communication networks. However, the design of fault-tolerant SDNs remains an open challenge. This paper considers the design of dependable SDNs through the lenses of self-stabilization - a very strong notion of fault-tolerance. In particular, we develop algorithms for an in-band and distributed control plane for SDNs, called Renaissance, which tolerates a wide range of (concurrent) controller, link, and communication failures. Our self-stabilizing algorithms ensure that after the occurrence of an arbitrary combination of failures, (i) every non-faulty SDN controller can eventually reach any switch in the network within a bounded communication delay (in the presence of a bounded number of concurrent failures) and (ii) every switch is managed by at least one non-faulty controller. We evaluate Renaissance through a rigorous worst-case analysis as well as a prototype implementation (based on OVS and Floodlight), and we report on our experiments using Mininet."
CASCADE: Reliable Distributed Session Handoff for Continuous Interaction Across Devices.,"Allowing users to navigate seamlessly between their personal devices while protecting their privacy remains today an ongoing challenge. Existing solutions rely on peer-to-peer designs, and blindly flood the network with session messages. It is particularly hard to come up with proposals that are both cost-efficient and dependable while relying on poorly connected mobile appliances. We propose Cascade, a distributed protocol to share applicative sessions among one's devices. Our proactive session handoff algorithm takes inspiration from the BitTorrent P2P file sharing protocol, but adapts it to the specific characteristics of our problem. It eschews in particular trackers, and limits the seeders of each session to the devices most likely to be used next, as computed by a decentralized aggregation protocol. A key aspect of our approach is to trade off network costs for reliability, while providing a faster session handoff than centralized solutions in the vast majority of the cases."
EC-Store: Bridging the Gap between Storage and Latency in Distributed Erasure Coded Systems.,"Cloud storage systems typically choose between replicating or erasure encoding data to provide fault tolerance. Replication ensures that data can be accessed from a single site but incurs a much higher storage overhead, which is a costly downside for large-scale storage systems. Erasure coding has a lower storage requirement but relies on encoding/decoding and distributed data retrieval, which can result in straggling requests that increase response times. We propose strategies for data access and data movement within erasure-coded storage systems that significantly reduce data retrieval times. We present EC-Store, a system that incorporates these dynamic strategies for data access and movement based on workload access patterns. Through detailed evaluation using two benchmark workloads, we show that EC-Store incurs significantly less storage overhead than replication while achieving better performance than both replicated and erasure-coded storage systems."
USTR: A High-Performance Traffic Engineering Approach for the Failed Link.,"Traffic Engineering for Failure Recovery (TEFR) optimizes the rerouted traffic during network failure. Using Linear Program (LP) to solve this optimization problem is too computationally intensive. Thus, we propose a novel Universal Single-link Traffic Rerouting (USTR) approach based on the maximum flow method. Having the same level of optimality as LP, USTR has a lower time complexity by orders of magnitude than LP, so that all the links in network can be protected. Since it is the first time to use maximum flow method to solve the traffic rerouting problem, we rigorously prove the correctness and the complexity of USTR. We evaluate USTR on real network topologies with thousands of links and nodes, and the running time is quite acceptable. Specifically, we use SDN to implement a prototype of USTR on the OpenDaylight (ODL) controller which works in OpenFlow, MPLS and IP networks."
ElMem: Towards an Elastic Memcached System.,"Memory caches, such as Memcached, are a critical component of online applications as they help maintain low latencies by alleviating the load at the database. However, memory caches are expensive, both in terms of power and operating costs. It is thus important to dynamically scale such caches in response to workload variations. Unfortunately, stateful systems, such as Memcached, are not elastic in nature. The performance loss that follows a scaling action can severely impact latencies and lead to SLO violations. This paper proposes ElMem, an elastic Memcached system that mitigates post-scaling performance loss by proactively migration hot data between nodes. The key enabler of our work is an efficient algorithm, FuseCache, that migrates the optimal amount of hot data to minimize performance loss. Our experimental results on OpenStack, across several workload traces, show that ElMem elastically scales Memcached while reducing the postscaling performance degradation by about 90%."
Vulnerability of Interdependent Networks with Heterogeneous Cascade Models and Timescales.,"The vulnerability of interdependent networks has recently drawn much attention, especially in the key infrastructure networks such as power and communication networks. However, the existing works mainly considered a single cascade model across the networks and there is a need for more accurate models and analysis. In this paper, we focus on the interdependent power/communication networks to accurately analyze their vulnerability by considering heterogeneous cascade models. Accurately analyzing interdependent networks is challenging as the cascades are heterogeneous yet interdependent. Also, including multiple timescales into the context can further increase the complexity. To better depict the vulnerability of interdependent networks, we first propose a method to learn a threshold model from historical data to characterize the cascades in the power network and alleviate the need of calculating complicated power network dynamics. Next, we introduce message passing equations to generalize the threshold model in the power network and the percolation model in the communication network, based on which we derive efficient solution for finding the most critical nodes in the interdependent networks. Removing the most critical nodes can cause the largest cascade and thus characterizes the vulnerability. We evaluate the performance of the proposed methods in various datasets and discuss how network parameters, such as the timescales, can impact the vulnerability."
Non-IT Energy Accounting in Virtualized Datacenter.,"Energy accounting plays a crucial role in datacenter energy management, wherein the energy consumption of non-IT units (e.g., UPS and cooling system) makes up a significant portion. However, it is challenging to fairly account for non-IT energy on an individual VM basis, because the non-IT units are shared by multiple VMs in a virtualized datacenter and only the system-level non-IT energy consumption can be measured. Existing policies, e.g., equally or proportionally allocating non-IT energy to VMs based on their IT energy, are not fair, in the sense that they can not satisfy a set of desired axiomatic principles of fair allocation. In this paper, we propose LEAPS, a Lightweight Energy Accounting Policy based on a provably fair methodology called Shapley value. We evaluate it using real-world datacenter trace and demonstrate that, compared to original Shapley value approach that has an exponential complexity, LEAPS yields almost the same energy accounting result within a maximum relative error less than 6.97%, while having a negligible computation time."
3DCS: A 3-D Dynamic Collaborative Scheduling Scheme for Wireless Rechargeable Sensor Networks with Heterogeneous Chargers.,"With the rise of wireless power transfer technology, charging scheduling issue is prevalent in wireless rechargeable sensor networks (WRSNs). Most prior arts focused on two-dimensional (2-D) networks with homogeneous mobile chargers. However, three-dimensional (3-D) networks with collaborations among heterogeneous mobile chargers are more practical. In this paper, we consider 3-D networks in which wireless charging vehicles (WCVs) are employed with unmanned aerial vehicles (UAVs). To prolong network lifetime, we focus on device sleep time and energy usage and propose a 3-D Dynamic Collaborative Scheduling scheme (3DCS). Theoretical values of energy threshold and partition number are determined to assign charging tasks to chargers. Then, scheduling algorithms that include target selection, infeasibility test, and target update, are developed. In addition, a collaborative algorithm is developed to re-assign charging tasks from busy chargers toward their neighboring chargers to further improve charging efficiency. Test-bed experiments and extensive simulations reveal that, compared with several distinguished scheduling schemes, our scheme has a superior performance in charging throughput, energy efficiency, and other characteristics."
Towards Personalized Learning in Mobile Sensing Systems.,"Nowadays, mobile devices have become an important part of our daily life. Numerous mobile sensing applications are enabled by various mobile platforms, which leverage machine learning techniques to detect or classify the events of interest such as human activities and health conditions. To achieve this, each user is required to provide a considerable amount of training samples. However, in practice, a large portion of the users may provide only a few or even zero labels, due to various reasons such as privacy concern or simply laziness. A straightforward solution to this problem is to gather the data of all the users in a central database, and train a global classifier from the combined data. Such global classifier, however, may not work well since it ignores the variety in different users' data. To address this challenge, we propose PLOS, a Personalized Learning framework for mObile Sensing applications. PLOS can jointly model the commonness shared among the users as well as the differences between them, which are inferred from both the label information and the underlying structures of individual data. We further develop the distributed PLOS where the raw data of the users are locally processed so that the users only need to send model parameters to the server. Through extensive experiments on both synthetic data and real mobile sensing systems, we show that the proposed PLOS framework is scalable and efficient in energy, computation, and communication costs, and can achieve more accurate classification results compared with the baseline methods."
ApDeepSense: Deep Learning Uncertainty Estimation without the Pain for IoT Applications.,"Recent advances in deep-learning-based applications have attracted a growing attention from the IoT community. These highly capable learning models have shown significant improvements in expected accuracy of various sensory inference tasks. One important and yet overlooked direction remains to provide uncertainty estimates in deep learning outputs. Since robustness and reliability of sensory inference results are critical to IoT systems, uncertainty estimates are indispensable for IoT applications. To address this challenge, we develop ApDeepSense, an effective and efficient deep learning uncertainty estimation method for resource-constrained IoT devices. ApDeepSense leverages an implicit Bayesian approximation that links neural networks to deep Gaussian processes, allowing output uncertainty to be quantified. Our approach is shown to significantly reduce the execution time and energy consumption of uncertainty estimation thanks to a novel layer-wise approximation that replaces the traditional computationally intensive sampling-based uncertainty estimation methods. ApDeepSense is designed for neural net-works trained using dropout; one of the most widely used regularization methods in deep learning. No additional training is needed for uncertainty estimation purposes. We evaluate ApDeepSense using four IoT applications on Intel Edison devices. Results show that ApDeepSense can reduce around 88.9% of the execution time and 90.0% of the energy consumption, while producing more accurate uncertainty estimates compared with state-of-the-art methods."
Conservative Channel Reuse in Real-Time Industrial Wireless Sensor-Actuator Networks.,"Wireless Sensor-Actuator Networks (WSANs) are being adopted as an enabling technology for Industrial Internet of Things (IIoT) in process industries. Industrial applications impose stringent requirements in reliability and real-time performance on WSANs. To enhance reliability, industrial standards, such as WirelessHART, embrace Time Slotted Channel Hopping (TSCH) that integrates channel hopping and TDMA at the MAC layer. Within a network governed by a same gateway, WirelessHART prohibits channel reuse, i.e., concurrent transmissions in the same channel, to avoid interference between concurrent transmissions. Preventing channel reuse however negatively affects real-time performance. To meet the demand for both reliability and real-time performance by industrial applications, we propose a conservative channel reuse approach designed to enhance the real-time performance while limiting its impact on reliability in WSANs. In contrast to traditional channel reuse designed to optimize performance at the cost of reliability, our conservative approach introduces channel reuse only when needed to meet the timing constraints of flows. Finally, we present an algorithm to detect reliability degradation caused by channel reuse so that channels can be reassigned to further improve reliability. Experimental results based on two physical testbeds show that our approach significantly improves real-time performance while maintaining a high degree of reliability."
DiGS: Distributed Graph Routing and Scheduling for Industrial Wireless Sensor-Actuator Networks.,"Wireless Sensor-Actuator Networks (WSANs) technology is appealing for use in industrial IoT applications because it does not require wired infrastructure. Battery-powered wireless modules easily and inexpensively retrofit existing sensors and actuators in industrial facilities without running cabling for communication and power. IEEE 802.15.4 based WSANs operate at low-power and can be manufactured inexpensively, which makes them ideal where battery lifetime and costs are important. Almost a decade of real-world deployments of WirelessHART standard has demonstrated the feasibility of using its core techniques including reliable graph routing and Time Slotted Channel Hopping (TSCH) to achieve reliable low-power wireless communication in industrial facilities. Today we are facing the 4th Industrial Revolution as proclaimed by political statements related to the Industry 4.0 Initiative of the German Government. There exists an emerging demand for deploying a large number of field devices in an industrial facility and connecting them through a WSAN. However, a major limitation of current WSAN standards is their limited scalability due to their centralized routing and scheduling that enhance the predictability and visibility of network operations at the cost of scalability. This paper decentralizes the network management in WirelessHART and presents the first Distributed Graph routing and autonomous Scheduling (DiGS) solution that allows the field devices to compute their own graph routes and transmission schedules. Experimental results from two physical testbeds and a simulation study show our approaches can significantly improve the network reliability, latency, and energy efficiency under dynamics."
It's Hard to Share: Joint Service Placement and Request Scheduling in Edge Clouds with Sharable and Non-Sharable Resources.,"Mobile edge computing is an emerging technology to offer resource-intensive yet delay-sensitive applications from the edge of mobile networks, where a major challenge is to allocate limited edge resources to competing demands. While prior works often make a simplifying assumption that resources assigned to different users are non-sharable, this assumption does not hold for storage resources, where users interested in services (e.g., data analytics) based on the same set of data/code can share storage resource. Meanwhile, serving each user request also consumes non-sharable resources (e.g., CPU cycles, bandwidth). We study the optimal provisioning of edge services with non-trivial demands of both sharable (storage) and non-sharable (communication, computation) resources via joint service placement and request scheduling. In the homogeneous case, we show that while the problem is polynomial-time solvable without storage constraints, it is NP-hard even if each edge cloud has unlimited communication or computation resources. We further show that the hardness is caused by the service placement subproblem, while the request scheduling subproblem is polynomial-time solvable via maximum-flow algorithms. In the general case, both subproblems are NP-hard. We develop a constant-factor approximation algorithm for the homogeneous case and efficient heuristics for the general case. Our trace-driven simulations show that the proposed algorithms, especially the approximation algorithm, can achieve near-optimal performance, serving 2-3 times more requests than a baseline solution that optimizes service placement and request scheduling separately."
WiBot! In-Vehicle Behaviour and Gesture Recognition Using Wireless Network Edge.,"Recent advancements in vehicular technology have meant that integrated wireless devices such as Wi-Fi access points or bluetooth are deployed in vehicles at an increasingly dense scale. These vehicular network edge devices, while enabling in car wireless connectivity and infotainment services, can also be exploited as sensors to improve environmental and behavioural awareness that in turn can provide better and more personalised driver feedback and improve road safety. We present WiBot! a network-edge based behaviour recognition and gesture based personal assistant system for cars. WiBot leverages the vehicular network edge to detect distracted behaviour based on unusual head turns and arm movements during driving situations by monitoring radio frequency fluctuation patterns in real-time. Additionally, WiBot can recognise known gestures from natural arm movements while driving and use such gestures for passenger-car interaction. A key element of WiBot design is its impulsive windowing approach that allows start and end of gestures to be accurately identified in a continuous stream of data. We validate the system in a realistic driving environment by conducting a non-choreographed continuous recognition study with 40 participants at BMW Group Research, New Technologies and Innovation centre. By combining impulsive windowing with a unique selection of features from peaks and subcarrier analysis of RF CSI phase information, the system is able to achieve 94.5% accuracy for head-vs. arm movement separation. We can further confidently differentiate relevant gestures from random arm and head movements, head turns and idle movement with 90.5% accuracy."
An Optimal Auction Mechanism for Mobile Edge Caching.,"With the explosive growth of wireless data, mobile edge caching has emerged as a promising paradigm to support mobile traffic recently, in which the service providers (SPs) prefetch some popular contents in advance and cache them locally at the network edge. When requested, those locally cached contents can be directly delivered to users with low latency, thus alleviating the traffic load over backhaul channels during peak hours and enhancing the quality-of-experience (QoE) of users simultaneously. Due to the limited available cache space, it makes sense for the SP to cache the most profitable contents. Nevertheless, users' true valuations of contents are their private knowledge, which is unknown to the SP in general. This information asymmetry poses a significant challenge for effective caching at the SP side. Further, the cached contents can be delivered with different quality, which needs to be chosen judiciously to balance delivery costs and user satisfaction. To tackle these difficulties, in this paper, we propose an optimal auction mechanism from the perspective of the SP. In the auction, the SP determines the cache space allocation over contents and user payments based on the users' (possibly untruthful) reports of their valuations so that the SP's expected revenue is maximized. The advocated mechanism is designed to elicit true valuations from the users (incentive compatibility) and to incentivize user participation (individual rationality). In addition, we devise a computationally efficient method for calculating the optimal cache space allocation and user payments. We further examine the optimal choice of the content delivery quality for the case with a large number of users and derive a closed-form solution to compute the optimal delivery quality. Finally, extensive simulations are implemented to evaluate the performance of the proposed optimal auction mechanism, and the impact of various model parameters is highlighted to obtain engineering insights into the content caching problem."
"ATMoN: Adapting the ""Temporality"" in Large-Scale Dynamic Networks.","With the widespread adoption of temporal graphs to study fast evolving interactions in dynamic networks, attention is needed to provide graph metrics in time and at scale. In this paper, we introduce ATMoN, an open-source library developed to computationally offload graph processing engines and ease the communication overhead in dynamic networks over an unprecedented wealth of data. This is achieved, by efficiently adapting, in place and inexpensively, the temporal granularity at which graph metrics are computed based on runtime knowledge captured by a low-cost probabilistic learning model capable of approximating both the metric stream evolution and the volatility of the graph topology. After a thorough evaluation with real-world data from mobile, face-to-face and vehicular networks, results show that ATMoN is able to reduce the compute overhead by at least 76%, data volume by 60% and overall cloud costs by at least 54%, while always maintaining accuracy above 88%."
ApproxIoT: Approximate Analytics for Edge Computing.,"IoT-enabled devices continue to generate a massive amount of data. Transforming this continuously arriving raw data into timely insights is critical for many modern online services. For such settings, the traditional form of data analytics over the entire dataset would be prohibitively limiting and expensive for supporting real-time stream analytics. In this work, we make a case for approximate computing for data analytics in IoT settings. Approximate computing aims for efficient execution of workflows where an approximate output is sufficient instead of the exact output. The idea behind approximate computing is to compute over a representative sample instead of the entire input dataset. Thus, approximate computing- based on the chosen sample size - can make a systematic tradeoff between the output accuracy and computation efficiency. This motivated the design of APPROXIOT- a data analytics system for approximate computing in IoT. To realize this idea, we designed an online hierarchical stratified reservoir sampling algorithm that uses edge computing resources to produce approximate output with rigorous error bounds. To showcase the effectiveness of our algorithm, we implemented APPROXIOT based on Apache Kafka and evaluated its effectiveness using a set of microbenchmarks and real-world case studies. Our results show that APPROXIOT achieves a speedup 1:3Ã—-9:9Ã— with varying sampling fraction of 80% to 10% compared to simple random sampling."
Speeding Up Multi-CDN Content Delivery via Traffic Demand Reshaping.,"Nowadays, more and more content providers (CPs) use multiple content delivery networks (CDNs) to deliver their content (a.k.a. content multihoming). Since the decisions on which CDN to use are made by the CP or by a CDN broker based on their local view of network conditions, content multihoming still has much room to improve for a better content delivery performance. In addition, content multihoming may negatively impact CDN vendors since in the price competition they are enforced to lower content delivery price to attract CPs to use their CDNs. To build a better CDN ecosystem, multi-CDN federation has been proposed to interconnect standalone CDNs. The real-world implementation of CDN interconnection (CDNI), however, poses significant technical obstacles not easy to solve in the short term. In order to improve the content delivery performance under current multi-CDN strategies, in this paper, we propose a feasible and efficient solution to multi-CDN, termed as CDN semi-federation, which can better schedule and utilize the resources from multiple CDNs without requiring full CDNI. The benefit of our solution comes from an effective optimization algorithm which reshapes the patterns of traffic from multiple CPs delivered over multipe CDN Points of Presence (PoPs). Experiments across North American and European ISP PoP networks demonstrate that, compared with current multi-CDN solutions, CDN semi-federation can reduce the content delivery latency by around 20% during peak traffic hours."
S3B: Software-Defined Secure Server Bindings.,"For decades, request-routing protocols operating at multiple layers of the network stack have been a staple of Internet services. Commonly deployed request-routing techniques use the requestor's IP address as an identifier of the client. For instance, using DNS as a request-routing protocol, the local DNS resolver's IP address is used as a surrogate identifier of the client in order to assign the client to the closest server. While such coarse associations may be acceptable for performance-centric purposes, they are not appropriate in settings that require fine-grained, enforceable bindings of clients to servers - e.g., to ensure that malicious clients are unable to bypass their bindings and issue their request to a server of their choosing. In this paper, we propose S3B (Software-defined Secure Server Bindings), a protocol that provides precise and enforceable client-server assignments. S3B uses a server module to assign clients unique access keys. Using HTTP redirection with the key encrypted as an additional domain label, the name server is able to distribute precise server assignments specific to each client. In addition, the server module maintains an access control list to enforce these assignments. As an implementation of the S3B protocol, we have developed an HTTP/S prototype and deployed it to Amazon AWS. Our performance evaluation suggests that our prototype introduces no discernible overhead for client requests. To evaluate S3B's effectiveness as a security appliance, we developed an application to isolate clients suspected as spiders, capable of virtually immediate containment once detected."
Time-Zone Geolocation of Crowds in the Dark Web.,"Dark Web platforms like the infamous Silk Road market, or other cyber-criminal or terrorism related forums, are only accessible by using anonymity mechanisms like Tor. In this paper we are concerned with geolocating the crowds accessing Dark Web forums. We do not focus on single users. We aim at uncovering the geographical distribution of groups of visitors into time-zones as a whole. Our approach, to the best of our knowledge, is the first of its kind applied to the Dark Web. The idea is to exploit the time of all posts in the Dark Web forums to build profiles of the visiting crowds. Then, to uncover the geographical origin of the Dark Web crowd by matching the crowd profile to that of users from known regions on regular web platforms. We assess the effectiveness of our methodology on standard web and two Dark Web platforms with users of known origin, and apply it to three controversial anonymous Dark Web forums. We believe that this work helps the community better understand the Dark Web from a sociological point of view and support the investigation of authorities when the security of citizens is at stake."
TACTIC: Tag-Based Access ConTrol Framework for the Information-Centric Wireless Edge Networks.,"Pervasive content caching is one of the information-centric networking (ICN) fundamentals. Although advantageous, pervasive caching introduces new challenges. In particular, the high possibility of content providers losing control over their published contents, which clients can access without authenticating themselves. The approaches that constitute the state-of-the-art in access control either have high computation overhead or require an always-online authentication server, thus suffering in terms of scalability for large number of end devices. In this paper, we propose TACTIC, a lightweight access control mechanism for the ICN wireless edge, which allows legitimate clients to utilize the cached content without per-request authentication at the providers. TACTIC delegates the authentication and authorization tasks to the (semi-trusted) routers in an ISP's network to eliminate the need for an always-online authentication server. It prevents delivery of the encrypted content to unauthorized users; a bandwidth-wasteful practice, which may lead to Distributed Denial of Service (DDoS) attack. Experimental results demonstrate the scalability and effectiveness of TACTIC in providing low-overhead access to legitimate clients while preventing malicious users' access."
CYCLOSA: Decentralizing Private Web Search through SGX-Based Browser Extensions.,"By regularly querying Web search engines, users (unconsciously) disclose large amounts of their personal data as part of their search queries, among which some might reveal sensitive information (e.g. health issues, sexual, political or religious preferences). Several solutions exist to allow users querying search engines while improving privacy protection. However, these solutions suffer from a number of limitations: some are subject to user re-identification attacks, while others lack scalability or are unable to provide accurate results. This paper presents CYCLOSA, a secure, scalable and accurate private Web search solution. CYCLOSA improves security by relying on trusted execution environments (TEEs) as provided by Intel SGX. Further, CYCLOSA proposes a novel adaptive privacy protection solution that reduces the risk of user re-identification. CYCLOSA sends fake queries to the search engine and dynamically adapts their count according to the sensitivity of the user query. In addition, CYCLOSA meets scalability as it is fully decentralized, spreading the load for distributing fake queries among other nodes. Finally, CYCLOSA achieves accuracy of Web search as it handles the real query and the fake queries separately, in contrast to other existing solutions that mix fake and real query results."
Hybrid Differentially-Private String Matching.,"Private record linkage enables multiple parties to exchange records of similar entities without disclosing their own data sets. Several techniques solve private matching for numerical attributes, but they cannot handle well string-typed data. We propose a protocol for differentially-private linkage of records with string-valued attributes. String-valued data are partitioned into groups, and each partition is represented by a synopsis. Strings are first embedded into a metric space which preserves the relative distance among them, and then they are translated to multi-dimensional points. We propose two partitioning algorithms: the first one uses solely Lipschitz embedding coordinates, whereas the second one also leverages string length information. To the best of our knowledge, this is the first efficient and provably secure solution to the private string matching problem. Extensive experimental results on a real dataset demonstrate that our solution is efficient and accurate. We also show that our solution can be adapted to alternative protection models, e.g., output-constrained differential privacy."
SDNProbe: Lightweight Fault Localization in the Error-Prone Environment.,"Probe-based fault localization identifies potential faulty nodes, which are manually inspected for confirmation. This work explores efficient and accurate fault localization, which is crucial for reducing the manual effort without affecting network functionality. Prior work suffers from either high bandwidth overhead or false detection (i.e., incorrectly attributing good nodes or missing faulty nodes), especially in the presence of multiple or inconsistent faults. We propose SDNProbe, a lightweight SDN application that sends a provably minimized number of probe packets to pinpoint malfunctioning switches. We extend SDNProbe to randomize tested paths and packet headers to further improve the detection accuracy. Using realistic topologies and flow rules, our evaluation results confirm that SDNProbe can rapidly localize faulty switches while reducing the number of required test packets by 30%, compared to prior approaches. Even with 50% of switches being faulty, the extended SDNProbe can detect all faulty switches in 33 seconds, whereas prior approaches have false negative rates of 15-40%."
Symbol-Level Cross-Technology Communication via Payload Encoding.,"To mitigate the issue of cross-technology interference (CTI) under dense wireless, cross-technology communication (CTC) was recently proposed, which enables direct communication among heterogeneous wireless technologies. We present SymBee, a novel ZigBee to WiFi CTC with symbol-level encoding for performance breakthrough from packet-level state-of-the-arts. SymBee is uniquely built on the new insight on ZigBee-WiFi physical layer cross-observability - i.e., the output on WiFi when fed with ZigBee signal (due to frequency overlap). This is analyzed experimentally and theoretically through rigorous derivations, from which the key innovation in SymBee design, i.e., payload encoding, stems; Conveying data across technologies is as simple as putting specific symbols in ZigBee packet payload, such that they yield unique and easily detectable patterns when cross-observed at WiFi. This symbol-level encoding is fully compatible with any commodity ZigBee device. Decoding at WiFi is a light-weight function that recycles the output from idle listening, thereby minimizing the computation while keeping compatibility to WiFi standard. SymBee is extensively evaluated both theoretically and experimentally through testbed evaluations on six distinct locations including outdoor. The result demonstrate that SymBee reaches the throughput of up to 31.25kbps, 145.4Ã— faster than the state-of-the-art."
SURF: Supervisory Control of User-Perceived Performance for Mobile Device Energy Savings.,"Two critical quality factors for mobile devices (e.g., smartphones, tablets) are battery life and apps' userperceived performance, e.g., responsiveness of user actions and frame rate of video playback. Sadly, state-of-the-art solutions have at least one of the following two limitations: 1) they cannot efficiently handle concurrent foreground apps and so may lead to performance imbalance and high energy consumption, 2) they try to regulate app performance periodically and thus may not efficiently handle the aperiodicity of user actions, which can result in poor responsiveness or high overheads. In this paper, we present SURF, Supervisory control of User-perceived peRFormance, which is designed to overcome the two limitations. First, it dynamically allocates resources to concurrent apps for balanced performance. Second, SURF uses supervisory control theory to handle the aperiodicity of user actions. SURF features a two-level architecture design that performs the two tasks at different time scales, according to their different overheads and timing requirements. We test SURF on several mobile device models with real-world opensource apps and show that it can reduce the CPU energy consumption by 30-90% compared to state-of-the-art solutions while causing no perceivable performance degradation."
eBrowser: Making Human-Mobile Web Interactions Energy Efficient with Event Rate Learning.,"Due to the limited screen size of mobile devices, finger movements on touchscreen, such as scrolling and pinching (i.e., zooming in or out), are frequently used on mobile Web browsers and WebView-based apps, consuming considerable energy on mobile devices. While existing works on mobile Web browsers focus on reducing the power consumption or optimizing the performance of webpage loading, the power consumption of mobile Web interactions, especially after webpage loading, has received comparatively little attention. Motivated by an empirical study of the power consumption and user experience survey of human-mobile interactions, we design and implement eBrowser, an energy-efficient mobile Web interaction framework. It leverages a cloud-based machine learning model to enable personalized interaction event rate for individual users according to the interaction speed of their finger movement and the content of rendered webpages. To adapt to user behavior changes, eBrowser continuously monitors the interaction experience on each mobile device and periodically updates the personalized event rate model with incremental learning in the cloud. We implement eBrowser in Chromium and deploy the event rate model in a remote Aliyun cloud instance. Experimental results show that eBrowser reduces the energy consumption of mobile Web interactions by up to 43.8% with negligible runtime overhead, while guaranteeing user satisfaction on both mobile browsers and WebView-based apps."
RF-MVO: Simultaneous 3D Object Localization and Camera Trajectory Recovery Using RFID Devices and a 2D Monocular Camera.,"Most of the existing RFID-based localization systems cannot well locate RFID-tagged objects in a 3D space. Limited robot-based RFID solutions require reader antennas to be carried by a robot moving along an already-known trajectory at a constant speed. As the first attempt, this paper presents RF-MVO, which fuses battery-free RFID and monocular visual odometry to locate stationary RFID tags in a 3D space and recover an unknown trajectory of reader antennas binding with a 2D monocular camera. The proposed hybrid system exhibits three unique features. Firstly, since the trajectory of a 2D monocular camera can only be recovered up to an unknown scale factor, RF-MVO combines the relative-scale camera trajectory with depth-enabled RF phase to estimate an absolute scale factor and spatially incident angles of an RFID tag. Secondly, we propose a joint optimization algorithm consisting of coarse-to-fine angular refinement, 3D tag localization and parameter nonlinear optimization, to improve real-time performance. Thirdly, RF-MVO can determine the effect of relative tag-antenna geometry on the estimation precision, providing optimal tag positions and absolute scale factors. Our experiments show that RF-MVO can achieve 6.23cm tag localization accuracy in a 3D space and 0.0158 absolute scale factor estimation accuracy for camera trajectory recovery."
Multiple Object Activity Identification Using RFIDs: A Multipath-Aware Deep Learning Solution.,"RFID-based human activity identification has become a key component in today's Internet-of-Things applications. State-of-the-art solutions mostly focus on the simple scenario with a single person in the open space. Extension to the more realistic realworld scenarios with multiple persons however is non-trivial. Given the much richer interactions among them, the backscattered signals will inevitably mixed, obscuring the information of individual activities. This is further complicated with multi-path in a common indoor environment. In this paper, we however argue that, though often considered harmful, the rich interactions combined with multi-path indeed offer more observable data. After careful processing the raw signals, critical information about the activities can be unveiled through modern learning tools. We present M
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
AI, which for the first time accommodates both multi-path and multi-object for activity identification. M
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
AI incorporates a phase calibration mechanism to automatically eliminate the frequency hopping offsets, and a novel decoupling mechanism for the periodogram and pseduospectrum in the raw signal mixture. The refined data are then fed into an advanced deep-learning engine that integrates a Convolutional Neural Network and a Long Short Term Memory network, which examines both spatial and temporal information in realtime for activity identification. Our M
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
AI is readily deployable using off-the-shelf RFID readers. We have implemented an M
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
AI prototype with Impinj UHF passive tags and a Speedway R420 reader. Experiments with multiple objects in a multipath-rich indoor environments report an activity identification accuracy of 97%, a significant gain (27%) over state-of-art solutions."
Environment-Adaptive Malicious Node Detection in MANETs with Ensemble Learning.,"This paper presents a robust machine learning-based method for detecting malicious nodes in mobile ad hoc networks (MANETs). Since general machine learning methods rely on training data, trained detectors do not work well in test environments that are different from training environments. This is an inherent problem in malicious detection in dynamic MANETs environments, where network parameters, such as the average node speed and density of nodes, differ in each environment. In this study, we propose a method for environment-adaptive malicious node detection based on ensemble learning. We first prepare weak malicious node detectors trained in diverse environments, and then construct a strong ensemble malicious node detector, which is tailored to a given test environment, by fusing weak detectors whose performances are estimated to be high in the test environment. We investigate the performance of our method and confirm that our method significantly outperforms the state-of-the-art methods in terms of detection accuracy and false detection rate."
Generating Synthetic Social Graphs with Darwini.,"Synthetic graph generators facilitate research in graph algorithms and graph processing systems by providing access to graphs that resemble real social networks while addressing privacy and security concerns. Nevertheless, their practical value lies in their ability to capture important metrics of real graphs, such as degree distribution and clustering properties. Graph generators must also be able to produce such graphs at the scale of real-world industry graphs, that is, hundreds of billions or trillions of edges. In this paper, we propose Darwini, a graph generator that captures a number of core characteristics of real graphs. Importantly, given a source graph, it can reproduce the degree distribution and, unlike existing approaches, the local clustering coefficient distribution. Furthermore, Darwini maintains a number of metrics, such as graph assortativity, eigenvalues, and others. Comparing Darwini with state-of-the-art generative models, we show that it can reproduce these characteristics more accurately. Finally, we provide an open source implementation of Darwini on the vertex-centric Apache Giraph model that can generate synthetic graphs with up to 3 trillion edges."
SnapTask: Towards Efficient Visual Crowdsourcing for Indoor Mapping.,"Visual crowdsourcing (VCS) offers an inexpensive method to collect visual data for implementing tasks, such as 3D mapping and place detection, thanks to the prevalence of smartphone cameras. However, without proper guidance, participants may not always collect data from desired locations with a required Quality-of-Information (QoI). This often causes either a lack of data in certain areas, or extra overheads for processing unnecessary redundancy. In this work, we propose SnapTask, a participatory VCS system that aims at creating complete indoor maps by guiding participants to efficiently collect visual data of high QoI. It applies Structure-from-Motion (SfM) techniques to reconstruct 3D models of indoor environments, which are then converted into indoor maps. To increase coverage with minimal redundancy, SnapTask determines locations for the next data collection tasks by analyzing the coverage of the generated 3D model and the camera views of the collected images. In addition, it overcomes the limitations of SfM techniques by utilizing crowdsourced annotations to reconstruct featureless surfaces (e.g. glass walls) in the 3D model. According to a field test in a library, the indoor map generated by SnapTask successfully reconstructs 100% of the library walls and 98.12% of objects and traversal areas within the library. With the same amount of input data our design of guided data collection increases the map coverage by 20.72% and 34.45%, respectively, compared with unguided participatory and opportunistic VCS."
Leveraging Crowdsensed Data Streams to Discover and Sell Knowledge: A Secure and Efficient Realization.,"Leveraging the wisdom of crowd for knowledge discovery and monetization is increasingly popular nowadays. Among others, one popular way of leveraging the crowd wisdom is crowdsensing with truth discovery, which is able to discover truthful knowledge from the unreliable sensory data harvested from mobile clients. In order to become truly successful, however, a number of challenges are yet to be addressed. First, safeguarding clients' sensory data is demanded for privacy protection. Second, in many real crowdsensing applications, data are usually collected in a streaming manner, so truth discovery is naturally required to be efficiently conducted in a streaming fashion. Thirdly, knowledge monetization should be made full-fledged, endowed with features of transparency and streamlined processing while fully addressing the practical needs of parties in the monetization ecosystem. In this paper, we present our initial effort on a crowdsensing framework that enables privacy-preserving knowledge discovery and full-fledged blockchain-based knowledge monetization. Our framework enables privacy-preserving and efficient truth discovery over encrypted crowdsensed data streams for truthful knowledge discovery. Meanwhile, with careful integration of the newly emerging blockchain-based smart contract technology, our framework allows full-fledged knowledge monetization. Tackling the challenges of monetization fairness and (on-chain) knowledge confidentiality, our customized knowledge monetization design well respects the interests of knowledge seller and requester, with full support of transparency, streamlined processing, and automatic quality-aware rewards for clients. Extensive experiments on Microsoft Azure cloud and Ethereum blockchain demonstrate the practically affordable performance of our design."
DeepMatching: A Structural Seed Identification Framework for Social Network Alignment.,"Network alignment aims at finding a bijective mapping between nodes of two networks. Due to its wide application in various fields (e.g., Computer Vision, Data Management, Bioinformatics, and Privacy Protection), researchers have proposed many network alignment algorithms, most of which rely on a set of pre-mapped seeds. However, it is challenging to identify an initial credible set of seeds solely with structural information. In this paper, by exploiting the observation that a true mapping leads to a large portion of consistent edges among the mapped nodes, we formally define the credibility of a mapping as its deviation from a random one. This enables us to measure the credibility of an initial set of seeds. We also present DeepMatching which is a seed identification framework for social network alignment. First, we represent the nodes of the two mapping networks with their structural feature vectors by employing graph embedding techniques. Second, we obtain an initial mapping of the nodes based on the obtained vectors by leveraging point set registration methods. Third, we develop a heuristic algorithm to extract a credible set of seed from the initial mapping. Finally, we utilize the extracted seed set as input of an efficient propagation-based algorithm for large scale network alignment. We conduct extensive experiments to evaluate the performance of DeepMatching, and the results clearly demonstrate its effectiveness and the efficiency."
Pay On-Demand: Dynamic Incentive and Task Selection for Location-Dependent Mobile Crowdsensing Systems.,"With the rich sensing capacity and ubiquitous usage of smartphones, crowdsensing leveraging the power of the crowd of mobile users has become an effective technique to collect data for various sensing applications. Many incentive mechanisms have been proposed to encourage people to participate in crowdsensing. However, most of them set unchangeable rewards for sensing tasks, while the inherent inequality and on-demand feature of sensing tasks have been long ignored, especially for location-dependent sensing tasks. In this paper, we focus on location-dependent crowdsensing systems and propose a demand-based dynamic incentive mechanism that dynamically changes the rewards of sensing tasks at each sensing round in an on-demand way to balance their popularity. A demand indicator is introduced to characterize the demand of each sensing task by considering its deadline, completing progress, and number of potential participants. At each sensing round, we use the Analytic Hierarchy Process to calculate the relative demands of all sensing tasks and then determine their rewards accordingly. Moreover, we prove that the distributed task selection problem with time budget is NP-hard. We propose an optimal dynamic programming based solution and a greedy solution to help each user select tasks while maximizing its profit. Extensive experiments show that the demand-based dynamic incentive mechanism outperforms existing incentive mechanisms."
DCMPTCP: Host-Based Load Balancing for Datacenters.,"Load balancing in datacenter networks (DCNs) is an important and challenging task for datacenter managers. A number of sophisticated technologies have been proposed to improve load balancing performance in a complicated circumstance, i.e., with various traffic characteristics. Many approaches need a high cost to implement, such as changing switch hardware. The efficiency problem has not been well addressed. MPTCP was proposed as a low-cost approach to improve data transmission in DCNs, which uses subflows to balance workloads across multiple paths. However, current MPTCP is not satisfying, especially when there are rack-local flows or many-to-one short flows. In this paper, we propose DCMPTCP to improve the efficacy of MPTCP. We gradually develop three mechanisms. First, DCMPTCP identifies rack-local traffic and eliminates unnecessary subflows to reduce the overhead. Second, DCMPTCP estimates flow length and establishes subflows in a smarter way. Third, DCMPTCP strengthens explicit congestion notification to improve the congestion control performance on inter-rack many-to-one short flows. DCMPTCP has a good compatibility and is easy to deploy. We implement DCMPTCP in ns-3 simulator and evaluate the performance by comprehensive simulations. The results show that DCMPTCP achieves ~65-771X and ~10-15X better FCT than MPTCP for rack-local and inter-rack traffic respectively."
PageRankVM: A PageRank Based Algorithm with Anti-Collocation Constraints for Virtual Machine Placement in Cloud Datacenters.,"There is a dramatic increase in the variety of virtual machines (VMs) and complexity of VM placement problems in clouds. Previous VM placement approaches attempt to accommodate more VMs efficiently on fewer PMs by balancing the resource usages across multiple dimensions. However, these approaches are not sufficiently accurate in measuring the quality of the PMs in terms of fully utilizing PM resource and having the potential to accommodate more VMs. Therefore, it is critical to design a new method that can more accurately measure the probability of a PM of fully utilizing its resources after accommodating a given VM with the consideration of different types of VMs. In addition, anti-collocation constraints must be handled efficiently. We propose a PageRank based VM placement algorithm with anti-collocation constraints (PageRankVM). PageRankVM defines the best PM resource usage profile, which means that the PM has full resource utilization for every resource dimension, and then ranks PM profiles according to their convergence of transferring (by accommodating VMs) to the best profile. PageRankVM then places a given VM to the PM based on the ranks of the resulted PM profiles after accommodating the VM with the consideration of anti-collocation constraints. Compared to previous approaches, PageRankVM effectively measures the ability of different PM profiles to reach the best profiles by accommodating a given VM, and hence differentiates the effectiveness of different VM placement decisions. We conducted extensive trace-driven simulation and GENI testbed experiments and demonstrated that PageRankVM has superior performance compared with other methods in terms of reducing the number of PMs, the energy consumption, the number of VM migrations, and the service level objective (SLO) violations."
Right-Sizing Server Capacity Headroom for Global Online Services.,"We present a capacity planning case study showing a significant opportunity for improving the utilization of a large, low-latency, highly available online service containing 100K+ servers spanning 9 geographic regions. Analyzing 30 PB of traces over 90 days we devised a new iterative black-box capacity planning model using the discovered relationships between workload, utilization, and quality. We verified the model on 1,000s of servers showing capacity reductions between 20% and 40% with effectively no impact on workload latency, availability, or the capacity required for disaster recovery. These results are confirmed experimentally by shrinking production server pools to cause the remaining servers to run at higher utilization, and using data from real-world large scale unplanned failures. Finally, we show examples of using our model for offline regression analysis to detect critical issues before their deployment."
Arrow: Low-Level Augmented Bayesian Optimization for Finding the Best Cloud VM.,"With the advent of big data applications, which tend to have longer execution time, choosing the right cloud VM has significant performance and economic implications. For example, in our large-scale empirical study of 107 different workloads on three popular big data systems, we found that a wrong choice can lead to a 20 times slowdown or an increase in cost by 10 times. Bayesian optimization is a technique for optimizing expensive (black-box) functions. Previous work has only used instance-level information (such as core counts and memory size) which is not sufficient to represent the search space. In this work, we discover that this may lead to the fragility problem-either incurs high search cost or finds only the sub-optimal solution. The central insight of this paper is to use low-level performance information to augment the process of Bayesian Optimization. Our novel low-level augmented Bayesian Optimization is rarely worse than current practices and often performs much better (in 46 of 107 cases). Further, it significantly reduces the search cost in nearly half of our case studies. Based on this work, we conclude that it is often insufficient to use general-purpose off-the-shelf methods for configuring cloud instances without augmenting those methods with essential systems knowledge such as CPU utilization, working memory size and I/O wait time."
Continuous and Parallel LiDAR Point-Cloud Clustering.,"In distributed digitalized environments in the context of the Internet of Things, we often need to do an analysis of big data originating at high rate-sensors at the edge of the infrastructure. A characteristic example is the light detection and ranging (LiDAR) technology, that allows sensing surrounding objects with fine-grained resolution in large areas. Their data (known as point clouds), generated continuously at very high rates, through appropriate analysis can provide information to support automated functionality in distributed cyber-physical? systems; clustering of point clouds is a key problem to extract this type of information. Methods for solving the problem in a continuous fashion can facilitate improved processing in fog architectures, through enabling low-latency, efficient continuous and streaming processing of data close to the sources; moreover, parallelism is a key requirement to exploit a variety of computing architectures in this context. We proposeLisco, a single-pass continuous Euclidean-distance-based clustering of LiDAR point clouds, that maximizes the granularity of the data processing pipeline and thus shows the potential for data-and pipeline-parallelism. We further present its parallel version, P-Lisco, that is architecture-independent and exploits the parallelism revealed byLisco'salgorithmic approach. Besides their algorithmic analysis, we provide a thorough experimental evaluation on architectures representative of high-end servers and of resource-constrained embedded devices and highlight the multiplicative improvements and scalability benefits of the proposed algorithms compared to the baseline, using both real-world datasets as well as synthetic ones to fully explore a wide spectrum of stress-levels for the algorithms."
ADWISE: Adaptive Window-Based Streaming Edge Partitioning for High-Speed Graph Processing.,"In recent years, the graph partitioning problem gained importance as a mandatory preprocessing step for distributed graph processing on very large graphs. Existing graph partitioning algorithms minimize partitioning latency by assigning individual graph edges to partitions in a streaming manner - at the cost of reduced partitioning quality. However, we argue that the mere minimization of partitioning latency is not the optimal design choice in terms of minimizing total graph analysis latency, i.e., the sum of partitioning and processing latency. Instead, for complex and long-running graph processing algorithms that run on very large graphs, it is beneficial to invest more time into graph partitioning to reach a higher partitioning quality - which drastically reduces graph processing latency. In this paper, we propose ADWISE, a novel window-based streaming partitioning algorithm that increases the partitioning quality by always choosing the best edge from a set of edges for assignment to a partition. In doing so, ADWISE controls the partitioning latency by adapting the window size dynamically at run-time. Our evaluations show that ADWISE can reach the sweet spot between graph partitioning latency and graph processing latency, reducing the total latency of partitioning plus processing by up to 23-47 percent compared to the state-of-the-art."
Edge Caching for Enriched Notifications Delivery in Big Active Data.,"In this paper, we propose a set of caching strategies for big active data (BAD) systems. BAD is a data management paradigm that allows ingestion of massive amount of data from heterogeneous sources, such as sensor data, social networks, web and crowdsourced data in a large data cluster consisting of many computing and storage nodes, and enables a very large number of end users to subscribe to those data items through declarative subscriptions. A set of distributed broker nodes connect these end users to the backend data cluster, manage their subscriptions and deliver the subscription results to the end users. Unlike the most traditional publish-subscribe systems that match subscriptions against a single stream of publications to generate notifications, BAD can match subscriptions across multiple publications (by leveraging storage in the backend) and thus can enrich notifications with a rich set of diverse contents. As the matched results are delivered to the end users through the brokers, the broker node caches the results for a while so that the subscribers can retrieve them with reduced latency. Interesting research questions arise in this context so as to determine which result objects to cache or drop when the cache becomes full (eviction-based caching) or to admit objects with an explicit expiration time indicating how much time they should reside in the cache (TTL based caching). To this end, we propose a set of caching strategies for the brokers and show that the schemes achieve varying degree of efficiency in terms of notification delivery in the BAD system. We evaluate our schemes via a prototype implementation and through detailed simulation studies."
Approaches for Resilience against Cascading Failures in Cloud Datacenters.,"In a modern cloud datacenter, a cascading failure will cause many Service Level Objective (SLO) violations. In a cascading failure, when a set of physical machines (PMs) in a failure domain are failed, their workloads are transferred to the PMs in another failure domain to continue. However, the new domain receiving additional workloads may become overloaded due to the resource oversubscription feature in the cloud, which easily leads to domain failures and subsequent workload transfer to other domains. This process repeats and a cascading failure is created finally. However, few previous methods can effectively handle the cascading failures. To handle this problem, we propose a Cascading Failure Resilience System (CFRS), which incorporates three methods: Overload-Avoidance VM Reassignment (OAVR), VM backup set placement (VMset) and Dynamic Oversubscription Ratio Adjustment (DOA). The experiments in trace-driven simulation show that CFRS outperforms other comparison methods in terms of the number of domain failures, the number of failed PMs and the number of SLO violations."
Chronos: A Unifying Optimization Framework for Speculative Execution of Deadline-Critical MapReduce Jobs.,"Meeting desired application deadlines in cloud processing systems such as MapReduce is crucial as the nature of cloud applications is becoming increasingly mission-critical and deadline-sensitive. It has been shown that the execution times of MapReduce jobs are often adversely impacted by a few slow tasks, known as stragglers, which result in high latency and deadline violations. While a number of strategies have been developed in existing work to mitigate stragglers by launching speculative or clone task attempts, none of them provide a quantitative framework that optimizes the speculative execution for offering guaranteed Service Level Agreements (SLAs) to meet application deadlines. In this paper, we bring several speculative scheduling strategies together under a unifying optimization framework, called Chronos, which defines a new metric, Probability of Completion before Deadlines (PoCD), to measure the probability that MapReduce jobs meet their desired deadlines. We systematically analyze PoCD for popular strategies including Clone, Speculative-Restart, and Speculative-Resume, and quantify their PoCD in closed-form. The results illuminate an important tradeoff between PoCD and the cost of speculative execution, measured by the total (virtual) machine time required under different strategies. We propose an optimization problem to jointly optimize PoCD and execution cost in different strategies, and develop an algorithmic solution that is guaranteed to be optimal. Chronos is prototyped on Hadoop MapReduce and evaluated against three baseline strategies using both experiments and trace-driven simulations, and achieves 50% net utility increase with up to 80% PoCD and 88% cost improvements."
SGX-Aware Container Orchestration for Heterogeneous Clusters.,"Containers are becoming the de facto standard to package and deploy applications and micro-services in the cloud. Several cloud providers (e.g., Amazon, Google, Microsoft) begin to offer native support on their infrastructure by integrating container orchestration tools within their cloud offering. At the same time, the security guarantees that containers offer to applications remain questionable. Customers still need to trust their cloud provider with respect to data and code integrity. The recent introduction by Intel of Software Guard Extensions (SGX) into the mass market offers an alternative to developers, who can now execute their code in a hardware-secured environment without trusting the cloud provider. This paper provides insights regarding the support of SGX inside Kubernetes, an industry-standard container orchestrator. We present our contributions across the whole stack supporting execution of SGX-enabled containers. We provide details regarding the architecture of the scheduler and its monitoring framework, the underlying operating system support and the required kernel driver extensions. We evaluate our complete implementation on a private cluster using the real-world Google Borg traces. Our experiments highlight the performance trade-offs that will be encountered when deploying SGX-enabled micro-services in the cloud."
Efficient Sharing and Fine-Grained Scheduling of Virtualized GPU Resources.,"Graphics Processing Unit (GPU) provides acceleration services to many applications, such as AI, games, media transcoding, etc. Virtualization on GPU is an enabling technology which facilitates the hardware resource sharing among multiple virtual machines (VMs). Sharing a GPU not only brings pros such as high utilization but also introduces cons such as resource contention and performance degradation. Although the existing GPU scheduling policies have been to some extent optimized, there are still some deficiencies, such as inefficient GPU sharing among multiple VMs, and high overhead within VM switching. As a result, the performance of GPU virtualization is limited by the current design, which lacks fine-grained scheduling supports. In this paper, we propose the Fine-grained schEduLing of vIrtualized gPu rEsources (FELIPE) to fully utilize and efficiently share a physical GPU among multiple VMs. To this end, we achieve the FELIPE optimization by introducing fine-grained scheduling mechanisms for virtualized GPU resources in three aspects: 1) We design a mixed time/event-based scheduling policy to reduce the idle time within VM switching. 2) We create a seamless VM assignment process, which enables VMs to switch seamlessly by stages. 3) We develop a hybrid per-ring/VM scheduling strategy, which schedules workloads to different GPU engines to run simultaneously. Then we implement the FELIPE with Intel Graphics Virtualization Technology for shared vGPU technology (GVT-g). Finally, the experimental evaluations show that the performance of the first two scheduling policies can respectively achieve up to 21.5% and 19.7% improvement, and the last one can improve the performance from 57.9% to 98.5% compared with the native design for two virtual machines."
Wireless Aggregation at Nearly Constant Rate.,"One of the most fundamental tasks in sensor networks is the computation of a (compressible) aggregation function of the input measurements. What rate of computation can be maintained, by properly choosing the aggregation tree, the TDMA schedule of the tree edges, and the transmission powers? This can be viewed as the convergecast capacity of a wireless network. We show here that the optimal rate is effectively a constant. This holds even in arbitrary networks, under the physical model of interference. This compares with previous bounds that are logarithmic (e.g., Î©(1/log n)). Namely, we show that a rate of Î©(1/log* Î”) is possible, where Î” is the length diversity (ratio between the furthest to the shortest distance between nodes). It also implies that the scheduling complexity of wireless connectivity is O(log* Î”). This is achieved using the natural minimum spanning tree (MST). Our method crucially depends on choosing the appropriate power assignment for the instance at hand, since without power control, only a trivial O(1/n) rate can be guaranteed. We also show that there is a fixed power assignment that allows for a rate of Î”(1/log log Î”). Surprisingly, these bounds are essentially best possible. No aggregation network can guarantee a rate better than O(1/log log Î”) using fixed power assignment. Also, when using arbitrary power control, there are instances whose MSTs cannot be scheduled in fewer than Î©(1/log* Î”) slots."
Fast and Efficient Distributed Computation of Hamiltonian Cycles in Random Graphs.,"We present fast and efficient randomized distributed algorithms to find Hamiltonian cycles in random graphs. In particular, we present a randomized distributed algorithm for the G(n, p) random graph model, with number of nodes n and p = c ln n/n^Î´ (for any constant 0 <; Î´ â‰¤ 1 and for a suitably large constant c > 0), that finds a Hamiltonian cycle with high probability in Ã•(n^Î´) rounds. Our algorithm works in the (synchronous) CONGEST model (i.e., only O(log n)-sized messages are communicated per edge per round) and its computational cost per node is sublinear (in n) per round and is fully-distributed (each node uses only o(n) memory and all nodes' computations are essentially balanced). Our algorithm improves over the previous best known result in terms of both the running time as well as the edge sparsity of the graphs where it can succeed; in particular, the denser the random graph, the smaller is the running time."
Group Exploration of Dynamic Tori.,"Mobile agents (agents) are activities which can move autonomously in a networked system and execute actions at visited nodes. One of the most fundamental problems of agents is exploration, which requires that each node should be visited by at least one agent. For a long time, researchers focus on exploration of static networks. However, exploration of dynamic networks comes to be studied recently. In this paper, we consider exploration of a dynamic torus under some constraints on the dynamics (or topology changes). An n Ã— m torus (3 â‰¤ n â‰¤ m) is considered as a collection of n row rings and m column rings. The constraint on the dynamics is that each ring should be 1-interval connected, which allows at most one link to be missing at any time in each ring. On this n Ã— m dynamic torus, we propose exploration algorithms with and without the link presence detection. With the link presence detection, an agent can detect which incident links are missing (if exist) before determining its next move. On the other hand, without the link presence detection, an agent has to determine its next move without knowing which incident links are missing, which makes the agent stay on the same node when the link necessary to the move is missing. We prove for exploration of the n Ã— m dynamic torus that, without the link presence detection, n+1 agents are necessary and sufficient, and, with the link presence detection, âŒˆn/2âŒ‰ + 1 agents are necessary and sufficient. Moreover, for both cases, we propose asymptotically optimal algorithms with respect to both the numbers of agents and rounds when n = m."
"Slow Links, Fast Links, and the Cost of Gossip.","Consider the classical problem of information dissemination: one (or more) nodes in a network have some information that they want to distribute to the remainder of the network. In this paper, we study the cost of information dissemination in networks where edges have latencies, i.e., sending a message from one node to another takes some amount of time. We first generalize the idea of conductance to weighted graphs by defining Ï†
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">*</sub>
 to be the ""critical conductance"" and â„“
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">*</sub>
 to be the ""critical latency"". One goal of this paper is to argue that Ï†
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">*</sub>
 characterizes the connectivity of a weighted graph with latencies in much the same way that conductance characterizes the connectivity of unweighted graphs. We give near tight lower and upper bounds on the problem of information dissemination, up to polylogarithmic factors. Specifically, we show that in a graph with (weighted) diameter d (with latencies as weights) and maximum degree Î”, any information dissemination algorithm requires at least Î”(min(D+Î”, â„“
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">*</sub>
/Ï†
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">*</sub>
)) time in the worst case. We show several variants of the lower bound (e.g., for graphs with small diameter, graphs with small max-degree, etc.) by reduction to a simple combinatorial game. We then give nearly matching algorithms, showing that information dissemination can be solved in O(min((D+Î”)log
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">3</sup>
 n, (â„“
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">*</sub>
/Ï†;
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">*</sub>
)\log n) time. This is achieved by combining two cases. We show that the classical push-pull algorithm is (near) optimal when the diameter or the maximum degree is large. For the case where the diameter and the maximum degree are small, we give an alternative strategy in which we first discover the latencies and then use an algorithm for known latencies based on a weighted spanner construction. (Our algorithms are within polylogarithmic factors of being tight both for known and unknown latencies.) While it is easiest to express our bounds in terms of Ï†
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">*</sub>
 and â„“
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">*</sub>
, in some cases they do not provide the most convenient definition of conductance in weighted graphs. Therefore, we give a second (nearly) equivalent characterization, namely the average conductance Ï†
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">avg</sub>
."
CADET: Investigating a Collaborative and Distributed Entropy Transfer Protocol.,"The generation of random numbers has traditionally been a task confined to the bounds of a single piece of hardware. However, with the rapid growth and proliferation of resource-constrained devices in the Internet of Things (IoT), standard methods of generating randomness encounter barriers that can limit their effectiveness. In this work, we explore the design, implementation, and efficacy of a Collaborative and Distributed Entropy Transfer protocol (CADET), which aims to move random number generation from an individual task to a collaborative one. Through the sharing of excess random data, devices that are unable to meet their own needs can be aided by contributions from other devices. We implement and test a proof-of-concept version of CADET on a testbed of 49 Raspberry Pi 3B single-board computers, which have been underclocked to emulate the resource constraints of IoT devices. Through this, we evaluate and demonstrate the efficacy and baseline performance of remote entropy protocols of this type, as well as highlight remaining research questions and challenges in this area."
"I(TS, CS): Detecting Faulty Location Data in Mobile Crowdsensing.","Mobile Crowdsensing (MCS) is a promising paradigm that utilizes ubiquitous mobile devices to collect environmental data. Specially, location data is critical among all kinds of data because most MCS applications are location-based. Faulty data and missing values, however, may exist in the collected location data due to various reasons. This brings forth an important issue of detecting faulty location data in the presence of missing values. To address this issue, we propose I(TS, CS), a joint faulty data detection framework that combines TimeSeries and Compressive Sensing techniques. The framework adopts a DETECT-and-CORRECT approach to iteratively detect faulty data and reconstruct the dataset, which bypasses the tradeoff between false positive ratio (Type-I error) and false negative ratio (Type-II error), and thus detects more faulty data without increasing False Positive Rate. We have evaluated the proposed I(TS, CS) framework based on a real trace consisting of the trajectories of 2000 taxies, showing I(TS, CS) dramatically improve the performance of both faulty data detection and data reconstruction."
UniLoc: A Unified Mobile Localization Framework Exploiting Scheme Diversity.,"Current localization schemes on mobile devices are experiencing great diversity that is mainly shown in two aspects: the large number of available localization schemes and their diverse performance. This paper presents UniLoc, a unified framework that gains improved performance from multiple localization schemes by exploiting their diversity. UniLoc predicts the localization error of each scheme online based on an error model and real-time context. It further combines the results of all available schemes based on the error prediction results and an ensemble learning algorithm. The combined result is more accurate than any individual schemes. With the flexible design of error modeling and ensemble learning, UniLoc can easily integrate a new localization scheme. The energy consumption of UniLoc is low, since its computation, including both error prediction and ensemble learning, only involves simple linear calculation. Our experience with extensive experiments tells that such easy aggregation incurs little overhead in integrating and training a localization scheme, but gains substantially from the scheme diversity. UniLoc outperforms individual localization schemes by 1.6X in a variety of environments, including >89% new places where we did not train the error models."
FOCES: Detecting Forwarding Anomalies in Software Defined Networks.,"A crucial requirement for Software Defined Network (SDN) is that data plane forwarding behaviors should always agree with control plane policies. Such requirement cannot be met when there are forwarding anomalies, where packets deviate from the paths specified by the controller. Most anomaly detection methods for SDN install dedicated rules to collect statistics of each flow, and check whether the statistics conform to the flow conservation principle. Such per-flow detection methods have a limited detection scope: they look at one flow each time, thus can only check a limited number of flows simultaneously. In addition, dedicated rules for statistics collection can impose a large overhead on flow tables of SDN switches. To this end, this paper presents FOCES, a network-wide forwarding anomaly detection method in SDN. Different from previous methods, FOCES applies a new kind of flow conservation principle at network wide, and can check forwarding behaviors of all flows in the network simultaneously, without installing any dedicated rules. Experiments show FOCES can achieve a detection precision higher than 90% for four network topologies, even when packet loss rates are as high as 10%."
AliDrone: Enabling Trustworthy Proof-of-Alibi for Commercial Drone Compliance.,"Commercial use of Unmanned Aerial Vehicles (UAVs), or drones, promises to revolutionize the way in which consumers interact with retail services. However, the further adoption of UAVs has been significantly impeded by an overwhelming public outcry over the privacy implications of drone technology. While lawmakers have attempted to establish standards for drone use (e.g., No-Fly-Zones (NFZs)), at present a general technical mechanism for policy enforcement eludes state-of-the-art drones. In this work, we propose that Proof-of-Alibi (PoA) protocols should serve as the basis for enforcing drone privacy compliance. We design and implement AliDrone, a trustworthy PoA protocol that enables individual drones to prove their compliance with NFZs to a third party Auditor. AliDrone leverages trusted hardware to produce cryptographically-signed GPS readings within a secure enclave, preventing malicious drone operators from being able to forge geo-location information. AliDrone features an adaptive sampling algorithm that reacts to NFZ proximity in order to minimize the processing cost. Through laboratory benchmarks and field studies, we demonstrate that AliDrone provides strong assurance of geo-location while imposing an average of 1.5% overhead on CPU utilization and 0.3% of memory consumption. AliDrone thus enables the further proliferation of drone technology through the introduction of a trustworthy and accountable compliance mechanism."
ZebraLancer: Private and Anonymous Crowdsourcing System atop Open Blockchain.,"We design and implement the first private and anonymous decentralized crowdsourcing system ZebraLancer, and overcome two fundamental challenges of decentralizing crowdsourcing, i.e. data leakage and identity breach. First, our outsource-then-prove methodology resolves the tension between blockchain transparency and data confidentiality, which is critical in crowdsourcing use-case. ZebraLancer ensures: (i) a requester will not pay more than what data deserve, according to a policy announced when her task is published via the blockchain; (ii) each worker indeed gets a payment based on the policy, if he submits data to the blockchain; (iii) the above properties are realized not only without a central arbiter, but also without leaking the data to the open blockchain. Furthermore, the transparency of blockchain allows one to infer private information about workers and requesters through their participation history. On the other hand, allowing anonymity will enable a malicious worker to submit multiple times to reap rewards. ZebraLancer overcomes this problem by allowing anonymous requests/submissions without sacrificing the accountability. The idea behind is a subtle linkability: if a worker submits twice to a task, anyone can link the submissions, or else he stays anonymous and unlinkable across tasks. To realize this delicate linkability, we put forward a novel cryptographic concept, i.e. the common-prefix-linkable anonymous authentication. We remark the new anonymous authentication scheme might be of independent interest. Finally, we implement our protocol for a common image annotation task and deploy it in a test net of Ethereum. The experiment results show the applicability of our protocol with the existing real-world blockchain."
Path MTU Discovery Considered Harmful.,"Path MTU Discovery (PMTUD) allows to optimize the performance in the Internet by identifying the maximal packet size that can be transmitted through a network. Despite the central role that PMTUD plays in the Internet communication, it has a long history of software bugs, failures and misconfigurations. In this work we explore the benefits versus drawbacks of PMTUD in the Internet from the clients and servers perspective. First, we examine the fraction of clients that use PMTUD. To that end we analyse ICMP PTB messages in CAIDA Internet Traces and show that the fraction of networks using PMTUD is negligible and that this number is further decreasing over the period of 2008 - 2016. Second, we evaluate the fraction of popular web servers that support the PMTUD mechanism and show that a large number of the servers block ""ICMP packet too big"" messages. On the other hand, we show easy and efficient - even though well-known - degradation of service attacks that exploit the availability of PMTUD. Since the benefit of PMTUD is questionable, and in contrast it exposes to degradation of service attacks, we advocate to stop using it. As with any new change in the Internet, the implications of our recommendation should be carefully evaluated and gradually implemented. In the meanwhile, we provide recommendations for mitigations against the degradation of service attacks."
SSD-Insider: Internal Defense of Solid-State Drive against Ransomware with Perfect Data Recovery.,"Ransomware is a malware that encrypts victim's data, where the decryption key is released after a ransom is paid by the data owner to the attacker. Many ransomware attacks were reported recently, making anti-ransomware a crucial need in security operation, and an issue for the security community to tackle. In this paper, we propose a new approach to defending against ransomware inside NAND flash-based SSDs. To realize the idea of defense-inside-SSDs, both a lightweight detection technique and a perfect recovery algorithm to be used as a part of SSDs firmware should be developed. To this end, we propose a new set of lightweight behavioral features on ran-somware's overwriting pattern, which are invariant across various ransomwares. Our features rely on observing the block I/O request headers only, and not the payload. For perfect and instant recovery, we also propose using the delayed deletion feature of SSDs, which is intrinsic to NAND flash. To demonstrate their feasibility, we implement our algorithms atop an open-channel SSD as a working prototype called SSD-Insider. In experiments using eight real-world and two in-house ransomwares with various background applications running, SSD-Insider achieved a detection accuracy 0% FRR/FAR in most scenarios, and only 5% FAR when heavy overwriting resembling ransomware's data wiping occurs. SSD-Insider detects ransomware activity within 10s, and recovers instantly an infected SSD within 1s with 0% data loss. The additional software overheads incurred by the SSD-Insider is just 147 ns and 254 ns for 4-KB reads and writes, respectively, which is negligible considering NAND chip latency (50-1000 Î¼s)."
Token Account Algorithms: The Best of the Proactive and Reactive Worlds.,"Many decentralized algorithms allow both proactive and reactive implementations. Examples include gossip protocols for broadcasting and decentralized computing, as well as chaotic matrix iteration algorithms. In proactive systems, nodes communicate at a fixed rate in regular intervals, while in reactive systems they communicate in response to certain events such as the arrival of fresh data. Although reactive algorithms tend to stabilize/converge/self-heal much faster, they have serious drawbacks: they may cause bursts in bandwidth consumption, and they may also cause starvation when the number of messages circulating in the system becomes too low. Proactive algorithms do not have these problems, but nodes waste a lot of time sitting on fresh information. Here, we propose a novel family of adaptive protocols that apply rate limiting inspired by the token bucket algorithm to prevent bursts, but they also include proactive communication to prevent starvation. With the help of our traffic shaping service, some applications approach the speed of the reactive implementation, while maintaining strong guarantees regarding the total communication cost and burstiness. Due to the proactive component we can help maintain a certain level of activity despite losing messages due to faults or the application semantics. We perform simulation experiments in different scenarios including a real smartphone availability trace. Our results suggest up to a fourfold speedup in a broadcast application, and an order of magnitude speedup in the case of gossip learning, when compared to the purely proactive implementation."
ACCIO: How to Make Location Privacy Experimentation Open and Easy.,"The advent of mobile applications collecting and exploiting the location of users opens a number of privacy threats. To mitigate these privacy issues, several protection mechanisms have been proposed this last decade to protect users' location privacy. However, these protection mechanisms are usually implemented and evaluated in monolithic way, with heterogeneous tools and languages. Moreover, they are evaluated using different methodologies, metrics and datasets. This lack of standard makes the task of evaluating and comparing protection mechanisms particularly hard. In this paper, we present ACCIO, a unified framework to ease the design and evaluation of protection mechanisms. Thanks to its Domain Specific Language, ACCIO allows researchers and practitioners to define and deploy experiments in an intuitive way, as well as to easily collect and analyse the results. ACCIO already comes with several state-of-the-art protection mechanisms and a toolbox to manipulate mobility data. Finally, ACCIO is open and easily extensible with new evaluation metrics and protection mechanisms. This openness, combined with a description of experiments through a user-friendly DSL, makes ACCIO an appealing tool to reproduce and disseminate research results easier. In this paper, we present ACCIO's motivation and architecture, and demonstrate its capabilities through several use cases involving multiples metrics, state-of-the-art protection mechanisms, and two real-life mobility datasets collected in Beijing and in the San Francisco area."
Improving Asynchronous Invocation Performance in Client-Server Systems.,"In this paper, we conduct an experimental study of asynchronous invocation on the performance of client-server systems. Through extensive measurements of both realistic macro-and micro-benchmarks, we show that servers with the asynchronous event-driven architecture may perform significantly worse than the thread-based version resulting from two nontrivial reasons. First, the traditional wisdom of one-event-one-handler event processing flow can create large amounts of intermediate context switches that significantly degrade the performance of an asynchronous server. Second, some runtime workload (e.g., response size) and network conditions (e.g., network latency) may cause significant negative performance impact on the asynchronous event-driven servers, but not on threadbased ones. We provide a hybrid solution by taking advantage of different asynchronous architectures to adapt to varying workload and network conditions. Our hybrid solution searches for the most efficient execution path for each client request based on the runtime request profiling and type checking. Our experimental results show that the hybrid solution outperforms all the other types of servers up to 19%~90% on throughput, depending on specific workload and network conditions."
Fast Lookup Is Not Enough: Towards Efficient and Scalable Flow Entry Updates for TCAM-Based OpenFlow Switches.,"With an increasing demand for flexible management in software-defined networks (SDNs), it becomes critical to minimize the network policy update time. Although major SDN controllers are now optimized for rapid network update at the control plane, there is still room for data plane optimization in terms of update time, when using TCAM-based physical SDN commodity-off-the-shelf switches. A slow update directly affects network performance creating bottlenecks. To minimize flow entry update time, a dependency graph, a kind of DAG (directed acyclic graph), can be used for the access management of flow entries at the switch. Thanks to the DAG, unnecessary entry movements, which are the main factor slowing down flow entry updates, can be avoided. However, existing algorithms show limitations when updates become very frequent. We propose a new flow entry update algorithm, called FastRule, that exploits a greedy strategy with an efficient data structure to accelerate flow entry update with a DAG approach. Moreover, we also adjust our algorithm for other flow table layouts to make it scalable. We elaborate on the correctness of FastRule and test our algorithm using a hardware switch. Compared with existing algorithms, the evaluation shows that our algorithm is about 100x faster than state-of-the-art solutions with a flow table of 1k line size."
FlowTime: Dynamic Scheduling of Deadline-Aware Workflows and Ad-Hoc Jobs.,"With rapidly increasing volumes of data to be processed in modern data analytics, it is commonplace to run multiple data processing jobs with inter-job dependencies in a datacenter cluster, typically as recurring data processing workloads. Such a group of inter-dependent data analytic jobs is referred to as a workflow, and may have a deadline due to its mission-critical nature. In contrast, non-recurring ad-hoc jobs are typically best-effort in nature, and rather than meeting deadlines, it is desirable to minimize their average job turnaround time. The state-of-the-art scheduling mechanisms focused on meeting deadlines for individual jobs only, and are oblivious to workflow deadlines. In this paper, we present FlowTime, a new system framework designed to make scheduling decisions for workflows so that their deadlines are met, while simultaneously optimizing the performance of ad-hoc jobs. To achieve this objective, we first adopt a divide-and-conquer strategy to transform the problem of workflow scheduling to a deadline-aware job scheduling problem, and then design an efficient algorithm that tackles the scheduling problem with both deadline-aware jobs and ad-hoc jobs by solving its corresponding optimization problem directly using a linear program solver. Our experimental results have clearly demonstrated that FlowTime achieves the lowest deadline-miss rates for deadline-aware workflows and 2-10 times shorter average job turnaround time, as compared to the state-of-the-art scheduling algorithms."
To Sell or Not To Sell: Trading Your Reserved Instances in Amazon EC2 Marketplace.,"Recently, Amazon EC2 offers a reserved instance marketplace, where cloud users can sell their idle reserved instances varying in contract lengths and pricing options for avoiding the waste of their unused reservations. However, without knowing the future demands, it is hard for users to determine how to sell instances optimally, for it would incur more cost if new demands arrive after selling their reserved instances. For dealing with this problem, in this paper we first propose three online selling algorithms to guide cloud users in making decisions whether or not to sell their reservations in Amazon EC2 marketplace while guaranteeing competitive ratios. We prove theoretically that the three proposed online algorithms can guarantee bounded competitive ratios, whose values are specific to the type of reserved instances under consideration. Specifically, for all standard instances (Linux, US East) for 1-year terms in Amazon EC2, compared with a benchmark optimal offline algorithm, our algorithm A3T/4 can achieve a ratio of 2-Î±-a/4 in managing instance purchasing cost, where Î± is the entitled discount due to reservation and a is the selling discount specified by the user who sells its reservations. Finally, through extensive experiments based on workload data collected from real-world applications, we validate the effectiveness of our online instance selling algorithms by showing that it can bring significant cost savings to cloud users compared with always keeping their reservations in Amazon EC2 reserved instance marketplace."
ROSE: Cluster Resource Scheduling via Speculative Over-Subscription.,"A long-standing challenge in cluster scheduling is to achieve a high degree of utilization of heterogeneous resources in a cluster. In practice there exists a substantial disparity between perceived and actual resource utilization. A scheduler might regard a cluster as fully utilized if a large resource request queue is present, but the actual resource utilization of the cluster can be in fact very low. This disparity results in the formation of idle resources, leading to inefficient resource usage and incurring high operational costs and an inability to provision services. In this paper we present a new cluster scheduling system, ROSE, that is based on a multi-layered scheduling architecture with an ability to over-subscribe idle resources to accommodate unfulfilled resource requests. ROSE books idle resources in a speculative manner: instead of waiting for resource allocation to be confirmed by the centralized scheduler, it requests intelligently to launch tasks within machines according to their suitability to oversubscribe resources. A threshold control with timely task rescheduling ensures fully-utilized cluster resources without generating potential task stragglers. Experimental results show that ROSE can almost double the average CPU utilization, from 36.37% to 65.10%, compared with a centralized scheduling scheme, and reduce the workload makespan by 30.11%, with an 8.23% disk utilization improvement over other scheduling strategies."
MPCSToken: Smart Contract Enabled Fault-Tolerant Incentivisation for Mobile P2P Crowd Services.,"Mobile peer to peer (P2P) networks offer a huge potential for distributed mobile P2P crowd services (MPCS), which enable data and computational tasks to be offloaded and executed directly between mobile devices. Similar to centralised mobile crowd services, such as mobile crowdsensing, incentivisation mechanisms are core to encouraging mobile users to participate in MPCS systems. However, due to the impact of task execution failures and unreliable behaviours of mobile users (particularly task requesters), it is a daunting task to design and implement an incentivisation mechanism to cater for the needs of MPCS systems. In this paper, we propose a fault-tolerant incentivisation mechanism (FTIM) for MPCS systems. With conditional payment strategies, FTIM is proven to accommodate the requirements of two important application scenarios by achieving mechanism properties such as incentive compatibility, economic efficiency, individual rationality, and weak budget balance. Moreover, to tackle the practical challenges in implementing FTIM in the real world, we design a MPCSTo-ken smart contract to facilitate its service auction, task execution and payment settlement process. We implement the MPCSToken contract on Ethereum blockchain. Both real-world experiment and simulation results show that the system is cost effective for deployments and improves the overall mobile users' utility by exploring the opportunities offered by MPCS."
A Decentralized Medium Access Protocol for Real-Time Wireless Ad Hoc Networks With Unreliable Transmissions.,"This paper proposes a feasibility-optimal decentralized algorithm for real-time wireless ad hoc networks, where a strict deadline is imposed for each packet. While centralized scheduling algorithms provide provably optimal theoretical guarantees, they may not be practical in many settings, such as industrial networked control systems. Therefore, it is of great importance to design an algorithm that achieves feasibility optimality in a decentralized manner. To design a decentralized algorithm, we leverage two widely-used functions of wireless devices: carrier sensing and backoff timers. Different from the conventional approach, the proposed algorithm utilizes a collision-free backoff scheme to enforce the transmission priority of different links. This design obviates the capacity loss due to collision with quantifiably small backoff overhead. The algorithm is fully decentralized in the sense that every link only needs to know its own priority, and links contend for priorities only through carrier sensing. We prove that the proposed algorithm is feasibility-optimal. NS-3 simulation results show that the proposed algorithm indeed performs as well as the feasibility-optimal centralized algorithm. Moreover, the results also show that the proposed algorithm converges to optimality very fast."
TurboStream: Towards Low-Latency Data Stream Processing.,"Data Stream Processing (DSP) applications are often modelled as a directed acyclic graph: operators with data streams among them. Inter-operator communications can have a significant impact on the latency of DSP applications, accounting for 86% of the total latency. Despite their impact, there has been relatively little work on optimizing inter-operator communications, focusing on reducing inter-node traffic but not considering inter-process communication (IPC) inside a node, which often generates high latency due to the multiple memory-copy operations. This paper describes the design and implementation of TurboStream, a new DSP system designed specifically to address the high latency caused by inter-operator communications. To achieve this goal, we introduce (1) an improved IPC framework with OSRBuffer, a DSP-oriented buffer, to reduce memory-copy operations and waiting time of each single message when transmitting messages between the operators inside one node, and (2) a coarse-grained scheduler that consolidates operator instances and assigns them to nodes to diminish the inter-node IPC traffic. Using a prototype implementation, we show that our improved IPC framework reduces the end-to-end latency of intra-node IPC by 45.64% to 99.30%. Moreover, TurboStream reduces the latency of DSP by 83.23% compared to JStorm."
Consume Local: Towards Carbon Free Content Delivery.,"P2P sharing amongst consumers has been proposed as a way to decrease load on Content Delivery Networks. This paper develops an analytical model that shows an 
<i xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">additional</i>
 benefit of sharing content locally: Selecting close by peers to share content from leads to shorter paths compared to traditional CDNs, decreasing the overall carbon footprint of the system. Using data from a month-long trace of over 3 million monthly users in London accessing TV shows online, we show that local sharing can result in a decrease of 24-48% in the system-wide carbon footprint of online video streaming, despite various obstacle factors that can restrict swarm sizes. We confirm the robustness of the savings by using realistic energy parameters drawn from two widely used settings. We also show that if the energy savings of the CDN servers are transferred as carbon credits to the end users, over 70% of users can become carbon positive, i.e., are able to support their content consumption without incurring any carbon footprint, and are able to offset their other carbon consumption. We suggest carbon credit transfers from CDNs to end users as a novel way to incentivise participation in peer-assisted content delivery."
Scalable Transaction Processing Using Functors.,"Distributed transactions, which access data items at multiple sites atomically, face well-known scalability challenges. To avoid the high overhead, in prior work Fan et al. proposed Epoch-based Concurrency Control (ECC), which makes transactions visible at epoch boundaries, and presented a system that supports high performance read-only and write-only transactions. However, this idea has a clear difficulty to overcome: the common case of a single transaction that does both reading and writing. This paper proposes ALOHA-DB, a scalable distributed transaction processing system. ALOHA-DB uses a novel paradigm of serializable transaction processing using functors, which conceptually resemble futures in modern programming languages. A functor is a placeholder for the value of a key, which can be computed asynchronously in the future in parallel with other functor computations of the same or other transactions. With multi-versioning in ECC, the functor computations only rely on accessing historical versions, and so the traditional locking mechanism is not needed for concurrency control. Functors elevate ECC to a new level: supporting serializable distributed read-write transactions. This combination of techniques never aborts transactions due to read-write or write-write conflicts, but allows transactions to fail due to logic errors or constraint violations. We used functor-enabled ECC to implement ALOHA-DB and evaluated it using TPC-C and YCSB read-write distributed transactions. Experimental results demonstrate that our system's performance on the TPC-C benchmark is nearly 2 million transactions per second over 20 eight-core virtual machines, which outperforms Calvin, a state-of-the-art transaction processing and replication layer, by one to two orders of magnitude."
HaaS: Cloud-Based Real-Time Data Analytics with Heterogeneity-Aware Scheduling.,"Real-time data analytics has become increasingly important in modern times as many organizations and companies are generating and analyzing high volume of data constantly. Despite of the impressive technical development, it remains a challenging job to analyze the stream data effectively and efficiently because traditional hardware and software lack specific designs and optimizations for those emerging requirements. In this paper, we discuss our experience on real-time data analytics, with our in-house processing framework HaaS. HaaS is designed to exploit existing data analytics tools and libraries as well as distributed computing technologies to embrace heterogeneous computation resources in the cloud. HaaS utilizes hierarchical clustering to partition physical topology of clusters weighted with task topology information into densely connected sub-graphs. HaaS is also equipped with a heterogeneity-aware scheduling algorithm to facilitate holistic optimization over multiple running tasks with various service level agreements. To the best of our knowledge, HaaS is the first ever streaming analytical framework providing users with flexible and optimized usage with CPUs, GPUs and FPGAs in the cloud. Users with stream processing tasks can easily enjoy remarkable advantages of CPUs, GPUs and FPGAs in throughput, power consumption and monetary cost over others. In our empirical evaluations with highly diversified workloads, HaaS saves over 18% on power consumption and 24% on monetary cost over existing system design architecture, while the overall throughput of HaaS remains no lower than 90% of the theoretical limit."
BeeFlow: A Workflow Management System for In Situ Processing across HPC and Cloud Systems.,"In this paper, we propose BeeFlow - an in situ analysis enabled workflow management system across multiple platforms using Docker containers. BeeFlow can support both traditional workflows as well as workflows with in situ analysis. BeeFlow leverages Docker containers to provide a portable, flexible, and reproducible workflow management system across HPC and cloud platforms. We showcase how current in situ visualization workflows can apply BeeFlow with DOE production codes VPIC and Flecsale."
SQLoop: High Performance Iterative Processing in Data Management.,"Increasingly more iterative and recursive query tasks are processed in data management systems, such as graph-structured data analytics, demanding fast response time. However, existing CTE-based recursive SQL and its implementation ineffectively respond to this intensive query processing with two major drawbacks. First, its iteration execution model is based on implicit set-oriented terminating conditions that cannot express aggregation-based tasks, such as PageRank. Second, its synchronous execution model cannot perform asynchronous computing to further accelerate execution in parallel. To address these two issues, we have designed and implemented SQLoop, a framework that extends the semantics of current SQL standard in order to accommodate iterative SQL queries. SQLoop interfaces between users and different database engines with two powerful components. First, it provides an uniform SQL expression for users to access any database engine so that they do not need to write database dependent SQL or move datasets from a target engine to process in their own sites. Second, SQLoop automatically parallelizes iterative queries that contain certain aggregate functions in both synchronous and asynchronous ways. More specifically, SQLoop is able to take advantage of intermediate results generated between different iterations and to prioritize the execution of partitions that accelerate the query processing. We have tested and evaluated SQLoop by using three popular database engines with real-world datasets and queries, and shown its effectiveness and high performance."
LogLens: A Real-Time Log Analysis System.,"Administrators of most user-facing systems depend on periodic log data to get an idea of the health and status of production applications. Logs report information, which is crucial to diagnose the root cause of complex problems. In this paper, we present a real-time log analysis system called 
<i xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">LogLens</i>
 that automates the process of anomaly detection from logs with no (or minimal) target system knowledge and user specification. In 
<i xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">LogLens</i>
, we employ unsupervised machine learning based techniques to discover patterns in application logs, and then leverage these patterns along with the real-time log parsing for designing advanced log analytics applications. Compared to the existing systems which are primarily limited to log indexing and search capabilities, 
<i xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">LogLens</i>
 presents an extensible system for supporting both stateless and stateful log analysis applications. Currently, 
<i xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">LogLens</i>
 is running at the core of a commercial log analysis solution handling millions of logs generated from the large-scale industrial environments and reported up to 12096x man-hours reduction in troubleshooting operational problems compared to the manual approach."
Design of Global Data Deduplication for a Scale-Out Distributed Storage System.,"Scale-out distributed storage systems can uphold balanced data growth in terms of capacity and performance on an on-demand basis. However, it is a challenge to store and manage large sets of contents being generated by the explosion of data. One of the promising solutions to mitigate big data issues is data deduplication, which removes redundant data across many nodes of the storage system. Nevertheless, it is non-trivial to apply a conventional deduplication design to the scale-out storage due to the following root causes. First, chunk-lookup for deduplication is not as scalable and extendable as the underlying storage system supports. Second, managing the metadata associated to deduplication requires a huge amount of design and implementation modifications of the existing distributed storage system. Lastly, the data processing and additional I/O traffic imposed by deduplication can significantly degrade performance of the scale-out storage. To address these challenges, we propose a new deduplication method, which is highly scalable and compatible with the existing scale-out storage. Specifically, our deduplication method employs a double hashing algorithm that leverages hashes used by the underlying scale-out storage, which addresses the limits of current fingerprint hashing. In addition, our design integrates the meta-information of file system and deduplication into a single object, and it controls the deduplication ratio at online by being aware of system demands based on post-processing. We implemented the proposed deduplication method on an open source scale-out storage. The experimental results show that our design can save more than 90% of the total amount of storage space, under the execution of diverse standard storage workloads, while offering the same or similar performance, compared to the conventional scale-out storage."
Stad: Stateful Diffusion for Linear Time Community Detection.,"Community detection is one of the preeminent topics in network analysis. Communities in real-world networks vary in their characteristics, such as their internal cohesion and size. Despite a large variety of methods proposed to detect communities so far, most of existing approaches fall into the category of global approaches. Specifically, these global approaches adapt their detection model focusing on approximating the global structure of the whole network, instead of performing approximation at the communities level. Global techniques tune their parameters to ""one size fits all model, so they are quite successful with extracting communities in homogeneous cases but suffer in heterogeneous community size distributions. %Furthermore, majority of existing techniques target extracting disjoint communities. In this paper, we present a stateful diffusion approach (Stad) for community detection that employs diffusion. Stad boosts diffusion with a conductance-based function that acts like a tuning parameter to control the diffusion speed. In contrast to existing diffusion mechanisms which operate with global and fixed speed, Stad introduces stateful diffusion to treat every community individually. Particularly, Stad controls the diffusion speed at node level, such that each node determines the diffusion speed associated with every possible community membership independently. Thus, Stad is able to extract communities more accurately in heterogeneous cases by dropping ""one size fits all"" model. Furthermore, Stad employs a vertex-centric approach which is fully decentralized and highly scalable, and requires no global knowledge. So as, Stad can be successfully applied in distributed environments, such as large-scale graph processing or decentralized machine learning. The results with both real-world and synthetic datasets show that Stad outperforms the state-of-the-art techniques, not only in the community size scale issue but also by achieving higher accuracy that is twice the accuracy achieved by the state-of-the-art techniques."
Geodabs: Trajectory Indexing Meets Fingerprinting at Scale.,"Finding trajectories and discovering motifs that are similar in large datasets is a central problem for a wide range of applications. Solutions addressing this problem usually rely on spatial indexing and on the computation of a similarity measure in polynomial time. Although effective in the context of sparse trajectory datasets, this approach is too expensive in the context of dense datasets, where many trajectories potentially match with a given query. In this paper, we apply fingerprinting, a copy-detection mechanism used in the context of textual data, to trajectories. To this end, we fingerprint trajectories with geodabs, a construction based on geohash aimed at trajectory fingerprinting. We demonstrate that by relying on the properties of a space filling curve geodabs can be used to build sharded inverted indexes. We show how normalization affects precision and recall, two key measures in information retrieval. We then demonstrate that the probabilistic nature of fingerprinting has a marginal effect on the quality of the results. Finally, we evaluate our method in terms of performances and show that, in contrast with existing methods, it is not affected by the density of the trajectory dataset and that it can be efficiently distributed."
Toward Reliable and Rapid Elasticity for Streaming Dataflows on Clouds.,"The pervasive availability of streaming data is driving Fast Data platforms for low-latency streaming applications. Such applications need to respond to dynamism in the input rates and task behavior using scale-in and -out on elastic Cloud resources. Platforms like Apache Storm do not provide robust means to respond to such dynamism and for rapid task migration across VMs. We propose several dataflow checkpoint and migration approaches that allow a running streaming dataflow to migrate, without any loss of in-flight messages or their internal tasks states, while reducing the time to recover and stabilize. We implement these strategies on Storm and evaluate them using micro and application dataflows for scaling in and out on 2 - 21 Cloud VMs. Our results show that we can migrate large dataflows and catchup with their processing 75% faster than Storm, which takes over 140 secs. We also find that our approaches stabilize the application up to 42% faster, and there is no failure and re-processing of messages."
Swing: Swarm Computing for Mobile Sensing.,"This paper presents Swing, a framework that aggregates a swarm of mobile devices to perform collaborative computation on sensed data streams. It endows performance and efficiency to the new generation of mobile sensing applications, in which the computation is overly intensive for a single device. After studying the source of performance slowdown of the sensing applications on a single device, we design and implement Swing to manage (i) parallelism in stream processing, (ii) dynamism from mobile users, and (iii) heterogeneity from the swarm devices. We build an Android-based prototype and deploy sensing apps - face recognition and language translation - on a wireless testbed. Our evaluations show that with proper management policies, such a distributed processing framework can achieve up to 2.7x improvement in throughput and 6.7x reduction in latency, allowing intensive sensing apps to reach real-time performance goals under different device usages, network conditions and user mobility."
ShmCaffe: A Distributed Deep Learning Platform with Shared Memory Buffer for HPC Architecture.,"One of the reasons behind the tremendous success of deep learning theory and applications in the recent days is advances in distributed and parallel high performance computing (HPC). This paper proposes a new distributed deep learning platform, named ShmCaffe, which utilizes remote shared memory for communication overhead reduction in massive deep neural network training parameter sharing. ShmCaffe is designed based on Soft Memory Box (SMB), a virtual shared memory framework. In the SMB framework, the remote shared memory is used as a shared buffer for asynchronous massive parameter sharing among many distributed deep learning processes. Moreover, a hybrid method that combines asynchronous and synchronous parameter sharing methods is also discussed in this paper for improving scalability. As a result, ShmCaffe is 10.1 times faster than Caffe and 2.8 times faster than Caffe-MPI for deep neural network training when Inception_v1 is trained with 16 GPUs. We verify the convergence of the Inception_v1 model training using ShmCaffe-A and ShmCaffe-H by varying the number of workers. Furthermore, we evaluate scalability of ShmCaffe by analyzing the computation and communication times per one iteration of deep learning training in four convolutional neural network (CNN) models."
Will Distributed Computing Revolutionize Peace? The Emergence of Battlefield IoT.,"An upcoming frontier for distributed computing might literally save lives in future military operations. In civilian scenarios, significant efficiencies were gained from interconnecting devices into networked services and applications that automate much of everyday life from smart homes to intelligent transportation. The ecosystem of such applications and services is collectively called the Internet of Things (IoT). Can similar benefits be gained in a military context by developing an IoT for the battlefield? This paper describes unique challenges in such a context as well as potential risks, mitigation strategies, and benefits."
Rational Interoperability: A Pragmatic Path toward a Data-Centric IoT.,"IoT interoperability is critical to the realization of the IoT on a global scale. This paper examines how seemingly obvious choices for perfect IoT interoperability are not so perfectly rational and offers the concept of ""Rational Interoperability"" as a means to frame and to scope the discussion. We identify why a new framing conceptualization adds value. We explore the gradations, progression and implications of Rational Interoperability: from good-enough interoperability in a pervasively connected world, to enriched interoperability for trusted groups of Things, to the aspirational goal to use interoperability requirements to guide the behavior of groups of Things toward the common good. To motivate Rational Interoperability, we describe architectural shifts underway in the IoT and present key building blocks, both of which play pivotal roles and impact its realization. We examine pragmatic near-term goals for IoT interoperability, as well as longer-term research directions. Throughout, we share lessons learned from IoT standards, highlighting pragmatic and philosophical viewpoints. Finally, we identify the gaps, challenges and opportunities in realizing IoT interoperability, including meta-data handling, enhanced discovery, data-driven insights, data-centric networks and the coordination of collective behaviors."
Vegvisir: A Partition-Tolerant Blockchain for the Internet-of-Things.,"While the intersection of blockchains and the Internet of Things (IoT) have received considerable research interest lately, Nakamoto-style blockchains possess a number of qualities that make them poorly suited for many IoT scenarios. Specifically, they require high network connectivity and are power-intensive. This is a drawback in IoT environments where battery-constrained nodes form an unreliable ad hoc network such as in digital agriculture. In this paper we present Vegvisir, a partition-tolerant blockchain for use in power-constrained IoT environments with limited network connectivity. It is a permissioned, directed acyclic graph (DAG)-structured blockchain that can be used to create a shared, tamperproof data repository that keeps track of data provenance. We discuss the use cases, architecture, and challenges of such a blockchain."
On Managing the Social Components in a Smart City.,"Recent technological and societal developments, reflected in the appearance of Internet of Things, Cloud Computing, Crowdsourcing and the shift towards a sharing economy, put the humans in the position not only to consume the services, provide data or execute simple (computational) tasks, but also to actively engage and shape the hybrid collaborative activities. These changes are opening up the possibilities for novel forms of interaction, collaboration and organization of labor. This becomes especially relevant in the context of the Smart City, where the focus is shifting from optimizing physical infrastructure and resource savings to include empowerment of citizens and support for neighborhood-scale complex/creative human collaborations. The expectation is that such collective activities can bring a disruptive change to the society. In this paper we present our vision for initiating and managing socially-driven collaborations in a Smart City context by considering research challenges related primarily to the human-centric aspects of the said collaborations."
A Distributed Systems Perspective on Industrial IoT.,"Industrial Internet of Things (IoT) is frequently mentioned as one of the emerging areas in computing that may have a high potential real-world impact in the coming decade. In this paper, we analyze the challenges posed and opportunities offered by industrial IoT solutions from the distributed systems perspective. We focus on the sensing and actuation layer, which results from the the tight coupling of such solutions with the physical objects they monitor and control. We analyze this layer with respect to interoperability, scalability, and dependability, which are key features of many distributed systems."
Re-Thinking: Design and Development of Mobility Aware Applications in Smart and Connected Communities.,"Recently, several problems concerning smart and connected communities (S&CC) have been studied. In many S&CC applications such as autonomous driving, mobile crowdsourcing and crowd sensing, the mobility of vehicles and pedestrians has large impact on their performance and reliability. Many research works have used mobility generators to simulate realistic mobility, and evaluated the performance and reliability of the proposed applications and protocols. Although such mobility generators might be useful for producing typical mobility patterns, those mobility patterns are just snapshots and they cover only some part in the possible mobility patterns. Since many S&CC applications are used as social systems, their reliability and efficiency are very important. In order to improve the reliability of such mobility aware applications and accurately evaluate their performance, we need to collect many mobility patterns via simulation and/or observation from the real world, analyze their mobility influence statistically and provide adequate design platforms. In this paper, we first show the fact that many research works adopt snapshot-based mobility analyses. Then we propose a technique to reproduce a large part of possible mobility patterns, and provide a design platform to analyze their features and develop high-reliable mobility aware applications for S&CC. Some experimental results are also given."
Cognified Distributed Computing.,"Cognification - the act of transforming ordinary objects or processes into their intelligent counterparts through Data Science and Artificial Intelligence - is a disruptive technology that has been revolutionizing disparate fields ranging from corporate law to medical diagnosis. Easy access to massive data sets, data analytics tools and High-Performance Computing (HPC) have been fueling this revolution. In many ways, cognification is similar to the electrification revolution that took place more than a century ago when electricity became a ubiquitous commodity that could be accessed with ease from anywhere in order to transform mechanical processes into their electrical counterparts. In this paper, we consider two particular forms of distributed computing - Data Centers and HPC systems - and argue that they are ripe for cognification of their entire ecosystem, ranging from top-level applications down to low-level resource and power management services. We present our vision for what ""Cognified Distributed Computing"" might look like and outline some of the challenges that need to be addressed and new technologies that need to be developed in order to make it a reality. In particular, we examine the role cognification can play in tackling power consumption, resiliency and management problems in these systems. We describe intelligent software-based solutions to these problems powered by on-line predictive models built from streamed real-time data. While we cast the problem and our solutions in the context of large Data Centers and HPC systems, we believe our approach to be applicable to distributed computing in general. We believe that the traditional systems research agenda has much to gain by crossing discipline boundaries to include ideas and techniques from Data Science, Machine Learning and Artificial Intelligence."
Towards Intelligent Distributed Data Systems for Scalable Efficient and Accurate Analytics.,"Large analytics tasks are currently executed over Big Data Analytics Stacks (BDASs) which comprise a number of distributed systems as layers for back-end storage management, resource management, distributed/parallel execution frameworks, etc. In the current state of the art, the processing of analytical queries is too expensive, accessing large numbers of data server nodes where data is stored, crunching and transferring large volumes of data and thus consuming too many system resources, taking too much time, and failing scalability desiderata. With this vision paper, we wish to push the research envelope, offering a drastically different view of analytics processing in the big data era. The radical new idea is to process analytics tasks employing learned models of data and queries, instead of accessing any base data - we call this data-less big data analytics processing. We put forward the basic principles for designing the next generation intelligent data system infrastructures realizing this new analytics-processing paradigm and present a number of specific research challenges that will take us closer to realizing the vision, which are based on the harmonic symbiosis of statistical and machine learning models with traditional system techniques. We offer a plausible research program that can address said challenges and offers preliminary ideas towards their solution. En route, we describe initial successes we have had recently with achieving scalability, efficiency, and accuracy for specific analytical tasks, substantiating the potential of the new paradigm."
Towards a Novel Architecture for Enabling Interoperability amongst Multiple Blockchains.,"Next-generation blockchain ecosystem will be fuelled by a large variety of blockchain systems. These systems increasingly demand proper cross-chain cooperation to provide richer functionalities and enhanced capabilities in the future landscape. How to enable such `interoperability' - effective communication and efficient data transfer across multiple blockchain systems is thus critical and facing many unprecedented theoretical and practical challenges. In this paper, we first clarify the definition of interoperability based on the cross disciplinary nature. Second, a roadmap of challenges needed to be addressed for interoperability has been laid out. Third, we articulate novel architectural approaches to fill in the gap by enforcing the interoperability from different blockchain layers."
Efficient Shared Memory Orchestration towards Demand Driven Memory Slicing.,"Memory is increasingly becoming a bottleneck for big data and latency-sensitive applications in virtualized systems. Memory efficiency is critical for high-performance execution of virtual machines (VMs). Mechanisms proposed for improving memory utilization often rely on an accurate estimation of VM working set size at runtime, which is difficult under changing workloads. This paper explores opportunities for improving memory efficiency and their impacts on the performance of VM executions. First, we show that if each VM is initialized with an application-specified lower bound memory, then by maintaining a shared memory region across VMs in the presence of temporal memory usage variations on the host, those VMs under high memory pressure can minimize their performance loss by opportunistically and transparently harvesting idle memory on other VMs. Second, we show that by enabling on-demand VM memory allocation and deallocation in the presence of changing workloads, VM performance degradation due to memory swapping can be reduced effectively, compared to the conventional VM configuration scenario, in which all VMs are allocated with the upper-bound of memory requested by their applications. Third, we show that by providing shared memory pipes between co-located VMs, the inter-VM communication can speed up by avoiding unnecessary overhead of communication via the network. We develop MemLego, a lightweight shared memory based system, to achieve all these benefits without requiring any modification to user applications and the OSes. We demonstrate the effectiveness of these opportunities through extensive experiments on unmodified Redis and MemCached. Using MemLego, the throughput of Redis and Memcached improves by up to 4x over the native system without MemLego, up to 2 orders of magnitude when the applications working set size does not fit in memory."
"Massivizing Computer Systems: A Vision to Understand, Design, and Engineer Computer Ecosystems Through and Beyond Modern Distributed Systems.","Our society is digital: industry, science, governance, and individuals depend, often transparently, on the inter-operation of large numbers of distributed computer systems. Although the society takes them almost for granted, these computer ecosystems are not available for all, may not be affordable for long, and raise numerous other research challenges. Inspired by these challenges and by our experience with distributed computer systems, we envision Massivizing Computer Systems, a domain of computer science focusing on understanding, controlling, and evolving successfully such ecosystems. Beyond establishing and growing a body of knowledge about computer ecosystems and their constituent systems, the community in this domain should also aim to educate many about design and engineering for this domain, and all people about its principles. This is a call to the entire community: there is much to discover and achieve."
A Trusted Healthcare Data Analytics Cloud Platform.,"This paper presents a cloud-based system for health care applications. Our system has advanced features for preserving privacy which are essential for health care applications that deal with confidential data. We describe some of the bioinformatics applications which our system is designed for. Performance is significantly enhanced by caching, and enhanced clients for performing part of the computations are a key component of our system. Cloud, due to its pay-as-you-go pricing and API based deployment model, has become widely used for delivering and maintaining infrastructure technology for businesses. However, there are significant challenges with using the cloud for applications with strict privacy and compliance requirements; health care applications fall in this domain. This paper describes an architecture and solutions for handling these types of applications."
"Crossover Service: Deep Convergence for Pattern, Ecosystem, Environment, Quality and Value.","Crossover service is a kind of services, which can provide multi-dimension service, great user experience and high values, through deeply converging services from different industries, different organizations and different value chains. Convergence is the key challenges for crossover service application. Using Alibaba's crossover service case, this paper illustrates five challenges of the service convergence process: pattern convergence, ecosystem convergence, environment convergence, quality convergence and value convergence. In addition, we propose a technical framework addressing these technical challenges, which includes all the major theories and models, techniques and methods, and tools and platforms supporting enterprises' crossover service convergence in the modeling phase, the design phase, the running phase and the management phase."
"Benchmarking Deep Learning Frameworks: Design Considerations, Metrics and Beyond.","With increasing number of open-source deep learning (DL) software tools made available, benchmarking DL software frameworks and systems is in high demand. This paper presents design considerations, metrics and challenges towards developing an effective benchmark for DL software frameworks and illustrate our observations through a comparative study of three popular DL frameworks: TensorFlow, Caffe, and Torch. First, we show that these deep learning frameworks are optimized with their default configurations settings. However, the default configuration optimized on one specific dataset may not work effectively for other datasets with respect to runtime performance and learning accuracy. Second, the default configuration optimized on a dataset by one DL framework does not work well for another DL framework on the same dataset. Third, experiments show that different DL frameworks exhibit different levels of robustness against adversarial examples. Through this study, we envision that unlike traditional performance-driven benchmarks, benchmarking deep learning software frameworks should take into account of both runtime and accuracy and their latent interaction with hyper-parameters and data-dependent configurations of DL frameworks."
Software-Defined Software: A Perspective of Machine Learning-Based Software Production.,"As the Moore's Law is ending, and increasingly high demand of software development continues in the human society, we are facing two serious challenges in the computing field. First, the general-purpose computing ecosystem that has been developed for more than 50 years will have to be changed by including many diverse devices for various specialties in high performance. Second, human-based software development is not sustainable to respond the requests from all the fields in the society. We envision that we will enter a time of developing high quality software by machines, and we name this as Software-defined Software (SDS). In this paper, we will elaborate our vision, the goals and its roadmap."
Towards Distributed Cyberinfrastructure for Smart Cities Using Big Data and Deep Learning Technologies.,"Recent advances in big data and deep learning technologies have enabled researchers across many disciplines to gain new insight into large and complex data. For example, deep neural networks are being widely used to analyze various types of data including images, videos, texts, and time-series data. In another example, various disciplines such as sociology, social work, and criminology are analyzing crowd-sourced and online social network data using big data technologies to gain new insight from a plethora of data. Even though many different types of data are being generated and analyzed in various domains, the development of distributed city-level cyberinfrastructure for effectively integrating such data to generate more value and gain insights is still not well-addressed in the research literature. In this paper, we present our current efforts and ultimate vision to build distributed cyberinfrastructure which integrates big data and deep learning technologies with a variety of data for enhancing public safety and livability in cites. We also introduce several methodologies and applications that we are developing on top of the cyberinfrastructure to support diverse community stakeholders in cities."
Toward IoT-Friendly Learning Models.,"In IoT environments, data are collected by many distinct devices, at the periphery, so that their feature-sets can be naturally endowed with a faceted structure. In this work, we argue that the IoT requires specialized ML models, able to exploit this faceted structure in the learning strategy. We demonstrate the application of this principle, by a multiple kernel learning approach, based on the exploration of the partition lattice driven by the natural partitioning of the feature set. Furthermore, we consider that the whole data management, acquisition, pre-processing and analytics pipeline results from the composition of processes pursuing different and non-perfectly aligned goals (most often, enacted by distinct agents with different constraints, requirements competencies and with non-aligned interests). We propose the adoption of an adversarial modeling paradigm across the overall pipeline. We argue that knowledge of the composite nature of the learning process, as well as of the adversarial character of the relationship among phases, can help in developing heuristics for improving the learning algorithms efficiency and accuracy. We develop our argument with reference to few exemplary use cases."
Transform Blockchain into Distributed Parallel Computing Architecture for Precision Medicine.,"This paper provides a vision and proposes mechanisms to transform the blockchain duplicated computing into distributed parallel computing architecture by transforming smart contract which features data driven from the ground up to support moving computing to native data strategy. This new distributed parallel computing architecture can be employed to build a large size of data set from various distributed hosted medical data sets which might consist of personal electronic medical record (EMR) and various medical data. This large medical data set will enable researchers to jump start the deep learning research for medical domain. Distributed data management, distributed data sharing, and distributed learning are the core mechanisms in the new architecture. The required new researches and developments to employ Google federated learning and transfer learning algorithms in this new architecture are discussed. The approach and mechanism enabled by the new architecture is illustrated to build a real world evidence of clinical trial toward personal and precision medicine. Research issues and technical challenges are provided."
"Computing In-Memory, Revisited.","The Von Neumann's architecture has been the dominant computing paradigm ever since its inception in the mid-forties. It revolves around the concept of a ""stored program"" in memory, and a central processing unit that executes the program. As an alternative, Processing-In-Memory (PIM) ideas have been around for at least two decades, however with very limited adoption. Today, three trends are creating a compelling motivation to take a second look. Novel devices such as memristor blur the boundary between memory and compute, effectively providing both in the same element. Power efficiency has become very important, both in the datacenter and at the edge. Machine learning applications driven by a data-flow model have become ubiquitous. In this paper, we sketch our Computing-In-Memory (CIM) vision, and its substantial performance and power improvement potential. Compared to PIM models, CIM more clearly separates computing from memory. We then discuss the programming model, which we consider the biggest challenge. We close by describing how CIM impacts non-functional characteristics, such as reliability, scale, and configurability."
OpenVDAP: An Open Vehicular Data Analytics Platform for CAVs.,"In this paper, we envision the future connected and autonomous vehicles (CAVs) as a sophisticated computer on wheels, with substantial on-board sensors as data sources and a variety of services running on top to support autonomous driving or other functions. In general, these services are computationally expensive, especially for the machine learning based applications (e.g., CNN-based object detection). Nevertheless, the on-board computation unit possess limited compute resources, raising a huge challenge to deploy these computation-intensive services on the vehicle. On the contrary, the cloud-based architecture conceptually with unconstrained resources suffers from unexpected extended latency that attributes to the large-scale Internet data transmission; thus, adversely affecting the services' real-time performance, quality of services and user experiences. To address this dilemma, inspired by the promising edge computing paradigm, we propose to build an Open Vehicular Data Analytics Platform (OpenVDAP) for CAVs, which is a full-stack edge based platform including an on-board computing/communication unit, an isolation-supported and security & privacy-preserved vehicle operation system, an edge-aware application library, as well as an optimal workload of?oading and scheduling strategy, allowing CAVs to dynamically detect each service's status, computation overhead and the optimal of?oading destination so that each service could be finished within an acceptable latency and limited bandwidth consumption. Most importantly, contrast to the proprietary platform, OpenVDAP is an open-source platform that offers free APIs and real-?eld vehicle data to the researchers and developers in the community, allowing them to deploy and evaluate applications on the real environment."
Toward an Intrusion-Tolerant Power Grid: Challenges and Opportunities.,"While cyberattacks pose a relatively new challenge for power grid control systems, commercial cloud systems have needed to address similar threats for many years. However, technology and approaches developed for cloud systems do not necessarily transfer directly to the power grid, due to important differences between the two domains. We discuss our experience adapting intrusion-tolerant cloud technologies to the power domain and describe the challenges we have encountered and potential directions for overcoming those obstacles."
Private Memoirs of IoT Devices: Safeguarding User Privacy in the IoT Era.,"The rise of the Internet-of-Things (IoT) holds great promise to transform people's lives by making society more efficient in many areas, including energy, transportation, healthcare, commerce, manufacturing, etc. At their core, IoT devices use sensors to collect data on real-world physical processes and then transmit it over the Internet to cloud servers, which store, process, and learn from the data to better optimize these processes, either directly (by issuing remote commands that actuate IoT devices) or indirectly (by issuing notifications that direct users to take some action). Unfortunately, IoT devices also expose users to multiple new types of privacy attacks. In particular, the sensor data collected from IoT devices can indirectly reveal a variety of sensitive private information. In addition, users generally connect IoT devices to local networks, which they implicitly trust, with little understanding of what the IoT device is doing on the network. In this visionpaper, we discuss recent work on sensor data privacy in the context of smart energy systems to provide examples of i) the surprising types of private information we can glean from seemingly innocuous IoT data and ii) the different types of defenses we have developed to preserve IoT data privacy for smart energy systems. These defenses lie at different discrete points in the tradeoff between user privacy and IoT functionality, which motivates ongoing work on developing defenses that provide a more tunable tradeoff. We also discuss the privacy implications of connecting tens-to-hundreds of untrusted IoT devices to implicitly trusted local networks, and avenues for research to mitigate these concerns."
"Towards Dependable, Scalable, and Pervasive Distributed Ledgers with Blockchains.","Distributed blockchain ledgers are on the verge of becoming a disruptive technology, profoundly impacting a wide range of industries and established applications, such as cryptocurrency, and allowing for novel use cases in both the public sector (e.g., eGovernment, eHealth, etc.) and the private sector (e.g., finance, supply chain management, etc.). Blockchains promise the ability to maintain critical information in a trustworthy repository without any centralized management. The reliability of blockchain-enabled applications is based on the innate immutability of stored data, maintained through cryptographic means, which enables blockchains to provide transparency, efficiency, auditability, trust, and security. As the technology is still in its infancy, a number of pain points must be addressed in order to make distributed ledgers more dependable, scalable, and pervasive. In this paper, we present the research landscape in distributed ledger technology (DLT). To do so, we describe a taxonomy of blockchain applications called blockchain generations. We also present the DCS properties (Decentralization, Consistency, and Scalability) as an analogy to the CAP theorem. Furthermore, we provide a general structure of the blockchain platform which decomposes the distributed ledger into six layers: Application, Modeling, Contract, System, Data, and Network. Finally, we classify research angles across three dimensions: DCS properties impacted, targeted applications, and related layers."
"Rethinking Resource Management in Mobile Web: Measurement, Deployment, and Runtime.","After its birth since early 1990s, the Web has been the major factor that drives the success of the Internet. In the past decade, the access to the Web has undergone a tremendous evolution from PC to mobile devices, i.e., via smartphones, tablet computers, and wearable devices. It is a key challenge to make the future mobile Web more ""user friendly"", i.e., smooth interactions, fast page load time, reasonable data traffic volume, efficient energy drain, etc. Due to the device diversity and dynamic network connections, a better client-side resource management is quite critical. This paper presents a new perspective of measuring the resource management in a proactive fashion, and identifies various insights that have not been covered by existing efforts. Then we present our visionary holistic approach to optimizing the mobile Web, including the new deployment model and runtime support along with the preliminary principled design."
A View from ORNL: Scientific Data Research Opportunities in the Big Data Age.,"One of the core issues across computer and computational science today is adapting to, managing, and learning from the influx of ""Big Data"". In the commercial space, this problem has led to a huge investment in new technologies and capabilities that are well adapted to dealing with the sorts of human-generated logs, videos, texts, and other large-data artifacts that are processed and resulted in an explosion of useful platforms and languages (Hadoop, Spark, Pandas, etc.). However, translating this work from the enterprise space to the computational science and HPC community has proven somewhat difficult, in part because of some of the fundamental differences in type and scale of data and timescales surrounding its generation and use. We describe a forward-looking research and development plan which centers around the concept of making Input/Output (I/O) intelligent for users in the scientific community, whether they are accessing scalable storage or performing in situ workflow tasks. Much of our work is based on our experience with the Adaptable I/O System (ADIOS 1.X), and our next generation version of the software ADIOS 2.X [1]."
How to Prevent Skynet from Forming (A Perspective from Policy-Based Autonomic Device Management).,"Artificial Intelligence (AI) in the context of military systems has frequently been portrayed as dangerous, and as leading to humanity being put in danger by an errant AI system, such as the Skynet imagined in the Terminator movie series. At the same time, the benefits of using AI in such systems are numerous. Therefore, we need to develop techniques that will let military systems benefit from the advances in AI, while ensuring that a system like Skynet never turns against humanity. In this paper, we examine the problem from the perspective of device management, a set of intelligent systems that manage themselves and determine their own policies. We discuss mechanisms that could be used to prevent these systems from becoming malignant."
Operating Systems for Internetware: Challenges and Future Directions.,"An operating system is an essential layer of system software that is responsible for resource management and application support on a computer system. As the evolvement of computer systems, the concept of OSs has also been evolved into many new forms beyond the traditional OSs such as Linux and Windows. We call this new generation of OSs as ubiquitous operating systems (UOSs). Among many new types of UOSs, we are particularly interested in the operating systems for Internetware, i.e., Internetware Operating Systems. Internetware is a paradigm for new types of Internet applications that are autonomous, cooperative, situational, evolvable, and trustworthy. An Internetware OS represents our perspective on the OS for future Internet-based applications. This paper discusses the examples, technical challenges and our recent effort on Internetware OSs, as well as our vision on the future of Internetware OSs. We believe that, in the foreseeable future, Internetware OSs will become ubiquitous and could be built for many different types of computer systems and beyond."
Deep Learning towards Mobile Applications.,"Recent years have witnessed an explosive growth of mobile devices. Mobile devices are permeating every aspect of our daily lives. With the increasing usage of mobile devices and intelligent applications, there is a soaring demand for mobile applications with machine learning services. Inspired by the tremendous success achieved by deep learning in many machine learning tasks, it becomes a natural trend to push deep learning towards mobile applications. However, there exist many challenges to realize deep learning in mobile applications, including the contradiction between the miniature nature of mobile devices and the resource requirement of deep neural networks, the privacy and security concerns about individuals' data, and so on. To resolve these challenges, during the past few years, great leaps have been made in this area. In this paper, we provide an overview of the current challenges and representative achievements about pushing deep learning on mobile devices from three aspects: training with mobile data, efficient inference on mobile devices, and applications of mobile deep learning. The former two aspects cover the primary tasks of deep learning. Then, we go through our two recent applications that apply the data collected by mobile devices to inferring mood disturbance and user identification. Finally, we conclude this paper with the discussion of the future of this area."
Mobile-Friendly HTTP Middleware with Screen Scrolling.,"The pervasive penetration of mobile smart devices has significantly enriched Internet applications and undoubtedly reshaped the way that users access Internet services. Different from traditional desktop applications, mobile Internet applications require users to input via touch screens and view outputs on the displays with considerably limited size. The significant conflict between the limited-size of touch screens and the richness of online media contents requires the mobile Internet applications to download contents way beyond the user's viewing region (referred as viewport). In this paper, we present a Mobile-Friendly HTTP middleware (MF-HTTP), which interprets user touch screen inputs and optimize the HTTP downloading of media objects to improve quality of experience (QoE) and cost efficiency. We first demystify screen scrolling in mobile operating systems and precisely break down the viewport moving process. We identify the key influential factors for media object downloading and develop an optimal download scheme. Towards building a practical middleware, we further discuss and address the implementation issues in detail. We implement a MF-HTTP prototype based on Android platforms and evaluate the performance of MF-HTTP by conducting concrete case studies on two representative applications, namely, web browsing and 360-degree video streaming."
The Fusion of VMs and Processes: A System Perspective of cKernel.,"Virtual machines (VMs) and processes are two important abstractions for cloud virtualization, where VMs usually install a complete operating system (OS) executing user processes. Although existing in different layers in the virtualization hierarchy, VMs and processes have overlapped functionalities. For example, they are both intended to provide execution abstraction (e.g., physical/virtual memory address space), and share similar objectives of isolation, cooperation and scheduling. However, neither of them could provide the benefits of the other: VMs provide higher isolation, security and portability, while processes are more efficient, flexible and easier to schedule and cooperate. Currently, this heavyweight architecture degrades both efficiency and security of cloud services. There are two trends for cloud virtualization: the first is to enhance processes to achieve VM-like security, and the second is to reduce VMs to achieve process-like flexibility. Based on these observations, our vision is that in the near future VMs and processes might be fused into one new abstraction for cloud virtualization that embraces the best of both, providing VM-level isolation and security while preserving process-level efficiency and flexibility. We describe a reference implementation, dubbed cKernel (customized kernel), for the new abstraction. Essentially, cKernel enhances the exokernel architecture by (i) adopting the LibOS paradigm to assemble isolated, smallest possible ""execution environments"", and (ii) following the the ""core-shell"" model to dynamically add traditional process features to the environments."
Complex Distributed Systems: The Need for Fresh Perspectives.,"Distributed systems are at a watershed due to their increasing complexity. The heart of the problem is the extreme level of heterogeneity exhibited by contemporary distributed systems coupled with the need to be dynamic and responsive to change. In effect, we have moved from distributed systems to systems of systems. Following on from this, middleware is also at a watershed. The traditional view of middleware is no longer valid (i.e. as a layer of abstraction, masking the complexity of the underlying distributed system and providing a high-level programming model). In practice, middleware is often by-passed with complex systems constructed in a rather ad hoc manner as a mash-up of a variety of technologies. The end result is that middleware is no longer sure of its form or purpose and this lack of a viable approach is a huge barrier to the emergence of areas such as smart cities and emergency response systems. This paper argues that there is a need to fundamentally rethink the middleware landscape related to complex distributed systems. The core contribution of the paper is a set of fresh perspectives, which lead us in turn to novel principles and patterns for middleware and subsequently to new styles of platform. These perspectives include a move to emergent middleware, seeking flexible meta-structures for distributed systems, and a step away from generic to domain-specific technologies. A number of case studies are also presented to demonstrate what this might mean for future distributed systems."
Improving Communication through Overlay Detours: Pipe Dream or Actionable Insight?,"It has been long observed that communication between a client and a content server using overlay detours may result in substantially better performance than a native path offered by IP routing. Yet the use of detours has been limited to distributed platforms such as Akamai. This paper poses a question - how can clients practically take advantage of overlay detours without modification to content servers (which are obviously outside clients' control)? We have posited elsewhere that the emergence of gigabit-to-the-home access networks would precipitate a new home network appliance, which would maintain permanent presence on the Internet for the users and have general computing and storage capabilities. Given such an appliance, our vision is that Internet users may form cooperatives in which members agree to serve as waypoints points to each other to improve each other's Internet experience. To make detours transparent to the server, we leverage MPTCP, which normally allows a device to communicate with the server on several network interfaces in parallel but we use it to communicate through external waypoint hosts. The waypoints then mimic MPTCP's subflows to the server, making the server oblivious to the overlay detours as long as it supports MPTCP."
EASY: Efficient Segment Assignment Strategy for Reducing Tail Latencies in Pinot.,"Customer facing online services, such as LinkedIn and Uber, rely on scalable and low-latency data stores to maintain acceptable query tail latencies. An important challenge for managing the performance of these systems is the assignment of newly created data segments to data nodes to balance load. Given the rate at which these services are accessed (thus generating new data), the segment assignment problem is particularly important. This paper presents EASY, an efficient segment assignment strategy that leverages analytical modeling to predict the future load induced by data segments, thus allowing for long-term balancing of load across data nodes. Our implementation and evaluation of EASY on Pinot shows that we can significantly reduce query tail latencies in the presence of dynamically generated data segments."
Anti-Entropy Bandits for Geo-Replicated Consistency.,"Eventually consistent systems can be made more consistent by reducing the time until a write is fully replicated, thereby improving global update visibility. While gossip-based anti-entropy methods scale well, random selection of anti-entropy partners is less than efficient. Moreover, while eventual consistency may be consistent enough in a single data center, geographic replication increases visibility latency and leads to externally observable inconsistencies. In this paper, we explore an improvement to pairwise, bilateral anti-entropy; instead of uniform random selection, we introduce reinforcement learning mechanisms to assign selection probabilities to replicas most likely to have information. The result is more efficient replication, faster visibility, and stronger eventual consistency while maintaining high availability and partition tolerance."
On Device Grouping for Efficient Multicast Communications in Narrowband-IoT.,"Narrowband IoT (NB-IoT) is a new cellular network technology that has been designed for low capability, low power consumption devices that are expected to operate for more than 10 years on a single battery. These types of devices will be inexpensive (less than $5) and deployed on massive scales. This long life expectancy will lead to the need for occasional software updates, to very large groups of devices. While a new multicast mechanism has recently been proposed for the efficient multicast transmission of such updates, it assumes that devices can be grouped together and synchronized in order to receive the multicast data. In this paper, we explore three different approaches to achieve device grouping, with different trade-offs between bandwidth usage, energy consumption and compliance with the NB-IoT standard. To assess the performance of each pproach, we conducted a thorough experimental evaluation under realistic operating conditions."
Replica-Group Leadership Change as a Performance Enhancing Mechanism in NoSQL Data Stores.,"In this paper we investigate replica-group reconfiguration as a way to mask performance bottlenecks on the primary node of a primary-backup replication group in a NoSQL data store. We investigate the benefit of changing replica-group leadership prior to resource-intensive background tasks such as LSM-tree compactions or data backups on the primary node, a method that can improve throughput by up to 23% during LSM-tree compactions and by 35% during backup tasks. Our implementation is based on MongoRocks (MongoDB 3.7 and RocksDB 5.7) using leveled compaction. We experimentally demonstrate the performance impact of compactions and data backups when they occur at replica-group primaries, and the benefits of targeted leadership-change actions. We evaluate our system using the Yahoo Cloud Serving Benchmark (YCSB) and compare to unmodified MongoRocks on dedicated infrastructure."
Towards Realistic Energy Profiling of Blockchains for Securing Internet of Things.,"Internet of Things (IoTs) offers a plethora of opportunities for remote monitoring and communication of everyday objects known as things with applications in numerous domains. The advent of blockchains can be a significant enabler for IoTs towards conducting and verifying transactions in a secure manner. However, applying blockchains to IoTs is challenging due to the resource constrained nature of the embedded devices coupled with significant delay incurred in processing and verifying transactions in the blockchain. Thus there exists a need for profiling the energy consumption of blockchains for securing IoTs and analyzing energy-performance trade-offs. Towards this goal, we profile the impact of workloads based on Smart Contracts and further quantify the power consumed by different operations performed by the devices on the Ethereum platform. In contrast to existing approaches that are focused on performance, we characterize performance and energy consumption for real workloads and analyse energy-performance trade-offs. Our proposed methodology is generic in that it can be applied to other platforms. The insights obtained from the study can be used to develop secure protocols for IoTs using blockchains."
Concurrent Ranging with Ultra-Wideband Radios: From Experimental Evidence to a Practical Solution.,"To enable future location-aware Internet of Things (IoT) applications, Ultra-wideband (UWB) technology provides centimeter-accurate distance estimations. In the common case of a non-synchronized network, at least NÂ·(N-1) message exchanges are required to derive the distance between N nodes. Enabling concurrent ranging between an initiator and an arbitrary number of responders can drastically reduce the amount of necessary transmissions and hence increases the efficiency of UWB systems. Although the feasibility of concurrent ranging has been proven experimentally, several key challenges still need to be addressed to practically implement concurrent ranging in real-world UWB systems, such as the automatic detection of multiple responses, the identification of a responder, as well as the detection of overlapping responses (especially in the presence of multipath components). In this paper, we provide a concurrent ranging solution tackling the aforementioned challenges. Among others, our solution enables (i) to detect responses in the CIR reliably, (ii) to encode the responder ID in the CIR to allow personalized ranging, as well as (iii) to mitigate the impact of overlapping responses and multipath components. We further show how the proposed solution increases the scalability of concurrent ranging in real-world UWB-based distributed systems."
DDP: Distributed Network Updates in SDN.,"How to quickly and consistently update a network is among the most fundamental and common challenges in software defined networking (SDN) systems. Current approaches heavily rely on the (logically) centralized controller to initiate and orchestrate the network updates, resulting in long latency of update completion. In this paper, we present DDP, a system for fast, distributed network updates while preserving various consistency properties. The key technique in DDP is a novel primitive named datapath operation container (DOC), where each DOC is encoded with an individual operation and its dependency logic. DDP adopts the simple, but powerful DOCs to configure the network, so that network updates can be triggered and executed at the data plane in a distributed and local manner. Novel algorithms are designed to compute and optimize the DOCs for consistent updates. We implement DDP to evaluate its performance in various update scenarios. Experimental results show that DDP significantly improves network update speed by up to 52.1% for the real-time updates initiated by the controller, and further improves the speed by 55.6-61.4% for the updates directly triggered at the data plane, such as failure recovery."
Geolocation of Transmitters Using Minimally Accurate Receivers.,"The location of adversarial signals is of great importance to the military. Whether the enemy is jamming friendly communications or transmitting for other reasons, finding out where those signals are coming from may be the first goal of any response mission. If highly accurate direction-finding information is available, triangulation of the source can be trivial. The purpose of this paper is to analyze the possibilities for geolocation using substantially less accurate sources. We proceed using simulated receivers with high standard deviations relative to the possible values, and use simulations to demonstrate that geolocation is possible using a small number of such receivers."
KerA: Scalable Data Ingestion for Stream Processing.,"Big Data applications are increasingly moving from batch-oriented execution models to stream-based models that enable them to extract valuable insights close to real-time. To support this model, an essential part of the streaming processing pipeline is data ingestion, i.e., the collection of data from various sources (sensors, NoSQL stores, filesystems, etc.) and their delivery for processing. Data ingestion needs to support high throughput, low latency and must scale to a large number of both data producers and consumers. Since the overall performance of the whole stream processing pipeline is limited by that of the ingestion phase, it is critical to satisfy these performance goals. However, state-of-art data ingestion systems such as Apache Kafka build on static stream partitioning and offset-based record access, trading performance for design simplicity. In this paper we propose KerA, a data ingestion framework that alleviate the limitations of state-of-art thanks to a dynamic partitioning scheme and to lightweight indexing, thereby improving throughput, latency and scalability. Experimental evaluations show that KerA outperforms Kafka up to 4x for ingestion throughput and up to 5x for the overall stream processing throughput. Furthermore, they show that KerA is capable of delivering data fast enough to saturate the big data engine acting as the consumer."
A Flexible Network Approach to Privacy of Blockchain Transactions.,"For preserving privacy, blockchains can be equipped with dedicated mechanisms to anonymize participants. However, these mechanism often take only the abstraction layer of blockchains into account whereas observations of the underlying network traffic can reveal the originator of a transaction request. Previous solutions either provide topological privacy that can be broken by attackers controlling a large number of nodes, or offer strong and cryptographic privacy but are inefficient up to practical unusability. Further, there is no flexible way to trade privacy against efficiency to adjust to practical needs. We propose a novel approach that combines existing mechanisms to have quantifiable and adjustable cryptographic privacy which is further improved by augmented statistical measures that prevent frequent attacks with lower resources. This approach achieves flexibility for privacy and efficency requirements of different blockchain use cases."
Computation Offloading for Machine Learning Web Apps in the Edge Server Environment.,"Machine leaning apps require heavy computations, especially with the use of the deep neural network (DNN), so an embedded device with limited hardware cannot run the apps by itself. One solution for this problem is to offload DNN computations from the client to a nearby edge server. Existing approaches to DNN offloading with edge servers either specialize the edge server for fixed, specific apps, or customize the edge server for diverse apps, yet after migrating a large VM image that contains the client's back-end software system. In this paper, we propose a new and simple approach to offload DNN computations in the context of web apps. We migrate the current execution state of a web app from the client to the edge server just before executing a DNN computation, so that the edge server can execute the DNN computation with its powerful hardware. Then, we migrate the new execution state from the edge server to the client so that the client can continue to execute the app. We can save the execution state of the web app in the form of another web app called the snapshot, which immensely simplifies saving and restoring the execution state with a small overhead. We can offload any DNN app to any generic edge server, equipped with a browser and our offloading system. We address some issues related to offloading DNN apps such as how to send the DNN model and how to improve the privacy of user data. We also discuss how to install our offloading system on the edge server on demand. Our experiment with real DNN-based web apps shows that snapshot-based offloading achieves a promising performance result, comparable to running the app entirely on the server."
CAL: A Smart Home Environment for Monitoring Cognitive Decline.,"The increased growth of the aging population (i.e., 65 years or older) has led to emerging technologies in health care that provide in-home support to patients using devices throughout the household. Such smart home environments can monitor and interact with patients and their doctors/caregivers to augment patient medical data for diagnosis than can be generated via traditional doctor visits. Moreover, smart homes are enabling older adults to stay at home longer as opposed to permanent moves to assisted living or nursing facilities, increasing health and well-being and decreasing overall costs to the individual and society at large. This paper proposes Cognitive Assisted Living (CAL), a cyber-physical system comprising a network of embedded devices for collecting and analyzing patient speech patterns over time for monitoring cognitive function beginning in the early stages of Alzheimer's disease. Specifically, CAL will analyze patient speech patterns and spatial abilities, via a set of daily interactions, to provide a longitudinal analysis of speech deterioration, a significant indicator of cognitive decline resulting from Alzheimer's disease. Understanding the rate of cognitive decline can enable caregivers and health care professionals to better manage the patient's daily care and medical requirements. Additionally, the patient's cognitive state can be shared across household devices to increase the patient's comfort and better accommodate lifestyle changes. To these ends, we describe the architecture of the proposed system, the methods to which we will detect cognitive decline, and specify how the system will provide continuing fault tolerance and data security at run time."
SLoG: Large-Scale Logging Middleware for HPC and Big Data Convergence.,"Cloud developers traditionally rely on purpose-specific services to provide the storage model they need for an application. In contrast, HPC developers have a much more limited choice, typically restricted to a centralized parallel file system for persistent storage. Unfortunately, these systems often offer low performance when subject to highly concurrent, conflicting I/O patterns. This makes difficult the implementation of inherently concurrent data structures such as distributed shared logs. Yet, this data structure is key to applications such as computational steering, data collection from physical sensor grids, or discrete event generators. In this paper we tackle this issue. We present SLoG, shared log middleware providing a shared log abstraction over a parallel file system, designed to circumvent the aforementioned limitations. We evaluate SLoG's design on up to 100,000 cores of the Theta supercomputer: the results show high append velocity at scale while also providing substantial benefits for other persistent backend storage systems."
Identifying Privacy Risks in Distributed Data Services: A Model-Driven Approach.,"Online services are becoming increasingly data-centric; they collect, process, analyze and anonymously disclose growing amounts of personal data. It is crucial that such systems are engineered in a privacy-aware manner in order to satisfy both the privacy requirements of the user, and the legal privacy regulations that the system operates under. How can system developers be better supported to create privacy-aware systems and help them to understand and identify privacy risks? Model-Driven Engineering (MDE) offers a principled approach to engineer systems software. The capture of shared domain knowledge in models and corresponding tool support can increase the developers' understanding. In this paper, we argue for the application of MDE approaches to engineer privacy-aware systems. We present a general purpose privacy model and methodology that can be used to analyse and identify privacy risks in systems that comprise both access control and data pseudonymization enforcement technologies. We evaluate this method using a case-study based approach and show how the model can be applied to engineer privacy-aware systems and privacy policies that reduce the risk of unintended disclosure."
AdaptiveConfig: Run-Time Configuration of Cluster Schedulers for Cloud Short-Running Jobs.,"Cluster schedulers provide flexible resource sharing mechanism for short-running jobs, which occupy a majority of cloud jobs. A scheduler's configuration decides how to allocate resources among jobs and hence it is crucial to their performances. Today's cloud platforms usually rely on cluster administrators to set this configuration, thus it is difficult to optimally configure the scheduler so as to minimize the latencies of heterogeneous and dynamically changing jobs in the cloud. In this paper, we introduce AdaptiveConfig, a run-time configurator for cluster schedulers that automatically adapts to the changing workload and resource status. This includes: (1) an estimator to calculate jobs' performances under different configurations and various scheduling scenarios. The key idea here is to transform a scheduler's resource allocation mechanisms and their variable influence factors (configuration parameters, scheduling constraints, available resources, and workload status) into business rules and facts in a rule engine, thereby reasoning about these correlated factors in job performance estimation. (2) A run-time optimizer that efficiently searches the configuration space to find the optimal configuration for the current workload. We implemented AdaptiveConfig on the popular YARN Capacity and Fair schedulers and demonstrate its effectiveness using workloads of Facebook jobs, i.e. considerably reducing latencies by 2.22 times (and up to 4.50 times) with low optimization overheads."
Q-Placement: Reinforcement-Learning-Based Service Placement in Software-Defined Networks.,"In software-defined networking (SDN) paradigm, where the control and data plane are separated, the scalability of the SDN controller in the control plane is critical and can affect the overall network performance significantly. To improve controller scalability, efforts have been put into enhancing the capability of SDN switches in the data plane, to make them more autonomous in providing routine services without consulting the controller. In this regard, we investigate the service placement problem on SDN switches aiming at minimizing the average accumulated service costs for end users. To solve this problem, we propose a novel reinforcement-learning-based algorithm with guaranteed performance and convergence rate, called Q-placement. Comparing to traditional optimization techniques, Q-placement exhibits many appealing features, such as performance-tuneable optimization and off-the-shelf implementation. Extensive evaluations show that Q-placement consistently outperforms benchmarks and other state-of-the-art algorithms in both synthetic and real networks. Moreover, these evaluations reveal insights into how the network topological properties (e.g., density), servicing capacities, and controller's roles affect the accumulated service costs, which is useful in service planning tasks."
GraphU: A Unified Vertex-Centric Parallel Graph Processing Platform.,"Many synchronous and asynchronous distributed platforms based on the Bulk Synchronous Parallel (BSP) model have been built for large-scale vertex -centric graph processing. Unfortunately, a program designed for a synchronous platform may not work properly on an asynchronous one. As a result, given the same problem, end users may be required to design different parallel algorithms for different platforms. Recently, we have proposed a unified programming model, DFA-G (Deterministic Finite Automaton for Graph processing), which expresses the computation at a vertex as a series of message-driven state transitions. It has the attractive property that any program modeled after it can run properly across synchronous and asynchronous platforms. In this demo, we first propose a framework of complexity analysis for DFA-G automaton and show that it can significantly facilitate complexity analysis on asynchronous programs. Due to the existing BSP platforms' deficiency in supporting efficient DFA-G execution, we then develop a new prototype platform, GraphU. GraphU was built on the popular open-source Giraph project. But it entirely removes synchronization barriers and decouples remote communication from vertex computation. Finally, we empirically evaluate the performance of various DFA-G programs on GraphU by a comparative study. Our experiments validate the efficacy of the proposed complexity analysis approach and the efficiency of GraphU."
FLight: A Fast and Lightweight Elephant-Flow Detection Mechanism.,"In this work, we propose FLight, a fast, lightweight and adaptive mechanism for detecting elephant-flows while improving the detection accuracy and speed. FLight leverages the TCP communication behavior for its detection algorithm, it demonstrates a 100% elephant-flow detection accuracy, and is 242Ã— faster than other centralized solutions."
Liquid Mail - A Client Mail Based on CUBE Model.,"Nowadays we live with a large number of different connected devices with various operating systems. This is leading us to a lack of technological solutions to deal with this multiple connected devices owned by the same user. Thus, a new way to create applications is required in order to enhance the user experience regardless the operating systems of devices. In this work, we present a client mail, based on CUBE model principals, allowing the user changing device while using a mail service without losing session/data or connection. Preliminary results demonstrate its feasibility to move from a Windows system to an Android one while keeping the mail service running."
Embedding Non-Compliant Nodes into the Information Flow Monitor by Dependency Modeling.,"Observing semantic dependencies in large and heterogeneous networks is a critical task, since it is quite difficult to find the actual source of a malfunction in the case of an error. Dependencies might exist between many network nodes and among multiple hops in paths. If those dependency structures are unknown, debugging errors gets quite difficult. Since CPS and other large networks change at runtime and consists of custom software and hardware, as well as components off-the-shelf, it is necessary to be able to not only include own components in approaches to detect dependencies between nodes. In this paper we present an extension to the Information Flow Monitor approach. Our goal is that this approach should be able to handle unalterable blackbox nodes. This is quite challenging, since the IFM originally requires each network node to be compliant with the IFM protocol."
Cell Selection with Deep Reinforcement Learning in Sparse Mobile Crowdsensing.,"Sparse Mobile CrowdSensing (MCS) is a novel MCS paradigm where data inference is incorporated into the MCS process for reducing sensing costs while its quality is guaranteed. Since the sensed data from different cells (sub-areas) of the target sensing area will probably lead to diverse levels of inference data quality, cell selection (i.e., choose which cells of the target area to collect sensed data from participants) is a critical issue that will impact the total amount of data that requires to be collected (i.e., data collection costs) for ensuring a certain level of quality. To address this issue, this paper proposes a Deep Reinforcement learning based Cell selection mechanism for Sparse MCS, called DR-Cell. We properly model the key concepts in reinforcement learning including state, action, and reward, and then propose to use a deep recurrent Q-network for learning the Q-function that can help decide which cell is a better choice under a certain state during cell selection. Experiments on various real-life sensing datasets verify the effectiveness of DR-Cell over the state-of-the-art cell selection mechanisms in Sparse MCS by reducing up to 15% of sensed cells with the same data inference quality guarantee."
HDM-MC in-Action: A Framework for Big Data Analytics across Multiple Clusters.,"Big data are increasingly collected and stored in a highly distributed infrastructures due to the development of several emerging technologies including sensor network, cloud computing, IoT and mobile computing among many other emerging technologies. In practice, the majority of existing big data processing frameworks (e.g., Hadoop, Spark, Flink) are designed based on the single-cluster setup with the assumptions of centralized management and homogeneous connectivity which makes them sub-optimal and sometimes infeasible to be applied for scenarios that require implementing data analytics jobs on highly distributed data sets (across racks, data centers or multi organizations). We demonstrate HDM-MC, a big data processing framework that is designed to enable the capability of performing large scale data analytics across multi-clusters with minimum extra overhead due to additional scheduling requirements. We describe the architecture and realization of the system using a step-by-step example scenario."
Developing a Convenient and Fast to Deploy Simulation Environment for Cyber-Physical Systems.,"Cyber-Physical Systems (CPS) are interconnected systems, that adapt to their environment. They are quite challenging to engineer and to test, because the its interconnected and networked structures changes at runtime. Dependencies and influencing between nodes in the network might be difficult to test, because there are many hidden impacts on not directly connected nodes. In this paper, we present our experimentation environment which we use to simulate different CPSs. The focus of the development of this environment is to be able to rapidly generate, deploy and change software and network connections within the CPS and to observe resulting impacts on the network."
A Multi Tenant Computational Platform for Translational Medicine.,"Translational biomedical research has become a science driven by big data. Improving patient care by developing personalized therapies and new drugs depends increasingly on an organization's ability to rapidly and intelligently leverage complex molecular and clinical data from a variety of large-scale internal and external, partner and public, data sources. As analysing these large-scale and complex datasets has become increasingly computationally expensive, it is of paramount importance to enable researchers to seamlessly scale up their computation platform while being able to manage complex yet flexible scenario that biomedical scientists are asking for. We developed a new platform as an answer to those needs of analysing and exploring massive amounts of medical data with the constrain of enabling the broadest audience, ranging from the medical doctor to the advanced coders, to easily and intuitively exploit this new resource. The platform consists of three main components: Borderline UI, the eTRIKS Analytical Environment (eAE) and the eTRIKS Data Platform (eDP). Each component has been developed independently to address specific sets of problems, then, loosely connected to each other components as to form a coherent platform for large scale medical data analysis."
Low Latency Edge Rendering Scheme for Interactive 360 Degree Virtual Reality Gaming.,"This paper describes the core functionality and a proof-of-concept demonstration setup for remote 360 degree stereo virtual reality (VR) gaming. In this end-to-end scheme, the execution of a VR game is off-loaded from an end user device to a cloud edge server in which the executed game is rendered based on user's field of view (FoV) and control actions. Headset and controller feedback is transmitted over the network to the server from which the rendered views of the game are streamed to a user in real-time as encoded HEVC video frames. This approach saves energy and computation load of the end terminals by making use of the latest advancements in network connection speed and quality. In the showcased demonstration, a VR game is run in Unity on a laptop powered by i7 7820HK processor and GTX 1070 GPU. The 360 degree spherical view of the game is rendered and converted to a rectangular frame using equirectangular projection (ERP). The ERP video is sliced vertically and only the FoV is encoded with Kvazaar HEVC encoder in real time and sent over the network in UDP packets. Another laptop is used for playback with a HTC Vive VR headset. Our system can reach an end-to-end latency of 30 ms and bit rate of 20 Mbps for stereo 1080p30 format."
Docker-Sec: A Fully Automated Container Security Enhancement Mechanism.,"The popularity of containers is constantly rising in the virtualization landscape, since they incur significantly less overhead than Virtual Machines, the traditional hypervisor-based counterparts, while enjoying better performance. However, containers pose significant security challenges due to their direct communication with the host kernel, allowing attackers to break into the host system and co-located containers more easily than Virtual Machines. Existing security hardening mechanisms are based on the enforcement of Mandatory Access Control rules, which exclusively allow specified, desired operations. However, these mechanisms entail explicit knowledge of the container functionality and behavior and require manual intervention and setup. To overcome these limitations, we present Docker-sec, a user-friendly mechanism for the protection of Docker containers throughout their lifetime via the enforcement of access policies that correspond to the anticipated (and legitimate) activity of the applications they enclose. Docker-sec employs two mechanisms: (a) Upon container creation, it constructs an initial, static set of access rules based on container configuration parameters; (b) During container runtime, the initial set is enhanced with additional rules that further restrict the container's capabilities, reflecting the actual application operations. Through a rich interaction with our system the audience will experience firsthand how Docker-sec can successfully protect containers from zero-day vulnerabilities in an automatic manner, with minimal overhead on the application performance."
Chaff Allocation and Performance for Network Traffic Obfuscation.,"This work considers performance analysis of chaff-based traffic obfuscation against a passive adversary aiming to obtain contextual information, e.g. such as the protocol being used. The obfuscation could be either in terms of chaff bytes which are dummy bytes appended to packets of the intended traffic stream, or chaff packets which are dummy packets again inserted in specific intervals of the original packet stream. Despite consisting of dummy bytes, chaff deployment still results in additional resource consumption and potential drawbacks, and hence has to be deployed in a controlled manner. We first define notions of vulnerability of traffic patterns in terms of contextual privacy. Next, we fix the adversary and focus on optimal allocation of the chaff resources among the traffic to be obfuscated. For adversaries which perform statistical characterization based on packet sizes and interarrival times, we derive chaff placement algorithms based on the waterfilling algorithm commonly used in the field of information theory. We apply our derived algorithms to representative real-world scenarios to obfuscate certain applications vulnerable to contextual privacy leakage."
Distributed Ledger Technology: Blockchain Compared to Directed Acyclic Graph.,"In addition to blockchain, a new paradigm is gaining momentum in the filed of distributed ledger technology-directed acyclic graphs. This paper compares the two paradigms focusing on features relevant to distributed systems using the following representative implementations: Bitcoin, Ethereum and Nano. We examine the applied data structures for maintaining the ledger, consensus mechanisms, transaction confirmation confidence, ledger size, and scalability."
Shared Access to Spreadsheet Elements for End User Programming.,"Spreadsheets are the best-known form of End User Programming. End-user programmers (EUP) write programs, but not as their primary job function. They frequently develop software to perform tabular data visualization and analytics using spreadsheets and spreadsheets' formulas as a general purpose programming language. Recently spreadsheets shifted from personal office tools aimed at improving people productivity to enterprise level tools aimed at supporting distributed analytics and decisions. Distributed analytics often requires data sharing among different processes. The spreadsheet supported distributed analytics poses new challenges in spreadsheet data sharing. As in mainstream programming environments, end user spreadsheet programmers need an efficient way to handle shared data access inside spreadsheets to perform collaborative analysis of data. We propose a solution that focuses on spreadsheet element data sharing and collaboration instead of file sharing and collaboration. We present a distributed end to end encrypted tabular data sharing platform called Spreadsheet Space that supports end user programmers for fine grained spreadsheet element data sharing for collaborative analytics."
MIN-Max-Min: A Heuristic Scheduling Algorithm for Jobs across Geo-Distributed Datacenters.,"In geo-distributed datacenters, tasks in a job often need to run on different sites due to the input data locality or special preference for resources. The completion time (make span) of the job depends on the execution of the slowest task. Considering the heterogeneity of resources and potential skews in number of tasks per job, how to reduce the make span to improve application performance remains an open problem. In this paper, we propose a heuristic scheduling algorithm called MIN-Max-Min that coordinate job scheduling across datacenters. MIN-Max-Min gives priority to select the job with the shortest expected completion time to execute by heuristic rule. Experiments show that compared with first come first service strategy, MIN-Max-Min can reduce the average make span of jobs up to 40% under the simulation load."
Website Fingerprinting Attack Mitigation Using Traffic Morphing.,"Website fingerprinting attacks attempt to identify the website visited in anonymized and encrypted network traffic, that is, even if a user is using Tor and HTTPS. These attacks have been shown to be effective. Mitigations have been proposed which decreased the accuracy of the attacks from about 90% to about 20%. We propose a new mitigation technique based on traffic morphing and clustering. The intuition is that a lot of websites, by nature, are similar and can be clustered together. It is then easier and more efficient to make that whole cluster look exactly the same by using traffic morphing, rather than adding noise to make all websites look similar. All the websites in a cluster, thus, would become indistinguishable. There are many ways to perform traffic morphing. As a proof of concept, we used biggest, which means that all websites in a cluster will look exactly like the biggest website (in terms of network packet size) of that cluster. In simulating our proposed approach, the fingerprinting accuracy dropped from 70% to less than 1%."
Realistic Cover Traffic to Mitigate Website Fingerprinting Attacks.,"Website fingerprinting attacks have been shown to be able to predict the website visited even if the network connection is encrypted and anonymized. These attacks have achieved accuracies as high as 92%. Mitigations to these attacks are using cover/decoy network traffic to add noise, padding to ensure all the network packets are the same size, and introducing network delays to confuse an adversary. Although these mitigations have been shown to be effective, reducing the accuracy to 10%, the overhead is very high. The latency overhead is above 100% and the bandwidth overhead is at least 40%. We introduce a new realistic cover traffic algorithm, based on a user's previous network traffic, to mitigate website fingerprinting attacks. In simulations, our algorithm reduces the accuracy of attacks to 14% with zero latency overhead and about 20% bandwidth overhead."
Spatio-Temporal Analysis of HPC I/O and Connection Data.,"The HPC system consists of a set of layers of software and hardware for I/O and networking. System logs are helpful resources to understand what is going on in the system. A challenge is that it is non-trivial to analyze the logs maintained in various levels of the stack. Independent analysis might lead to an incomplete conclusion due to the limited coverage of each log. This work takes a comprehensive approach to analysis that incorporates the logs in the multiple layers and components, in order to facilitate the detection of anomalous activities. This research aims to identify and predict potential performance bottlenecks in the HPC system, by capturing the temporal variation patterns from heterogeneous, high-dimensional, and non-linear log data. In this paper, we share our preliminary efforts for spatio-temporal analysis of HPC I/O and connection data, with our initial observations from the analysis of one-week HPC log data sets collected from one of NERSC systems."
Modeling Data Transfers: Change Point and Anomaly Detection.,"To help the operations and resource planning of a large experimental facility, we model the time needed for transferring the data files produced by the facility to a computer center, with the goals of predicting expected file transfer time and identifying unusually slow transfers that might require attention from human operators. The file transfer time can be thought of having two parts: a base time depending on the hardware and software involved, and a congestion part due to uncontrollable interferences from other operations on the shared resources including network links, disk storage systems, and CPU involved in the transfers. Since many parameters important to the transfer time are not available to us, we employ a change point detection algorithm to separate the data records into time periods (called segments) with relatively stable behavior. Within each segment, we apply a non-parametric model to describe the congestion time. When the observed file transfer time is significantly longer than typical expected time, we declare the particular file transfer to be unusually slow. When many of these unusually slow file transfers are observed, it is worthwhile to notify the human operators to investigate the abnormal behavior of the system."
An Empirical Study on Network Anomaly Detection Using Convolutional Neural Networks.,"Deep learning has been widely applied to network anomaly detection to improve performance. In our past research, we empirically evaluated a set of deep learning models, including Fully Connected Network (FCN), Variational Auto Encoder (VAE), and Sequence to Sequence model with Long Short-Term Memory (Seq2Seq-LSTM), for network anomaly detection. Additionally, we evaluate Convolution Neural Networks (CNNs) for network anomaly detection in this paper. We set up three simple CNN models with different internal depths (shallow CNN, moderate CNN, and deep CNN) to see the impact of the depth to the performance. We evaluate the models using three different types of traffic datasets. Our experimental results show that deeper structures do not make any performance improvement. In addition, we observed that the evaluated CNN models occasionally outperform the VAE models, but do not work better than the other deep learning models based on FCN and Seq2Seq-LSTM."
A Computation Workload Characteristic Study of C-RAN.,"Driven by the surging demand of mobile applications and IoT devices, the amount of global mobile data traffic is estimated to increase sevenfold in the next few years and reach 69 exabytes per month by 2022. This rapid growing rate force telecom operator to adapt new wireless network technologies in order to deliver desired network performance and quality while reduce network deployment and operating costs. One of the approaches that has gained more traction recently is C-RAN, which aims to renovate the infrastructure of radio access network based on cloud technology. In this work, we built a cloudified LTE testbed environment of C-RAN by integrating the OpenAirInterface (OAI), an open-source software radio solution, with the OpenStack, and open-source cloud infrastructure solution. Using the testbed, we conducted workload study to understand the computation resource demand of C-RAN software, and proposed a function splitting technique to improve the resource utilization of C-RAN cloud platform."
A Comprehensive Study of Wide Area Data Movement at a Scientific Computing Facility.,"Wide-area data transfer is central to distributed science. Network capacity, data movement infrastructure, and tools in science environments continuously evolve to meet the requirements of distributed-science applications. Research and education (R&E) networks such as the U.S. Department of Energy's Energy Sciences network and Internet2 provide multiple 100 Gbps backbone networks. Large scientific facilities and research institutions have 100 Gbps wide-area network connectivity, and 10 Gbps wide-area network connectivity is common for a lot of R&E institutions. Many of these institutions employ Science DMZs, dedicated data transfer node(s), and high performance data movement tools to improve wide area data transfer performance. Large facilities may use 10 or more dedicated data transfer nodes to meet the needs of their users. In this work, we analyze various logs pertaining to wide area data transfers in and out of a large scientific facility to obtain insights on data transfer characteristics and behavior. We also show some of the inefficiencies in the state-of-the-art data movement tool and discuss approaches to address these inefficiencies."
QoE-Based User-Regulated Congestion Control.,"In response to poor quality of experience (QoE), users self-regulating, i.e. they immediately release bandwidth and abandon network. However, there are studies that show users are willing to tolerate poor QoE for some time to evaluate if network performance will improve before abandoning the network. In this paper, we investigate how users willingness to wait for improved QoE may influence network activities, such as network pricing, bandwidth allocation, network revenue, and performance. We develop and employ a self-regulation model that includes user evaluation of QoE before deciding to abandon or stay in the network. This model considers these two factors: user tolerance of low QoE and the price per unit a user is willing to pay. Our investigation uncovers a double edged problem - network may be populated with lower paying users, who are also dissatisfied. These lower paying users drive the price higher than the price produced by conventional solution for network congestion. This leads to our proposal for a market informed congestion control scheme, where network resolves congestion based on user profile that is defined by their ability to pay and demand for bandwidth."
Investigating the Impact of Advertisement Banners and Clips on Video QoE.,"Although Quality of Experience (QoE) of Internet services can be affected by context influence factors, their actual impact is not widely investigated yet. In the context of online video services, web portals often display advertisement banners or clips to monetize their service. However, these advertisements can distract or annoy the users, which might degrade the QoE of the actual video service. In this work, two crowdsourcing studies were conducted to investigate the impact of advertisement banners and clips on video QoE. Therefore, both theoretical opinions on in-service advertisements and subjective quality ratings are evaluated. The findings confirm that advertisements are negatively perceived by users during service consumption, but a generally negative impact on video QoE cannot be supported, as the interplay of advertisement and the QoE of video services is rather complex."
InspectorGadget: Inferring Network Protocol Configuration for Web Services.,"Over the last decade, in an attempt to improve the end-user experience, the community has proposed a multitude of changes to the configuration parameters of the networking protocol stack of modern web servers. These changes range from improving security (e.g., TLS 1.2) to improving performance (e.g., HTTP/2). While the performance implications of several of these configuration parameters have been studied in isolation. To date, there is no holistic and general tool to infer, analyze, and under-stand the actual configuration parameters employed by popular web services. Moreover, it is unclear how these parameters are tuned to account for differences in network conditions, devices characteristics, or web page complexity. Although little is known, the configuration of these parameters impact attempts to model and understand performance expectations across the internet. To this end, we present InspectorGadget, a framework for characterizing and fingerprinting the configuration parameters of a server's network stack. InspectorGadget leverages some domain-specific heuristics for reverse engineering configuration parameters and options (protocol versions). To demonstrate the efficacy of InspectorGadget, we implemented a prototype of InspectorGadget and used this prototype to survey the configuration parameters for the top 10K online content providers across different regions and end-user devices."
Web Browsing Measurements: An Above-the-Fold Browser-Based Technique.,"Web browsing is the most important Internet service, and offering the best performance to end-users is of prime importance. The World Wide Web Consortium (W3C) has brought along the Page Load Time (PLT) metric as a QoE (Quality of Experience) and QoS (Quality of Service) benchmarking indicator, which is nowadays the de facto web metric used by researchers, large service companies and web developers. Although alternative web metrics have been introduced to measure part of the loading process, the techniques used need additional computing power and timings are not offered in real-time. In order to provide real-time fined-grained timings during web browsing measurement campaigns, we present in this paper the TFVR (Time for Full Visual Rendering), a technique being browser-based to calculate the Above-The-Fold (ATF) offering the loading time of the visible portion at first glance of a web page. The TFVR exposes fine-grained timings such as networking and processing time for every downloaded resource. Based on a measurement campaign on top 10,000 Alexa websites, we have been able to better quantify and identify web page loading inefficiencies through a tool we have designed, namely, MORIS (Measuring and Observing Representative Information on webSites)."
Studying the Impact of HAS QoE Factors on the Standardized QoE Model P.1203.,"P.1203 is a recent standardized model for assessing the Quality of Experience (QoE) of HTTP Adaptive Video Streaming (HAS). However, its complex definition does not allow for a straightforward identification of the underlying assumptions. To overcome this issue, this work investigates the impact of the well-known QoE factors of HAS, namely, initial delay, stalling, and adaptation, on the output QoE score of the model. Therefore, parameter studies are conducted using a reference implementation of P.1203, and the model response to variations of the input QoE factors are compared to results of previous QoE studies in order to get a deeper understanding of the standardized model and its inherent weighting of the QoE factors of HAS."
Enhancing Machine Learning Based QoE Prediction by Ensemble Models.,"The number of smartphones connected to wireless networks and the volume of wireless network traffic generated by such devices have dramatically increased in the last few years, making it more challenging to tackle wireless network monitoring applications. The high-dimensionality of network data provided by current smartphone devices opens the door to the massive application of machine learning approaches to improve different wireless networking applications. In this paper we study the specific problem of Quality of Experience (QoE) prediction for popular smartphone apps, using machine learning models and in-smartphone measurements. We evaluate and compare different models for the analysis of smartphone generated data, including single models as well as machine learning ensembles such as bagging, boosting and stacking. Results suggest that, while decision-tree based models are the most accurate single models to predict QoE, ensemble learning models, and in particular stacking ones, are capable to significantly increase accuracy prediction and overall classification performance."
