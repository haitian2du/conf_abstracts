Session details: Brave New Ideas,No abstract available.
To Create What You Tell: Generating Videos from Captions,"We are creating multimedia contents everyday and everywhere. While automatic content generation has played a fundamental challenge to multimedia community for decades, recent advances of deep learning have made this problem feasible. For example, the Generative Adversarial Networks (GANs) is a rewarding approach to synthesize images. Nevertheless, it is not trivial when capitalizing on GANs to generate videos. The difficulty originates from the intrinsic structure where a video is a sequence of visually coherent and semantically dependent frames. This motivates us to explore semantic and temporal coherence in designing GANs to generate videos. In this paper, we present a novel Temporal GANs conditioning on Captions, namely TGANs-C, in which the input to the generator network is a concatenation of a latent noise vector and caption embedding, and then is transformed into a frame sequence with 3D spatio-temporal convolutions. Unlike the naive discriminator which only judges pairs as fake or real, our discriminator additionally notes whether the video matches the correct caption. In particular, the discriminator network consists of three discriminators: video discriminator classifying realistic videos from generated ones and optimizes video-caption matching, frame discriminator discriminating between real and fake frames and aligning frames with the conditioning caption, and motion discriminator emphasizing the philosophy that the adjacent frames in the generated videos should be smoothly connected as in real ones. We qualitatively demonstrate the capability of our TGANs-C to generate plausible videos conditioning on the given captions on two synthetic datasets (SBMG and TBMG) and one real-world dataset (MSVD). Moreover, quantitative experiments on MSVD are performed to validate our proposal via Generative Adversarial Metric and human study."
Harnessing A.I. for Augmenting Creativity: Application to Movie Trailer Creation,"In this paper, we describe the first-ever machine human collaboration at creating a real movie trailer (officially released by 20th Century Fox). We introduce an intelligent system designed to understand and encode patterns and types of emotions in horror movies that are useful in trailers. We perform multi-modal semantics extraction including audio visual sentiments and scene analysis and employ a statistical approach to model the key defining components that characterize horror movie trailers. The system was applied on a full-length feature film, ""Morgan'' released in 2016 where the system identified 10 moments as best candidates for a trailer. We partnered with a professional filmmaker who arranged and edited each of the moments together to construct a comprehensive trailer completing the entire processing as well as the final trailer assembly within 24 hours. We discuss disruptive opportunities for the film industry and the tremendous media impact of the AI trailer. We confirm the effectiveness of our trailer with a very supportive user study. Finally based on our close interaction with the film industry, we also introduce and investigate the novel paradigm of tropes within the context of movies for advancing content creation."
Brain2Image: Converting Brain Signals into Images,"Reading the human mind has been a hot topic in the last decades, and recent research in neuroscience has found evidence on the possibility of decoding, from neuroimaging data, how the human brain works. At the same time, the recent rediscovery of deep learning combined to the large interest of scientific community on generative methods has enabled the generation of realistic images by learning a data distribution from noise. The quality of generated images increases when the input data conveys information on visual content of images. Leveraging on these recent trends, in this paper we present an approach for generating images using visually-evoked brain signals recorded through an electroencephalograph (EEG). More specifically, we recorded EEG data from several subjects while observing images on a screen and tried to regenerate the seen images. To achieve this goal, we developed a deep-learning framework consisting of an LSTM stacked with a generative method, which learns a more compact and noise-free representation of EEG data and employs it to generate the visual stimuli evoking specific brain responses."
Do Individuals Smile More in Diverse Social Company?: Studying Smiles and Diversity Via Social Media Photos,"Photographs are one of the most fundamental ways for human beings to capture their social experiences and smiling is one of the most common actions associated with photo-taking. Photos, thus provide a unique opportunity to study the phenomena of mixing of different people and also the smiles expressed by individuals in these social settings. In this work, we study whether a social media-based computational framework can be employed to obtain smile and diversity scores at very fine, individual relationship resolution, and study their associations. We analyze two data sets from different social networks, Twitter and Instagram, over different time periods. Primarily looking at photographs, using computer vision APIs, we capture the diversity of social interactions in terms of age, gender, and race of those present, and smile levels. Analysis of both data sets suggest similar and significant findings: (a) people, in general, tend to smile more in the presence of others; and (b) people tend to smile more in a more diverse company. The results can help scale, test, and validate multiple theories related to affect and diversity in sociology, psychology, biology, and urban planning, and inform future mechanisms for encouraging people to smile more often in everyday settings"
How Personality Affects our Likes: Towards a Better Understanding of Actionable Images,"Messages like ""If You Drink Don't Drive"", ""Each water drop count"" or ""Smoking causes cancer"" are often paired with visual content in order to persuade an audience to perform specific actions, such as clicking a link, retweeting a post or purchasing a product. Despite its usefulness, the current way of discovering actionable images is entirely manual and typically requires marketing experts to filter over thousands of candidate images. To help understand the audience, marketers and social scientists have been investigating for years the role of personality in personalized services by leveraging AI technologies and social network data. In this work, we analyze how personality affects user actions on images in a social network website, and which visual stimuli contained in image content influence actions from users with certain Big Five traits. In order to achieve this goal, we ground this research on psychological studies which investigate the interplay between personality and emotions. Given a public Twitter dataset containing 1.6 million user-image timeline retweet actions, we carried out two extensive statistical analysis, which show significant correlation between personality traits and affective visual concepts in image content. We then proposed a novel model that combines user personality traits and image visual concepts for the task of predicting user actions in advance. This work is the first attempt to integrate personality traits and multimedia features, and moves an important step towards building personalized systems for automatically discovering actionable multimedia content."
